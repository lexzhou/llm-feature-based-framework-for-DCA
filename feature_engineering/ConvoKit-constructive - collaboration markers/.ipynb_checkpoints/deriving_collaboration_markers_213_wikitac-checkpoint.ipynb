{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703b9c0f",
   "metadata": {},
   "source": [
    "# 0. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ce9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Constructiveness analysis on StreetCrowd: message-level features.\n",
    "\n",
    "This is a reimplementation of the features from our paper,\n",
    "\n",
    "Conversational markers of constructive discussions. Vlad Niculae and\n",
    "Cristian Danescu-Niculescu-Mizil. In: Proc. of NAACL 2016.\n",
    "https://vene.ro/constructive/\n",
    "\n",
    "See the `test` function for a usage example.\n",
    "\"\"\"\n",
    "\n",
    "# Author: Vlad Niculae <vlad@vene.ro>\n",
    "# License: Simplified BSD\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from stopwords import stopwords as mallet_stopwords\n",
    "\n",
    "\n",
    "class Lexicon(object):\n",
    "    \"\"\"Word matching code for lexicons.\n",
    "\n",
    "    Since lexicons may contain multi-word phrases (\"I agree\") and lexicons may\n",
    "    overlap, we don't tokenize, use string matching instead.\n",
    "    \"\"\"\n",
    "    def __init__(self, wordlists):\n",
    "        self.wordlists = wordlists\n",
    "        self.regex = {cat: self.wordlist_to_re(wordlist)\n",
    "                      for cat, wordlist in wordlists.items()}\n",
    "\n",
    "    def wordlist_to_re(self, wordlist):\n",
    "        return re.compile(r'\\b(?:{})\\b'.format(\"|\".join(wordlist).lower()))\n",
    "\n",
    "    def count_words(self, text, return_match=False):\n",
    "        \"\"\"Returns a dict {category_name: sum 1[w in category]}\n",
    "\n",
    "        Words are double-counted if they occur in more\n",
    "        than one lexicon.\n",
    "        \"\"\"\n",
    "        text_ = text.lower()\n",
    "        match = {cat: reg.findall(text_) for cat, reg in self.regex.items()}\n",
    "        count = {cat: len(m) for cat, m in match.items()}\n",
    "\n",
    "        if return_match:\n",
    "            return count, match\n",
    "        else:\n",
    "            return count\n",
    "\n",
    "lexicons = {\n",
    "    'pron_me': ['i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'id', 'im', 'ive',\n",
    "                'me', 'mine', 'my', 'myself'],\n",
    "    'pron_we': [\"let's\", 'lets', 'our', 'ours', 'ourselves', 'us',\n",
    "                'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'weve'],\n",
    "    'pron_you': [\"y'all\", 'yall', 'you', \"you'd\", \"you'll\", \"you're\",\n",
    "                 \"you've\", 'youd', 'youll', 'your', 'youre', 'yours',\n",
    "                 'youve'],\n",
    "    'pron_3rd': ['he', \"he'd\", \"he's\", 'hed', 'her', 'hers', 'herself',\n",
    "                 'hes', 'him', 'himself', 'his', 'she', \"she'd\",\n",
    "                 \"she'll\", \"she's\", 'shes', 'their', 'them', 'themselves',\n",
    "                 'they', \"they'd\", \"they'll\", \"they've\", 'theyd', 'theyll',\n",
    "                 'theyve', \"they're\", \"theyre\"]\n",
    "}\n",
    "\n",
    "with open(os.path.join('lexicons', 'my_geo.txt')) as f:\n",
    "    lexicons['geo'] = [line.strip().lower() for line in f]\n",
    "\n",
    "with open(os.path.join('lexicons', 'my_meta.txt')) as f:\n",
    "    lexicons['meta'] = [line.strip().lower() for line in f]\n",
    "\n",
    "with open(os.path.join('lexicons', 'my_certain.txt')) as f:\n",
    "    lexicons['certain'] = [line.strip().lower() for line in f]\n",
    "\n",
    "with open(os.path.join('lexicons', 'my_hedges.txt')) as f:\n",
    "    lexicons['hedge'] = [line.strip().lower() for line in f]\n",
    "\n",
    "\n",
    "lex_matcher = Lexicon(lexicons)\n",
    "\n",
    "\n",
    "def get_content_tagged(words, tags):\n",
    "    \"\"\"Return content words based on tag\"\"\"\n",
    "    return [w for w, tag in zip(words.lower().split(), tags.split())\n",
    "            if tag in (\"N\", \"^\", \"S\", \"Z\", \"A\", \"T\", \"V\")]\n",
    "\n",
    "\n",
    "def message_features(reasons, stopwords=mallet_stopwords):\n",
    "    \"\"\"Compute message-level features from a chat.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reasons, iterable:\n",
    "        sequence of tuples (user, tokens, tags).\n",
    "        In StreetCrowd, users individually can leave an explanation for their\n",
    "        solo game decision.  These reasons populate the chat when the team\n",
    "        meets, but they have no intrinsic order, so they can only introduce\n",
    "        but not adopt idea words.  Also, more than one reason can introduce\n",
    "        an idea, because they are written independently. \n",
    "\n",
    "    \"\"\"\n",
    "    seen_words = set()\n",
    "    introduced = defaultdict(set)\n",
    "    where_introduced = defaultdict(list)\n",
    "    repeated = defaultdict(set)\n",
    "\n",
    "    reason_features = []\n",
    "\n",
    "    for k, (user, tokens, tags) in enumerate(reasons):\n",
    "        features = {}\n",
    "        content_words = [w for w in get_content_tagged(tokens, tags)\n",
    "                         if w not in stopwords]\n",
    "\n",
    "        # all new content words are new ideas here\n",
    "        introduced[user].update(content_words)\n",
    "        seen_words.update(content_words)\n",
    "        for w in content_words:\n",
    "            where_introduced[w].append(('reason', k))\n",
    "\n",
    "        # length statistics\n",
    "        features['n_words'] = len(tokens.split())\n",
    "        lex_counts = lex_matcher.count_words(tokens)\n",
    "        features.update(lex_counts)\n",
    "\n",
    "        # fillers\n",
    "        features['n_introduced'] = len(content_words)  # Modified for single utterance\n",
    "        features['n_introduced_w_certain'] = features['n_introduced'] * features['certain']\n",
    "        features['n_introduced_w_hedge'] = features['n_introduced'] * features['hedge']\n",
    "\n",
    "        reason_features.append(features)\n",
    "\n",
    "    return reason_features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3e293e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'n_words': 11, 'pron_me': 1, 'pron_we': 0, 'pron_you': 0, 'pron_3rd': 0, 'geo': 0, 'meta': 0, 'certain': 1, 'hedge': 0, 'n_introduced': 0, 'n_introduced_w_certain': 0, 'n_introduced_w_hedge': 0}]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Example usage \n",
    "my_utterance = \"This is a sample sentence to analyze. i am sure s.\"\n",
    "pos_tags = nltk.pos_tag(my_utterance.split())\n",
    "tokens = my_utterance\n",
    "tags = \" \".join([tag for (_, tag) in pos_tags])\n",
    "\n",
    "test_reasons = [('user1', tokens, tags)]  # Create the 'reason' structure\n",
    "reason_feat = message_features(test_reasons, stopwords) \n",
    "print(reason_feat)  # You might not need msg_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d64343",
   "metadata": {},
   "source": [
    "# 1. Deriving collaboration markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0905e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def load_data_wikitac():\n",
    "    with open('../../wikitactics.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    conversations = []\n",
    "    utterances_cleaned = []\n",
    "    labels = []\n",
    "    # Extract conversations/disputes for ESCALATED disputes\n",
    "    for dispute in data:\n",
    "        users = list()\n",
    "        conversation = ''\n",
    "        utt_cleaned = ''\n",
    "        for utterance in dispute['utterances']:\n",
    "            username = utterance['username']\n",
    "            text = utterance['text']\n",
    "            conversation += f\"<user_id={username}>\\n{text}\\n\\n\"\n",
    "            utt_cleaned += text + '\\n\\n'\n",
    "        conversations.append(conversation)\n",
    "        utterances_cleaned.append(utt_cleaned)\n",
    "        labels.append(dispute['escalation_label'])\n",
    "\n",
    "    return conversations, utterances_cleaned, labels\n",
    "\n",
    "\n",
    "def deriving_collaboration_markers(utterance):\n",
    "    pos_tags = nltk.pos_tag(utterance.split())\n",
    "    tokens = utterance\n",
    "    tags = \" \".join([tag for (_, tag) in pos_tags])\n",
    "\n",
    "    test_reasons = [('user1', tokens, tags)]  # Create the 'reason' structure\n",
    "    reason_feat = message_features(test_reasons, stopwords) \n",
    "    return reason_feat\n",
    "\n",
    "conversations, utterances, labels = load_data_wikitac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b8638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "annotations_utts = []\n",
    "for i, conv in enumerate(utterances):\n",
    "    print(i)\n",
    "    annotations_ci = []\n",
    "    \n",
    "    for utterance in conv.split('\\n\\n'):\n",
    "        markers = list(deriving_collaboration_markers(utterance)[0].values())\n",
    "        annotations_ci.append(markers)\n",
    "\n",
    "    annotations_utts.append(annotations_ci)\n",
    "\n",
    "annotations_utts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b28e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "markers = list(deriving_collaboration_markers(utterance)[0].keys())\n",
    "colab_features_mean = defaultdict(list)\n",
    "colab_features_grad = defaultdict(list)\n",
    "\n",
    "for ann_ci in annotations_utts:\n",
    "    for i, marker in enumerate(markers):\n",
    "        marker_values_ci = [ann_ci[j][i] for j in range(len(ann_ci))]\n",
    "        # print('\\n\\n', marker, marker_values_ci)\n",
    "\n",
    "        # MEAN\n",
    "        colab_features_mean[marker].append(np.mean(marker_values_ci))\n",
    "        \n",
    "        # GRADIENT (SLOPE)\n",
    "        x = np.arange(len(marker_values_ci))\n",
    "        y = marker_values_ci\n",
    "        try:\n",
    "            slope, intercept = np.polyfit(x, y, 1) # Fitting a linear regression (polynomial of degree 1) to the data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            slope = 0 # LinAlgError: SVD did not converge in Linear Least Squares\n",
    "        \n",
    "        colab_features_grad[marker].append(slope)        \n",
    "\n",
    "a = pd.DataFrame(colab_features_mean).add_suffix('_mean')\n",
    "b = pd.DataFrame(colab_features_grad).add_suffix('_grad')\n",
    "colab_feat_df = pd.concat([a, b], axis=1)\n",
    "# colab_feat_df.to_csv(\"../213_wikitac_collaboration_markers.csv\", index=False)\n",
    "colab_feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10aefe9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Random Forest - AUROC: 0.9929999063896671\n",
      "Random Forest - F1-Score: 0.9431939978563774\n",
      "Random Forest - PR-AUC: 0.993323609532877\n",
      "Logistic Regression - AUROC: 0.7469048723089751\n",
      "Logistic Regression - F1-Score: 0.6707589285714286\n",
      "Logistic Regression - PR-AUC: 0.7391034785974998\n",
      "TEST:\n",
      "Random Forest - AUROC: 0.6981132075471698\n",
      "Random Forest - F1-Score: 0.6604651162790698\n",
      "Random Forest - PR-AUC: 0.6871521602036799\n",
      "Logistic Regression - AUROC: 0.6851525304179158\n",
      "Logistic Regression - F1-Score: 0.6130653266331658\n",
      "Logistic Regression - PR-AUC: 0.683305667960362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, average_precision_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = colab_feat_df\n",
    "X = colab_feat_df.iloc[:,:] \n",
    "y = pd.DataFrame(labels, columns = ['label'])['label']\n",
    "\n",
    "# Initialize KFold (we use 10-fold cross-validation)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the models\n",
    "rf_model = RandomForestClassifier(max_depth=5, random_state=42)\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Initialize the oversampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# To accumulate predictions and true values for each model\n",
    "all_y_pred_train_rf = []\n",
    "all_y_pred_train_lr = []\n",
    "all_y_train = []\n",
    "all_y_pred_test_rf = []\n",
    "all_y_pred_test_lr = []\n",
    "all_y_test = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split into training and testing sets using .iloc for pandas\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Perform random oversampling on the training data\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Fit the models on the oversampled training data\n",
    "    rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "    lr_model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Training Predictions (probabilities)\n",
    "    y_pred_train_rf = rf_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train_lr = lr_model.predict_proba(X_train)[:, 1]\n",
    "    all_y_pred_train_rf.extend(y_pred_train_rf)\n",
    "    all_y_pred_train_lr.extend(y_pred_train_lr)\n",
    "    all_y_train.extend(y_train)\n",
    "    \n",
    "    # Testing Predictions (probabilities)\n",
    "    y_pred_test_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_test_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "    all_y_pred_test_rf.extend(y_pred_test_rf)\n",
    "    all_y_pred_test_lr.extend(y_pred_test_lr)\n",
    "    all_y_test.extend(y_test)\n",
    "\n",
    "# Evaluate TRAIN\n",
    "auc_train_rf = roc_auc_score(all_y_train, all_y_pred_train_rf)\n",
    "f1_train_rf = f1_score(all_y_train, np.round(all_y_pred_train_rf))\n",
    "pr_auc_train_rf = average_precision_score(all_y_train, all_y_pred_train_rf)\n",
    "auc_train_lr = roc_auc_score(all_y_train, all_y_pred_train_lr)\n",
    "f1_train_lr = f1_score(all_y_train, np.round(all_y_pred_train_lr))\n",
    "pr_auc_train_lr = average_precision_score(all_y_train, all_y_pred_train_lr)\n",
    "print('TRAIN:')\n",
    "print(\"Random Forest - AUROC:\", auc_train_rf)\n",
    "print(\"Random Forest - F1-Score:\", f1_train_rf)\n",
    "print(\"Random Forest - PR-AUC:\", pr_auc_train_rf)\n",
    "print(\"Logistic Regression - AUROC:\", auc_train_lr)\n",
    "print(\"Logistic Regression - F1-Score:\", f1_train_lr)\n",
    "print(\"Logistic Regression - PR-AUC:\", pr_auc_train_lr)\n",
    "\n",
    "# Evaluate TEST\n",
    "auc_test_rf = roc_auc_score(all_y_test, all_y_pred_test_rf)\n",
    "f1_test_rf = f1_score(all_y_test, np.round(all_y_pred_test_rf))\n",
    "pr_auc_test_rf = average_precision_score(all_y_test, all_y_pred_test_rf)\n",
    "auc_test_lr = roc_auc_score(all_y_test, all_y_pred_test_lr)\n",
    "f1_test_lr = f1_score(all_y_test, np.round(all_y_pred_test_lr))\n",
    "pr_auc_test_lr = average_precision_score(all_y_test, all_y_pred_test_lr)\n",
    "print('TEST:')\n",
    "print(\"Random Forest - AUROC:\", auc_test_rf)\n",
    "print(\"Random Forest - PR-AUC:\", pr_auc_test_rf)\n",
    "print(\"Logistic Regression - AUROC:\", auc_test_lr)\n",
    "print(\"Logistic Regression - PR-AUC:\", pr_auc_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02da8f98-7bd2-4c4b-a13e-a1c63a50a3f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Random Forest - AUROC: 0.9996788947885169\n",
      "Random Forest - F1-Score: 0.9922077922077921\n",
      "Random Forest - PR-AUC: 0.9996876890850922\n",
      "Logistic Regression - AUROC: 1.0\n",
      "Logistic Regression - F1-Score: 1.0\n",
      "Logistic Regression - PR-AUC: 0.9999999999999999\n",
      "TEST:\n",
      "Random Forest - AUROC: 0.6757185681537649\n",
      "Random Forest - F1-Score: 0.6576576576576575\n",
      "Random Forest - PR-AUC: 0.6864393942011422\n",
      "Logistic Regression - AUROC: 0.7138952565685064\n",
      "Logistic Regression - F1-Score: 0.6363636363636364\n",
      "Logistic Regression - PR-AUC: 0.6809712388447053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'conversations' is a list or array of text conversations\n",
    "# and 'labels' is a list or array of corresponding labels\n",
    "\n",
    "# Create a DataFrame with conversations and labels\n",
    "data = pd.DataFrame({'conversation': conversations, 'label': labels})\n",
    "\n",
    "# Initialize KFold (we use 10-fold cross-validation)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the models\n",
    "rf_model = RandomForestClassifier(max_depth=5, random_state=42)\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Initialize the oversampler\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# To accumulate predictions and true values for each model\n",
    "all_y_pred_train_rf = []\n",
    "all_y_pred_train_lr = []\n",
    "all_y_train = []\n",
    "all_y_pred_test_rf = []\n",
    "all_y_pred_test_lr = []\n",
    "all_y_test = []\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    # Split into training and testing sets using .iloc for pandas\n",
    "    train_data, test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "    \n",
    "    # Vectorize the conversations\n",
    "    X_train = vectorizer.fit_transform(train_data['conversation'])\n",
    "    X_test = vectorizer.transform(test_data['conversation'])\n",
    "    y_train, y_test = train_data['label'], test_data['label']\n",
    "\n",
    "    # Perform random oversampling on the training data\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Fit the models on the oversampled training data\n",
    "    rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "    lr_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Training Predictions (probabilities)\n",
    "    y_pred_train_rf = rf_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train_lr = lr_model.predict_proba(X_train)[:, 1]\n",
    "    all_y_pred_train_rf.extend(y_pred_train_rf)\n",
    "    all_y_pred_train_lr.extend(y_pred_train_lr)\n",
    "    all_y_train.extend(y_train)\n",
    "\n",
    "    # Testing Predictions (probabilities)\n",
    "    y_pred_test_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_test_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "    all_y_pred_test_rf.extend(y_pred_test_rf)\n",
    "    all_y_pred_test_lr.extend(y_pred_test_lr)\n",
    "    all_y_test.extend(y_test)\n",
    "\n",
    "# Evaluate TRAIN\n",
    "auc_train_rf = roc_auc_score(all_y_train, all_y_pred_train_rf)\n",
    "f1_train_rf = f1_score(all_y_train, np.round(all_y_pred_train_rf))\n",
    "pr_auc_train_rf = average_precision_score(all_y_train, all_y_pred_train_rf)\n",
    "auc_train_lr = roc_auc_score(all_y_train, all_y_pred_train_lr)\n",
    "f1_train_lr = f1_score(all_y_train, np.round(all_y_pred_train_lr))\n",
    "pr_auc_train_lr = average_precision_score(all_y_train, all_y_pred_train_lr)\n",
    "\n",
    "print('TRAIN:')\n",
    "print(\"Random Forest - AUROC:\", auc_train_rf)\n",
    "print(\"Random Forest - F1-Score:\", f1_train_rf)\n",
    "print(\"Random Forest - PR-AUC:\", pr_auc_train_rf)\n",
    "print(\"Logistic Regression - AUROC:\", auc_train_lr)\n",
    "print(\"Logistic Regression - F1-Score:\", f1_train_lr)\n",
    "print(\"Logistic Regression - PR-AUC:\", pr_auc_train_lr)\n",
    "\n",
    "# Evaluate TEST\n",
    "auc_test_rf = roc_auc_score(all_y_test, all_y_pred_test_rf)\n",
    "f1_test_rf = f1_score(all_y_test, np.round(all_y_pred_test_rf))\n",
    "pr_auc_test_rf = average_precision_score(all_y_test, all_y_pred_test_rf)\n",
    "auc_test_lr = roc_auc_score(all_y_test, all_y_pred_test_lr)\n",
    "f1_test_lr = f1_score(all_y_test, np.round(all_y_pred_test_lr))\n",
    "pr_auc_test_lr = average_precision_score(all_y_test, all_y_pred_test_lr)\n",
    "\n",
    "print('TEST:')\n",
    "print(\"Random Forest - AUROC:\", auc_test_rf)\n",
    "print(\"Random Forest - F1-Score:\", f1_test_rf)\n",
    "print(\"Random Forest - PR-AUC:\", pr_auc_test_rf)\n",
    "print(\"Logistic Regression - AUROC:\", auc_test_lr)\n",
    "print(\"Logistic Regression - F1-Score:\", f1_test_lr)\n",
    "print(\"Logistic Regression - PR-AUC:\", pr_auc_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc560bc-43a1-4fed-bba7-258ce4fb247f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
