{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703b9c0f",
   "metadata": {},
   "source": [
    "# 0. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ce9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Constructiveness analysis on StreetCrowd: message-level features.\n",
    "\n",
    "This is a reimplementation of the features from our paper,\n",
    "\n",
    "Conversational markers of constructive discussions. Vlad Niculae and\n",
    "Cristian Danescu-Niculescu-Mizil. In: Proc. of NAACL 2016.\n",
    "https://vene.ro/constructive/\n",
    "\n",
    "See the `test` function for a usage example.\n",
    "\"\"\"\n",
    "\n",
    "# Author: Vlad Niculae <vlad@vene.ro>\n",
    "# License: Simplified BSD\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from stopwords import stopwords as mallet_stopwords\n",
    "\n",
    "\n",
    "class Lexicon(object):\n",
    "    \"\"\"Word matching code for lexicons.\n",
    "\n",
    "    Since lexicons may contain multi-word phrases (\"I agree\") and lexicons may\n",
    "    overlap, we don't tokenize, use string matching instead.\n",
    "    \"\"\"\n",
    "    def __init__(self, wordlists):\n",
    "        self.wordlists = wordlists\n",
    "        self.regex = {cat: self.wordlist_to_re(wordlist)\n",
    "                      for cat, wordlist in wordlists.items()}\n",
    "\n",
    "    def wordlist_to_re(self, wordlist):\n",
    "        return re.compile(r'\\b(?:{})\\b'.format(\"|\".join(wordlist).lower()))\n",
    "\n",
    "    def count_words(self, text, return_match=False):\n",
    "        \"\"\"Returns a dict {category_name: sum 1[w in category]}\n",
    "\n",
    "        Words are double-counted if they occur in more\n",
    "        than one lexicon.\n",
    "        \"\"\"\n",
    "        text_ = text.lower()\n",
    "        match = {cat: reg.findall(text_) for cat, reg in self.regex.items()}\n",
    "        count = {cat: len(m) for cat, m in match.items()}\n",
    "\n",
    "        if return_match:\n",
    "            return count, match\n",
    "        else:\n",
    "            return count\n",
    "\n",
    "lexicons = {\n",
    "    'pron_me': ['i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'id', 'im', 'ive',\n",
    "                'me', 'mine', 'my', 'myself'],\n",
    "    'pron_we': [\"let's\", 'lets', 'our', 'ours', 'ourselves', 'us',\n",
    "                'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'weve'],\n",
    "    'pron_you': [\"y'all\", 'yall', 'you', \"you'd\", \"you'll\", \"you're\",\n",
    "                 \"you've\", 'youd', 'youll', 'your', 'youre', 'yours',\n",
    "                 'youve'],\n",
    "    'pron_3rd': ['he', \"he'd\", \"he's\", 'hed', 'her', 'hers', 'herself',\n",
    "                 'hes', 'him', 'himself', 'his', 'she', \"she'd\",\n",
    "                 \"she'll\", \"she's\", 'shes', 'their', 'them', 'themselves',\n",
    "                 'they', \"they'd\", \"they'll\", \"they've\", 'theyd', 'theyll',\n",
    "                 'theyve', \"they're\", \"theyre\"]\n",
    "}\n",
    "\n",
    "with open(os.path.join('lexicons', 'my_geo.txt')) as f:\n",
    "    lexicons['geo'] = [line.strip().lower() for line in f]\n",
    "\n",
    "with open(os.path.join('lexicons', 'my_meta.txt')) as f:\n",
    "    lexicons['meta'] = [line.strip().lower() for line in f]\n",
    "\n",
    "with open(os.path.join('lexicons', 'my_certain.txt')) as f:\n",
    "    lexicons['certain'] = [line.strip().lower() for line in f]\n",
    "\n",
    "with open(os.path.join('lexicons', 'my_hedges.txt')) as f:\n",
    "    lexicons['hedge'] = [line.strip().lower() for line in f]\n",
    "\n",
    "\n",
    "lex_matcher = Lexicon(lexicons)\n",
    "\n",
    "\n",
    "def get_content_tagged(words, tags):\n",
    "    \"\"\"Return content words based on tag\"\"\"\n",
    "    return [w for w, tag in zip(words.lower().split(), tags.split())\n",
    "            if tag in (\"N\", \"^\", \"S\", \"Z\", \"A\", \"T\", \"V\")]\n",
    "\n",
    "\n",
    "def message_features(reasons, stopwords=mallet_stopwords):\n",
    "    \"\"\"Compute message-level features from a chat.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reasons, iterable:\n",
    "        sequence of tuples (user, tokens, tags).\n",
    "        In StreetCrowd, users individually can leave an explanation for their\n",
    "        solo game decision.  These reasons populate the chat when the team\n",
    "        meets, but they have no intrinsic order, so they can only introduce\n",
    "        but not adopt idea words.  Also, more than one reason can introduce\n",
    "        an idea, because they are written independently. \n",
    "\n",
    "    \"\"\"\n",
    "    seen_words = set()\n",
    "    introduced = defaultdict(set)\n",
    "    where_introduced = defaultdict(list)\n",
    "    repeated = defaultdict(set)\n",
    "\n",
    "    reason_features = []\n",
    "\n",
    "    for k, (user, tokens, tags) in enumerate(reasons):\n",
    "        features = {}\n",
    "        content_words = [w for w in get_content_tagged(tokens, tags)\n",
    "                         if w not in stopwords]\n",
    "\n",
    "        # all new content words are new ideas here\n",
    "        introduced[user].update(content_words)\n",
    "        seen_words.update(content_words)\n",
    "        for w in content_words:\n",
    "            where_introduced[w].append(('reason', k))\n",
    "\n",
    "        # length statistics\n",
    "        features['n_words'] = len(tokens.split())\n",
    "        lex_counts = lex_matcher.count_words(tokens)\n",
    "        features.update(lex_counts)\n",
    "\n",
    "        # fillers\n",
    "        features['n_introduced'] = len(content_words)  # Modified for single utterance\n",
    "        features['n_introduced_w_certain'] = features['n_introduced'] * features['certain']\n",
    "        features['n_introduced_w_hedge'] = features['n_introduced'] * features['hedge']\n",
    "\n",
    "        reason_features.append(features)\n",
    "\n",
    "    return reason_features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e293e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'n_words': 11, 'pron_me': 1, 'pron_we': 0, 'pron_you': 0, 'pron_3rd': 0, 'geo': 0, 'meta': 0, 'certain': 1, 'hedge': 0, 'n_introduced': 0, 'n_introduced_w_certain': 0, 'n_introduced_w_hedge': 0}]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Example usage \n",
    "my_utterance = \"This is a sample sentence to analyze. i am sure s.\"\n",
    "pos_tags = nltk.pos_tag(my_utterance.split())\n",
    "tokens = my_utterance\n",
    "tags = \" \".join([tag for (_, tag) in pos_tags])\n",
    "\n",
    "test_reasons = [('user1', tokens, tags)]  # Create the 'reason' structure\n",
    "reason_feat = message_features(test_reasons, stopwords) \n",
    "print(reason_feat)  # You might not need msg_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d64343",
   "metadata": {},
   "source": [
    "# 1. Deriving collaboration markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0905e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "# Load the data\n",
    "def load_data(label='after'):\n",
    "    final_convs = []\n",
    "    final_labels = []\n",
    "    wizards_data = []\n",
    "    moral_foundations = [\"care\", \"fairness\", \"liberty\", \"loyalty\", \"authority\", \"sanctity\", \"none\"]\n",
    "    input_files = {\"wizards\": \"../../wizards_dialogues.json\", \"final_argubot\": \"../../argubot_final_exp.json\",\n",
    "                   \"models_dialogues\": \"../../models_dialogues.json\"}\n",
    "    dials_with_scores = {\"wizards\": {}, \"final_argubot\": {}, \"models_dialogues\": {}}\n",
    "\n",
    "\n",
    "    for key in input_files:\n",
    "        input_file = input_files[key]\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        for d in data:\n",
    "            is_wiki = False\n",
    "            for m in d[\"messages\"]:\n",
    "                if 'model' in m and (m['model'] == 'wikibot' or m['model'] == 'controlbot'):\n",
    "                    is_wiki = True\n",
    "                    break\n",
    "            if is_wiki:\n",
    "                continue\n",
    "            yes_no = 'none'\n",
    "            k = 'Did you vote for (Leave) or against (Remain) Brexit in the 2016 UK referendum?'\n",
    "            if k in d['participant_info']:\n",
    "                if d['participant_info'][k].lower() == 'against (remain)':\n",
    "                    yes_no = 'no'\n",
    "                elif d['participant_info'][k].lower() == 'for (leave)':\n",
    "                    yes_no = 'yes'\n",
    "                else:\n",
    "                    yes_no = 'none'\n",
    "\n",
    "            k = 'In the referendum on whether the UK should remain a member of the EU (BREXIT), how did you vote?'\n",
    "            if k in d['participant_info']:\n",
    "                if d['participant_info'][k].lower() == 'remain (against brexit)':\n",
    "                    yes_no = 'no'\n",
    "                elif d['participant_info'][k].lower() == 'leave (for brexit)':\n",
    "                    yes_no = 'yes'\n",
    "                else:\n",
    "                    yes_no = 'none'\n",
    "            k = 'Have you had at least one dose of an approved Covid-19 vaccine?'\n",
    "            if k in d['participant_info']:\n",
    "                if d['participant_info'][k].lower() == 'yes':\n",
    "                    yes_no = 'yes'\n",
    "                elif d['participant_info'][k].lower() == 'no':\n",
    "                    yes_no = 'no'\n",
    "            k = 'Are you a vegan?'\n",
    "            if k in d['participant_info']:\n",
    "                if d['participant_info'][k].lower() == 'yes':\n",
    "                    yes_no = 'yes'\n",
    "                elif d['participant_info'][k].lower() == 'no':\n",
    "                    yes_no = 'no'\n",
    "\n",
    "            if yes_no == 'none':\n",
    "                continue\n",
    "\n",
    "            if 'Questions' in d['participant_info']:\n",
    "                for q in d['participant_info']['Questions']:\n",
    "                    if \"final\" in input_file:\n",
    "                        if label == 'oum':\n",
    "                            continue\n",
    "                        if d['participant_info']['Questions'][q]['after'] == -1:\n",
    "                            continue\n",
    "                    elif d['participant_info']['Questions'][q]['before'] == -1 or d['participant_info']['Questions'][q]['after'] == -1:\n",
    "                        continue\n",
    "                    if 'good reasons' in q.lower():\n",
    "                        if d['topic'] != 'brexit' and 'not' in q.lower() and yes_no == 'no':\n",
    "                            continue\n",
    "                        if d['topic'] != 'brexit' and 'not' not in q.lower() and yes_no == 'yes':\n",
    "                            continue\n",
    "                        if 'leave' in q.lower() and yes_no == 'yes':\n",
    "                            continue\n",
    "                        if 'remain' in q.lower() and yes_no == 'no':\n",
    "                            continue\n",
    "                        if d[\"_id\"] not in dials_with_scores[key]:\n",
    "                            text = ''\n",
    "                            dials_with_scores[key][d[\"_id\"]] = {\"topic\": d[\"topic\"], \"dataset\": key}\n",
    "                            for message in d['messages']:\n",
    "                                if message['role'] == 'admin' or 'modified_argument' not in message:\n",
    "                                    continue\n",
    "\n",
    "                                text = text + '\\n\\n' + '<' + message['role'] + '>' + '\\n' + message['modified_argument']\n",
    "                            dials_with_scores[key][d[\"_id\"]]['text'] = text.strip()\n",
    "                            final_convs.append(text.strip())\n",
    "\n",
    "                    if 'good reasons' in q.lower():\n",
    "                        if False and label == 'oum': \n",
    "                            final_labels.append(float(d['participant_info']['Questions'][q]['after']) - float(d['participant_info']['Questions'][q]['before']))\n",
    "                        else:\n",
    "                            final_labels.append(float(d['participant_info']['Questions'][q]['after']))\n",
    "                        oum = d['participant_info']['Questions'][q]['after'] - d['participant_info']['Questions'][q]['before'] if \"final\" not in input_file else None\n",
    "                        dials_with_scores[key][d[\"_id\"]][\"good_reasons\"] = {\"oum\": oum, \"after\": d['participant_info']['Questions'][q]['after']}\n",
    "                        if 'before' in d['participant_info']['Questions'][q] and d['participant_info']['Questions'][q]['before'] != -1:\n",
    "                            dials_with_scores[key][d[\"_id\"]][\"good_reasons\"]['before'] = d['participant_info']['Questions'][q]['before']\n",
    "                        else:\n",
    "                            dials_with_scores[key][d[\"_id\"]][\"good_reasons\"]['before'] = None\n",
    "\n",
    "\n",
    "    assert len(final_convs) == len(final_labels)\n",
    "    return final_convs, final_labels\n",
    "\n",
    "def get_utterances(text):\n",
    "    \"\"\"\n",
    "    Given a conversation, this function returns a list of utterances\n",
    "    \"\"\"\n",
    "    # Splitting the input text into lines\n",
    "    lines = text.split('\\n')\n",
    "    # Variable to keep the cleaned lines\n",
    "    cleaned_lines = []\n",
    "    # Variable to keep track of whether the next line should be added\n",
    "    add_next_line = False\n",
    "    for line in lines:\n",
    "        # If the line is a participant tag, set the flag to add the next line\n",
    "        if line.strip() == '<participant>':\n",
    "            add_next_line = True\n",
    "        elif line.strip() in ['<woz>', '<chatbot>']:\n",
    "            add_next_line = True\n",
    "        elif add_next_line:\n",
    "            # If the flag is set, add this line to the cleaned list and reset the flag\n",
    "            cleaned_lines.append(line)\n",
    "            add_next_line = False\n",
    "    # Join the cleaned lines back into a single string\n",
    "    cleaned_text = '\\n\\n'.join(cleaned_lines)\n",
    "    return cleaned_text\n",
    "\n",
    "def deriving_collaboration_markers(utterance):\n",
    "    pos_tags = nltk.pos_tag(utterance.split())\n",
    "    tokens = utterance\n",
    "    tags = \" \".join([tag for (_, tag) in pos_tags])\n",
    "\n",
    "    test_reasons = [('user1', tokens, tags)]  # Create the 'reason' structure\n",
    "    reason_feat = message_features(test_reasons, stopwords) \n",
    "    return reason_feat\n",
    "\n",
    "conversations, labels = load_data(label='after')\n",
    "utterances = [get_utterances(c) for c in conversations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "381b8638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotations = collections.defaultdict(list)\n",
    "for i, conv in enumerate(utterances):\n",
    "    annotations_ci = []\n",
    "    \n",
    "    ### participant\n",
    "    for utterance in conv.split('\\n\\n'):\n",
    "        markers = list(deriving_collaboration_markers(utterance)[0].values())\n",
    "        annotations_ci.append(markers)\n",
    "\n",
    "    annotations['utterances'].append(annotations_ci)\n",
    "\n",
    "annotations_utts = annotations['utterances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "050b28e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "markers = list(deriving_collaboration_markers(utterance)[0].keys())\n",
    "colab_features_mean = defaultdict(list)\n",
    "colab_features_grad = defaultdict(list)\n",
    "\n",
    "for ann_ci in annotations_utts:\n",
    "    for i, marker in enumerate(markers):\n",
    "        marker_values_ci = [ann_ci[j][i] for j in range(len(ann_ci))]\n",
    "\n",
    "        # MEAN\n",
    "        colab_features_mean[marker].append(np.mean(marker_values_ci))\n",
    "        \n",
    "        # GRADIENT (SLOPE)\n",
    "        x = np.arange(len(marker_values_ci))\n",
    "        y = marker_values_ci\n",
    "        slope, intercept = np.polyfit(x, y, 1) # Fitting a linear regression (polynomial of degree 1) to the data\n",
    "        colab_features_grad[marker].append(slope)        \n",
    "\n",
    "a = pd.DataFrame(colab_features_mean).add_suffix('_mean')\n",
    "b = pd.DataFrame(colab_features_grad).add_suffix('_grad')\n",
    "colab_feat_df = pd.concat([a, b], axis=1)\n",
    "colab_feat_df.to_csv(\"../542_conversations_collaboration_markers_without_pnp.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
