{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f4be3f-4605-4778-9ae3-c81f17a125b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# !pip install openai\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "openai.api_key = \"XXXXXXXXX\"\n",
    "from sklearn.datasets import fetch_openml\n",
    "import time\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import jsonlines\n",
    "from datetime import date\n",
    "import csv\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17134b69-8e0b-4d27-a490-8e3799f78ad4",
   "metadata": {},
   "source": [
    "### 1. OUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137dfc9-6059-4fcd-bdcb-f0b50d219990",
   "metadata": {},
   "source": [
    "#### 1.1. Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b86519-8419-46f3-be15-0eefb064ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_oum(label='after'):\n",
    "    final_convs = []\n",
    "    final_labels = []\n",
    "    wizards_data = []\n",
    "    moral_foundations = [\"care\", \"fairness\", \"liberty\", \"loyalty\", \"authority\", \"sanctity\", \"none\"]\n",
    "    input_files = {\"wizards\": \"wizards_dialogues.json\", \"final_argubot\": \"argubot_final_exp.json\",\n",
    "                   \"models_dialogues\": \"models_dialogues.json\"}\n",
    "    dials_with_scores = {\"wizards\": {}, \"final_argubot\": {}, \"models_dialogues\": {}}\n",
    "\n",
    "    for key in input_files:\n",
    "        input_file = input_files[key]\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        for d in data:\n",
    "            is_wiki = False\n",
    "            for m in d[\"messages\"]:\n",
    "                if 'model' in m and (m['model'] == 'wikibot' or m['model'] == 'controlbot'):\n",
    "                    is_wiki = True\n",
    "                    break\n",
    "            if is_wiki:\n",
    "                continue\n",
    "            yes_no = 'none'\n",
    "            k = 'Did you vote for (Leave) or against (Remain) Brexit in the 2016 UK referendum?'\n",
    "            if k in d['participant_info']:\n",
    "                if d['participant_info'][k].lower() == 'against (remain)':\n",
    "                    yes_no = 'no'\n",
    "                elif d['participant_info'][k].lower() == 'for (leave)':\n",
    "                    yes_no = 'yes'\n",
    "                else:\n",
    "                    yes_no = 'none'\n",
    "\n",
    "            k = 'In the referendum on whether the UK should remain a member of the EU (BREXIT), how did you vote?'\n",
    "            if k in d['participant_info']:\n",
    "                if d['participant_info'][k].lower() == 'remain (against brexit)':\n",
    "                    yes_no = 'no'\n",
    "                elif d['participant_info'][k].lower() == 'leave (for brexit)':\n",
    "                    yes_no = 'yes'\n",
    "                else:\n",
    "                    yes_no = 'none'\n",
    "            k = 'Have you had at least one dose of an approved Covid-19 vaccine?'\n",
    "            if k in d['participant_info']:\n",
    "                if d['participant_info'][k].lower() == 'yes':\n",
    "                    yes_no = 'yes'\n",
    "                elif d['participant_info'][k].lower() == 'no':\n",
    "                    yes_no = 'no'\n",
    "            k = 'Are you a vegan?'\n",
    "            if k in d['participant_info']:\n",
    "                if d['participant_info'][k].lower() == 'yes':\n",
    "                    yes_no = 'yes'\n",
    "                elif d['participant_info'][k].lower() == 'no':\n",
    "                    yes_no = 'no'\n",
    "\n",
    "            if yes_no == 'none':\n",
    "                continue\n",
    "\n",
    "            if 'Questions' in d['participant_info']:\n",
    "                for q in d['participant_info']['Questions']:\n",
    "                    if \"final\" in input_file:\n",
    "                        if label == 'oum':\n",
    "                            continue\n",
    "                        if d['participant_info']['Questions'][q]['after'] == -1:\n",
    "                            continue\n",
    "                    elif d['participant_info']['Questions'][q]['before'] == -1 or d['participant_info']['Questions'][q]['after'] == -1:\n",
    "                        continue\n",
    "                    if 'good reasons' in q.lower():\n",
    "                        if d['topic'] != 'brexit' and 'not' in q.lower() and yes_no == 'no':\n",
    "                            continue\n",
    "                        if d['topic'] != 'brexit' and 'not' not in q.lower() and yes_no == 'yes':\n",
    "                            continue\n",
    "                        if 'leave' in q.lower() and yes_no == 'yes':\n",
    "                            continue\n",
    "                        if 'remain' in q.lower() and yes_no == 'no':\n",
    "                            continue\n",
    "                        if d[\"_id\"] not in dials_with_scores[key]:\n",
    "                            text = ''\n",
    "                            dials_with_scores[key][d[\"_id\"]] = {\"topic\": d[\"topic\"], \"dataset\": key}\n",
    "                            for message in d['messages']:\n",
    "                                if message['role'] == 'admin' or 'modified_argument' not in message:\n",
    "                                    continue\n",
    "\n",
    "                                text = text + '\\n\\n' + '<' + message['role'] + '>' + '\\n' + message['modified_argument']\n",
    "                            dials_with_scores[key][d[\"_id\"]]['text'] = text.strip()\n",
    "                            final_convs.append(text.strip())\n",
    "\n",
    "                    if 'good reasons' in q.lower():\n",
    "                        if False and label == 'oum':\n",
    "                            final_labels.append(float(d['participant_info']['Questions'][q]['after']) - float(d['participant_info']['Questions'][q]['before']))\n",
    "                        else:\n",
    "                            final_labels.append(float(d['participant_info']['Questions'][q]['after']))\n",
    "                        oum = d['participant_info']['Questions'][q]['after'] - d['participant_info']['Questions'][q]['before'] if \"final\" not in input_file else None\n",
    "                        dials_with_scores[key][d[\"_id\"]][\"good_reasons\"] = {\"oum\": oum, \"after\": d['participant_info']['Questions'][q]['after']}\n",
    "                        if 'before' in d['participant_info']['Questions'][q] and d['participant_info']['Questions'][q]['before'] != -1:\n",
    "                            dials_with_scores[key][d[\"_id\"]][\"good_reasons\"]['before'] = d['participant_info']['Questions'][q]['before']\n",
    "                        else:\n",
    "                            dials_with_scores[key][d[\"_id\"]][\"good_reasons\"]['before'] = None\n",
    "\n",
    "    assert len(final_convs) == len(final_labels)\n",
    "    return final_convs, final_labels\n",
    "\n",
    "\n",
    "\n",
    "def get_utterances_oum(text):\n",
    "    # Splitting the input text into lines\n",
    "    lines = text.split('\\n')\n",
    "    # Variable to keep the cleaned lines\n",
    "    cleaned_lines = []\n",
    "    # Variable to keep track of whether the next line should be added\n",
    "    add_next_line = False\n",
    "    for line in lines:\n",
    "        # If the line is a participant tag, set the flag to add the next line\n",
    "        if line.strip() == '<participant>':\n",
    "            add_next_line = True\n",
    "        elif line.strip() in ['<woz>', '<chatbot>']:\n",
    "            add_next_line = True\n",
    "        elif add_next_line:\n",
    "            # If the flag is set, add this line to the cleaned list and reset the flag\n",
    "            cleaned_lines.append(line)\n",
    "            add_next_line = False\n",
    "    # Join the cleaned lines back into a single string\n",
    "    cleaned_text = '\\n\\n'.join(cleaned_lines)\n",
    "    return cleaned_text\n",
    "\n",
    "### Bot/Woz's data\n",
    "conversations, labels = load_data_oum(label='after')\n",
    "utterances = [get_utterances_oum(c) for c in conversations]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b95349-3692-4e3a-9943-4f360301dbf1",
   "metadata": {},
   "source": [
    "#### 1.2. Run the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987304f3-ea92-41e8-810a-3daaecc7be9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 1.2.1. Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5619a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4o_fewshot_prompting(conversations, labels, n_shot, seed=123):\n",
    "    # Prepare the CSV file\n",
    "    with open(f'./gpt4o_nshot_baseline_results/gpt4o_{n_shot}shot_performance_oum_seed={seed}.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Conversation', 'True Label', 'Predicted Label'])\n",
    "\n",
    "        # Iterate through all conversations\n",
    "        for i, (conversation, label) in enumerate(zip(conversations, labels)):\n",
    "            print('\\n\\nINSTANCE: ', i)\n",
    "            # Select N random examples (excluding the current one)\n",
    "            examples = random.sample(list(zip(conversations[:i] + conversations[i+1:], \n",
    "                                              labels[:i] + labels[i+1:])), n_shot)\n",
    "            \n",
    "            # Construct the prompt\n",
    "            # prompt = \"Given a conversation, predict the score after the conversation. The score corresponds to a post-conversation open-mindedness, obtained by human participants rating their open-mindedness towards the opposing stance on a given controversial topic (veganism, Brexit or vaccination) on a 7-point Likert scale, assessing whether they believe people with opposing views to theirs have good reasons. The score is obtained after the human participant had a discussion with another entity (another human or a chatbot disguised with Wizard of Oz approach). Note, you must only output a single score between 1 to 7 and nothing else.\\n\\n\"\n",
    "            prompt = \"\"\"Predict the post-conversation open-mindedness score based on the given conversation. This score reflects a participant's self-reported level of open-mindedness towards opposing views on a controversial topic (veganism, Brexit, or vaccination) after discussing with another entity (human or disguised chatbot).\n",
    "            \n",
    "More specifically, the score is based on a 7-point Likert scale, where participants rate whether they believe people with opposing views to theirs have good reasons.\n",
    "\n",
    "Key points:\n",
    "1. Score range: 1 to 7\n",
    "2. Higher scores indicate greater open-mindedness\n",
    "3. Scores were self-reported after the conversation\n",
    "4. Topics: veganism, Brexit, or vaccination\n",
    "\n",
    "You will see example conversation/s with their corresponding scores before predicting the score for a new conversation.\n",
    "\n",
    "Important: Your response should consist of a single number, with no additional text.\n",
    "\n",
    "Examples:\n",
    "            \"\"\"\n",
    "            for ex_conv, ex_label in examples:\n",
    "                prompt += f\"CONVERSATION:\\n{ex_conv}\\n\\nSCORE: {ex_label}\\n\\n\\n\\n\"\n",
    "            \n",
    "            # prompt += f\"Now, predict the score for this conversation:\\n{conversation}\\nScore:\"\n",
    "            prompt += f\"CONVERSATION:\\n{conversation}\\n\\nSCORE:\"\n",
    "            # print(prompt)\n",
    "\n",
    "            # Call GPT-4o\n",
    "            ok = False\n",
    "            while not ok:\n",
    "                try:\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model='gpt-4o',\n",
    "                        messages=[\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        max_tokens=10,\n",
    "                        temperature=0\n",
    "                    )\n",
    "    \n",
    "                    # Extract the predicted label\n",
    "                    predicted_label = float(response.choices[0].message.content.strip())\n",
    "                    ok = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred: {e}\")\n",
    "                    predicted_label = \"Error\"\n",
    "                    time.sleep(5)\n",
    "            print('\\nPREDICTED SCORE: ', predicted_label)\n",
    "            # Write to CSV\n",
    "            csvwriter.writerow([conversation, label, predicted_label])\n",
    "\n",
    "            # Flush the CSV file to save progress\n",
    "            csvfile.flush()\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1} conversations\")\n",
    "\n",
    "for n_shot in [1, 2, 5, 10, 20]:\n",
    "    gpt4o_fewshot_prompting(conversations, labels, n_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907f933-d214-41d1-9f9f-209f31ab5180",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 1.2.2. Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in [42, 123, 456]:\n",
    "    random.seed(seed)\n",
    "    def gpt4o_zero_prompting(conversations, labels):\n",
    "        # Prepare the CSV file\n",
    "        with open(f'./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_oum_seed={seed}.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(['Conversation', 'True Label', 'Predicted Label'])\n",
    "\n",
    "            # Iterate through all conversations\n",
    "            for i, (conversation, label) in enumerate(zip(conversations, labels)):\n",
    "                print('\\n\\nINSTANCE: ', i)\n",
    "\n",
    "                \n",
    "                # Construct the prompt\n",
    "                # prompt = \"Given a conversation, predict the score after the conversation. The score corresponds to a post-conversation open-mindedness, obtained by human participants rating their open-mindedness towards the opposing stance on a given controversial topic (veganism, Brexit or vaccination) on a 7-point Likert scale, assessing whether they believe people with opposing views to theirs have good reasons. The score is obtained after the human participant had a discussion with another entity (another human or a chatbot disguised with Wizard of Oz approach). Note, you must only output a single score between 1 to 7 and nothing else.\\n\\n\"\n",
    "                prompt = \"\"\"Predict the post-conversation open-mindedness score based on the given conversation. This score reflects a participant's self-reported level of open-mindedness towards opposing views on a controversial topic (veganism, Brexit, or vaccination) after discussing with another entity (human or disguised chatbot).\n",
    "                \n",
    "More specifically, the score is based on a 7-point Likert scale, where participants rate whether they believe people with opposing views to theirs have good reasons.\n",
    "\n",
    "Key points:\n",
    "1. Score range: 1 to 7\n",
    "2. Higher scores indicate greater open-mindedness\n",
    "3. Scores were self-reported after the conversation\n",
    "4. Topics: veganism, Brexit, or vaccination\n",
    "\n",
    "Important: Your response should consist of a single number, with no additional text.\n",
    "\n",
    "                \"\"\"\n",
    "                # prompt += f\"Now, predict the score for this conversation:\\n{conversation}\\nScore:\"\n",
    "                prompt += f\"CONVERSATION:\\n{conversation}\\n\\nSCORE:\"\n",
    "                # print(prompt)\n",
    "\n",
    "                # Call GPT-4o\n",
    "                ok = False\n",
    "                while not ok:\n",
    "                    try:\n",
    "                        response = openai.ChatCompletion.create(\n",
    "                            model='gpt-4o',\n",
    "                            messages=[\n",
    "                                {\"role\": \"user\", \"content\": prompt}\n",
    "                            ],\n",
    "                            max_tokens=3,\n",
    "                            temperature=0\n",
    "                        )\n",
    "        \n",
    "                        # Extract the predicted label\n",
    "                        predicted_label = float(response.choices[0].message.content.strip())\n",
    "                        ok = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error occurred: {e}\")\n",
    "                        predicted_label = \"Error\"\n",
    "                        time.sleep(5)\n",
    "                print('\\nPREDICTED SCORE: ', predicted_label)\n",
    "                # Write to CSV\n",
    "                csvwriter.writerow([conversation, label, predicted_label])\n",
    "\n",
    "                # Flush the CSV file to save progress\n",
    "                csvfile.flush()\n",
    "\n",
    "                # Print progress\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"Processed {i + 1} conversations\")\n",
    "\n",
    "    # Assuming conversations and labels are already loaded\n",
    "    gpt4o_zero_prompting(conversations, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35bee05-b441-46fb-8712-c89ace98babf",
   "metadata": {},
   "source": [
    "#### 1.3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_oum_seed=42.csv')\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_oum_seed=123.csv')\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_oum_seed=456.csv')\n",
    "# Calculate Spearman correlation\n",
    "spearman_corr, _ = stats.spearmanr(df['True Label'], df['Predicted Label'])\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = np.mean(np.abs(df['True Label'] - df['Predicted Label']))\n",
    "\n",
    "# Print results\n",
    "print(f\"Spearman correlation: {spearman_corr}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ad1a5c5-cfe9-46d3-83e3-b82b4a978887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: vaccination\n",
      "Spearman correlation: 0.07339702140983752\n",
      "Mean Absolute Error: 1.8095238095238095\n",
      "\n",
      "\n",
      "Topic: Brexit\n",
      "Spearman correlation: 0.40462835692859317\n",
      "Mean Absolute Error: 1.6979865771812082\n",
      "\n",
      "\n",
      "Topic: veganism\n",
      "Spearman correlation: 0.21971562622189889\n",
      "Mean Absolute Error: 2.1323529411764706\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def conv2topic(c):\n",
    "  if 'Brexit' in c:\n",
    "    return 'Brexit'\n",
    "  elif 'veganism' in c or 'vegan' in c:\n",
    "    return 'veganism'\n",
    "  elif 'vaccination' in c or 'vaccine' in c:\n",
    "    return 'vaccination'\n",
    "  else:\n",
    "    raise 'NO TOPIC WAS CHOSEN'\n",
    "\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_oum_seed=42.csv')\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_oum_seed=123.csv')\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_oum_seed=456.csv')\n",
    "\n",
    "# Convert conversation column to topics\n",
    "df['Topic'] = df['Conversation'].apply(conv2topic)\n",
    "\n",
    "# Split the data based on topics\n",
    "topics = df['Topic'].unique()\n",
    "\n",
    "# Calculate performance metrics for each topic\n",
    "for topic in topics:\n",
    "    topic_df = df[df['Topic'] == topic]\n",
    "    \n",
    "    # Calculate Spearman correlation\n",
    "    spearman_corr, _ = stats.spearmanr(topic_df['True Label'], topic_df['Predicted Label'])\n",
    "    \n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = np.mean(np.abs(topic_df['True Label'] - topic_df['Predicted Label']))\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(f\"Spearman correlation: {spearman_corr}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076ee00-0ad9-4ead-af92-dccbb2a9fb0a",
   "metadata": {},
   "source": [
    "### 2. Wikitactics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96facb-bd6b-491c-b5d7-511605eddeaf",
   "metadata": {},
   "source": [
    "#### 2.1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ec69eca-4e97-43b1-ab1a-935b6f82cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def load_data_wikitac():\n",
    "    with open('./wikitactics.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    conversations = []\n",
    "    utterances_cleaned = []\n",
    "    labels = []\n",
    "    # Extract conversations/disputes for ESCALATED disputes\n",
    "    for dispute in data:\n",
    "        users = list()\n",
    "        conversation = ''\n",
    "        utt_cleaned = ''\n",
    "        for utterance in dispute['utterances']:\n",
    "            username = utterance['username']\n",
    "            text = utterance['text']\n",
    "            conversation += f\"<user_id={username}>\\n{text}\\n\" \n",
    "            utt_cleaned += text + '\\n\\n'\n",
    "        conversations.append(conversation)\n",
    "        utterances_cleaned.append(utt_cleaned)\n",
    "        labels.append(dispute['escalation_label'])\n",
    "\n",
    "    return conversations, utterances_cleaned, labels\n",
    "\n",
    "conversations, _, labels = load_data_wikitac()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67ce7e-d9b2-4dbf-81a8-e461a323fe74",
   "metadata": {},
   "source": [
    "#### 2.2. Run the Expriment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c07a20-166d-4ac7-a533-a6442a9b5f23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.2.1. Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9304622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4o_fewshot_prompting(conversations, labels, n_shot, seed=123):\n",
    "    # Prepare the CSV file\n",
    "    random.seed(seed)\n",
    "    with open(f'./gpt4o_nshot_baseline_results/gpt4o_{n_shot}shot_performance_wikitactics_seed={seed}.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Conversation', 'True Label', 'Predicted Label'])\n",
    "\n",
    "        # Iterate through all conversations\n",
    "        for i, (conversation, label) in enumerate(zip(conversations, labels)):\n",
    "            print('\\n\\nINSTANCE: ', i)\n",
    "            # Select 20 random examples (excluding the current one)\n",
    "            examples = random.sample(list(zip(conversations[:i] + conversations[i+1:], \n",
    "                                              labels[:i] + labels[i+1:])), n_shot)\n",
    "            \n",
    "            # Construct the prompt\n",
    "            # prompt = \"Given a conversation, predict the score after the conversation. The score corresponds to a post-conversation open-mindedness, obtained by human participants rating their open-mindedness towards the opposing stance on a given controversial topic (veganism, Brexit or vaccination) on a 7-point Likert scale, assessing whether they believe people with opposing views to theirs have good reasons. The score is obtained after the human participant had a discussion with another entity (another human or a chatbot disguised with Wizard of Oz approach). Note, you must only output a single score between 1 to 7 and nothing else.\\n\\n\"\n",
    "            prompt = \"\"\"When there is a content accuracy dispute or a violation of Wikipedia’s neutral point of view policy, an editor can create a ‘dispute’ for a potentially problematic article, in which they provide their rationale, vote and discuss them with others. If the editors cannot reach an agreement, they can request mediation from a community volunteer, which is considered an escalation. \n",
    "\n",
    "Below you have one dispute sourced from the Wikipedia Talk Page for which you have to predict escalation. We model this as a binary classification task by taking a textual dialogue as input to predict whether the dispute was eventually ‘escalated’ (y=1) or ‘non-escalated’ (y=0).\n",
    "            \n",
    "You will see example disputes/s with their corresponding escalation outcome before predicting the escalation for a new dispute.\n",
    "\n",
    "Important: Your prediction should consist of a single number between 0 and 1, with no additional text.\n",
    "\n",
    "Examples:\n",
    "\"\"\"\n",
    "            for ex_conv, ex_label in examples:\n",
    "                prompt += f\"DISPUTE:\\n{ex_conv}\\n\\nESCALATION: {ex_label}\\n\\n\\n\\n\"\n",
    "            \n",
    "            # prompt += f\"Now, predict the score for this conversation:\\n{conversation}\\nScore:\"\n",
    "            prompt += f\"DISPUTE:\\n{conversation}\\n\\nESCALATION:\"\n",
    "            # print(prompt)\n",
    "            # assert 1 == 0\n",
    "            # Call GPT-4o\n",
    "            ok = False\n",
    "            while not ok:\n",
    "                try:\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model='gpt-4o',\n",
    "                        messages=[\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        max_tokens=10,\n",
    "                        temperature=0\n",
    "                    )\n",
    "    \n",
    "                    # Extract the predicted label\n",
    "                    predicted_label = float(response.choices[0].message.content.strip())\n",
    "                    ok = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred: {e}\")\n",
    "                    predicted_label = \"Error\"\n",
    "                    time.sleep(5)\n",
    "            print('\\nPREDICTED SCORE: ', predicted_label)\n",
    "            # Write to CSV\n",
    "            csvwriter.writerow([conversation, label, predicted_label])\n",
    "\n",
    "            # Flush the CSV file to save progress\n",
    "            csvfile.flush()\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1} conversations\")\n",
    "\n",
    "\n",
    "for n_shot in [1, 2, 5, 10, 20]: \n",
    "    gpt4o_fewshot_prompting(conversations, labels, n_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afddbf8-b636-4afa-9996-8b1c02d00648",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.2.2. Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb40fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4o_zeroshot_prompting(conversations, labels, n_shot, seed):\n",
    "    random.seed(seed)\n",
    "    # Prepare the CSV file\n",
    "    with open(f'./gpt4o_nshot_baseline_results/gpt4o_{n_shot}shot_performance_wikitactics_seed={seed}.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Conversation', 'True Label', 'Predicted Label'])\n",
    "\n",
    "        # Iterate through all conversations\n",
    "        for i, (conversation, label) in enumerate(zip(conversations, labels)):\n",
    "            print('\\n\\nINSTANCE: ', i)\n",
    "            \n",
    "            # Construct the prompt\n",
    "            # prompt = \"Given a conversation, predict the score after the conversation. The score corresponds to a post-conversation open-mindedness, obtained by human participants rating their open-mindedness towards the opposing stance on a given controversial topic (veganism, Brexit or vaccination) on a 7-point Likert scale, assessing whether they believe people with opposing views to theirs have good reasons. The score is obtained after the human participant had a discussion with another entity (another human or a chatbot disguised with Wizard of Oz approach). Note, you must only output a single score between 1 to 7 and nothing else.\\n\\n\"\n",
    "            prompt = \"\"\"When there is a content accuracy dispute or a violation of Wikipedia’s neutral point of view policy, an editor can create a ‘dispute’ for a potentially problematic article, in which they provide their rationale, vote and discuss them with others. If the editors cannot reach an agreement, they can request mediation from a community volunteer, which is considered an escalation. \n",
    "\n",
    "Below you have one dispute sourced from the Wikipedia Talk Page for which you have to predict escalation. We model this as a binary classification task by taking a textual dialogue as input to predict whether the dispute was eventually ‘escalated’ (y=1) or ‘non-escalated’ (y=0).\n",
    "            \n",
    "Important: Your prediction should consist of a single number between 0 and 1, with no additional text.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "            # prompt += f\"Now, predict the score for this conversation:\\n{conversation}\\nScore:\"\n",
    "            prompt += f\"DISPUTE:\\n{conversation}\\n\\nESCALATION:\"\n",
    "            # print(prompt)\n",
    "            # assert 1 == 0\n",
    "            # Call GPT-4o\n",
    "            ok = False\n",
    "            while not ok:\n",
    "                try:\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model='gpt-4o',\n",
    "                        messages=[\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        max_tokens=10,\n",
    "                        temperature=0\n",
    "                    )\n",
    "    \n",
    "                    # Extract the predicted label\n",
    "                    predicted_label = float(response.choices[0].message.content.strip())\n",
    "                    ok = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred: {e}\")\n",
    "                    predicted_label = \"Error\"\n",
    "                    time.sleep(5)\n",
    "            print('\\nPREDICTED SCORE: ', predicted_label)\n",
    "            # Write to CSV\n",
    "            csvwriter.writerow([conversation, label, predicted_label])\n",
    "\n",
    "            # Flush the CSV file to save progress\n",
    "            csvfile.flush()\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1} conversations\")\n",
    "\n",
    "# Assuming conversations and labels are already loaded\n",
    "for seed in [42, 123, 456]:\n",
    "    gpt4o_zeroshot_prompting(conversations, labels, 0, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9502e54-107b-4b8d-ab16-ff9288078419",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33b7a0c1-0212-4e80-a9f1-b78cf27e3a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.652442250044084\n",
      "AUPR: 0.5947221649702495\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Read the CSV file\n",
    "\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_wikitactics_seed=42.csv')\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_wikitactics_seed=123.csv')\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_wikitactics_seed=456.csv')\n",
    "# Extract true labels and predicted labels\n",
    "true_labels = df['True Label']\n",
    "predicted_labels = df['Predicted Label']\n",
    "\n",
    "# Calculate AUROC\n",
    "auroc = roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate AUPR\n",
    "aupr = average_precision_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"AUROC: {auroc}\")\n",
    "print(f\"AUPR: {aupr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a53cc-98cf-4549-ae50-b5a4398e4315",
   "metadata": {},
   "source": [
    "### 3. AFD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0329e5-13fe-44a8-bef6-e1807924f9b8",
   "metadata": {},
   "source": [
    "#### 3.1. Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d56c1b0d-10d5-44a1-a2d9-c847832f9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_afd():\n",
    "    # Load the data from the JSON file\n",
    "    with open('afd_1000_randomised_dialogues.json', 'r') as json_file:\n",
    "        data_dict = json.load(json_file)\n",
    "\n",
    "    # Extract the conversations, utterances, and labels from the data dictionary\n",
    "    conversations = data_dict['conversations']\n",
    "    utterances = data_dict['utterances']\n",
    "    labels = data_dict['labels']\n",
    "    labels = [1 if i == 0 else 0 for i in labels]\n",
    "    return conversations, utterances, labels\n",
    "\n",
    "conversations, _, labels = load_data_afd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c3225-2de5-44d2-b850-067a2d0814b1",
   "metadata": {},
   "source": [
    "#### 3.2. Run the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff80b8-acb9-45eb-a364-13e4ff439afb",
   "metadata": {},
   "source": [
    "##### 3.2.1. Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123) \n",
    "def gpt4o_fewshot_prompting(conversations, labels, n_shot):\n",
    "    # Prepare the CSV file\n",
    "    with open(f'gpt4o_{n_shot}shot_performance_afd_seed=123.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Conversation', 'True Label', 'Predicted Label'])\n",
    "\n",
    "        # Iterate through all conversations\n",
    "        for i, (conversation, label) in enumerate(zip(conversations, labels)):\n",
    "            print('\\n\\nINSTANCE: ', i)\n",
    "            # Select N random examples (excluding the current one)\n",
    "            examples = random.sample(list(zip(conversations[:i] + conversations[i+1:], \n",
    "                                              labels[:i] + labels[i+1:])), n_shot)\n",
    "            \n",
    "            # Construct the prompt\n",
    "            prompt = \"\"\"I will show you a debate where Wikipedia editors discuss whether to delete certain Wikipedia articles. \n",
    "            \n",
    "The debate (which lasts at least seven days) is initiated by a Wikipedia editor, who provides reasoning for nominating an article for deletion and then discusses it with other editors. Administrators then aggregate the discussion to decide the outcome. \n",
    "\n",
    "For our analysis, we model a binary classification task by including articles labelled as ‘Delete’ (y=0) and ‘Keep’ (y=1).\n",
    "            \n",
    "You will see example debate/s with their corresponding outcome before predicting whether a new given dispute will be kept or deleted.\n",
    "\n",
    "Important: Your prediction should consist of a single number, with no additional text.\n",
    "\n",
    "Examples:\n",
    "\"\"\"\n",
    "            for ex_conv, ex_label in examples:\n",
    "                prompt += f\"DEBATE:\\n{ex_conv}\\n\\nOUTCOME: {ex_label}\\n\\n\\n\\n\"\n",
    "            \n",
    "            prompt += f\"DEBATE:\\n{conversation}\\n\\nOUTCOME:\"\n",
    "            ok = False\n",
    "            while not ok:\n",
    "                try:\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model='gpt-4o',\n",
    "                        messages=[\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        max_tokens=10,\n",
    "                        temperature=0\n",
    "                    )\n",
    "    \n",
    "                    # Extract the predicted label\n",
    "                    predicted_label = float(response.choices[0].message.content.strip())\n",
    "                    ok = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred: {e}\")\n",
    "                    predicted_label = \"Error\"\n",
    "                    time.sleep(5)\n",
    "            print('\\nPREDICTED SCORE: ', predicted_label)\n",
    "            # Write to CSV\n",
    "            csvwriter.writerow([conversation, label, predicted_label])\n",
    "\n",
    "            # Flush the CSV file to save progress\n",
    "            csvfile.flush()\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1} conversations\")\n",
    "\n",
    "for n_shot in [1, 2, 5, 10, 20]:\n",
    "    gpt4o_fewshot_prompting(conversations, labels, n_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb69148-cd4f-4b2d-a447-b6af7142bc84",
   "metadata": {},
   "source": [
    "##### 3.2.2. Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4o_0shot_prompting(conversations, labels, nshot, seed):\n",
    "    # Prepare the CSV file\n",
    "    random.seed(seed)\n",
    "    with open(f'./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_afd_seed={seed}.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Conversation', 'True Label', 'Predicted Label'])\n",
    "\n",
    "        # Iterate through all conversations\n",
    "        for i, (conversation, label) in enumerate(zip(conversations, labels)):\n",
    "            print('\\n\\nINSTANCE: ', i)\n",
    "\n",
    "\n",
    "            prompt = \"\"\"Below is a discussion between different individuals who discuss whether to delete a given Wikipedia article. \n",
    "            \n",
    "The debate is initiated by an individual, who provides reasoning for nominating an article for deletion and then discusses it with other individuals. Administrators will in the end aggregate the discussion to decide whether to keep or delete the article. \n",
    "\n",
    "This is modelled as a binary classification task, taking an input discussion to predict if the outcome will be ‘Keep’ (y=1) or ‘Delete’ (y=0).\n",
    "            \n",
    "Important: Your prediction should consist of a single number, with no additional text.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "            prompt += f\"DEBATE:\\n{conversation}\\n\\nOUTCOME:\"\n",
    "\n",
    "            ok = False\n",
    "            while not ok:\n",
    "                try:\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model='gpt-4o',\n",
    "                        messages=[\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        max_tokens=3,\n",
    "                        temperature=0\n",
    "                    )\n",
    "    \n",
    "                    # Extract the predicted label\n",
    "                    predicted_label = float(response.choices[0].message.content.strip())\n",
    "                    ok = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred: {e}\")\n",
    "                    predicted_label = \"Error\"\n",
    "                    time.sleep(5)\n",
    "            print('\\nPREDICTED SCORE: ', predicted_label)\n",
    "            # Write to CSV\n",
    "            csvwriter.writerow([conversation, label, predicted_label])\n",
    "\n",
    "            # Flush the CSV file to save progress\n",
    "            csvfile.flush()\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1} conversations\")\n",
    "\n",
    "# Assuming conversations and labels are already loaded\n",
    "for seed in [42, 123, 456]:\n",
    "    gpt4o_0shot_prompting(conversations, labels, 0, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2a27d-cb16-465e-a7c5-4d518e6eaa4e",
   "metadata": {},
   "source": [
    "#### 3.3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce584815-e865-4bee-8cb0-c5ab719f0779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.9169574129396628\n",
      "AUPR: 0.8447070920622718\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_afd_seed=42.csv')\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_0shot_performance_afd_seed=123.csv')\n",
    "df = pd.read_csv('./gpt4o_nshot_baseline_results/gpt4o_5shot_performance_afd_seed=456.csv')\n",
    "# Extract true labels and predicted labels\n",
    "true_labels = df['True Label']\n",
    "predicted_labels = df['Predicted Label']\n",
    "\n",
    "# Calculate AUROC\n",
    "auroc = roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate AUPR\n",
    "aupr = average_precision_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"AUROC: {auroc}\")\n",
    "print(f\"AUPR: {aupr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7988111-b37e-4bbe-b831-70af4c9373db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.909, 0.003, 0.855, 0.005)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Provided values for calculation\n",
    "auroc_values = [0.9057055321747094, 0.9119038792821473, 0.9080474279857824]\n",
    "aupr_values = [0.85086818389468, 0.8602636445462729, 0.8541563197681251]\n",
    "\n",
    "# Calculate mean and standard deviation for AUROC\n",
    "auroc_mean = np.mean(auroc_values)\n",
    "auroc_sd = np.std(auroc_values, ddof=1)\n",
    "\n",
    "# Calculate mean and standard deviation for AUPR\n",
    "aupr_mean = np.mean(aupr_values)\n",
    "aupr_sd = np.std(aupr_values, ddof=1)\n",
    "\n",
    "# Round the results to three decimal places\n",
    "auroc_mean_rounded = round(auroc_mean, 3)\n",
    "auroc_sd_rounded = round(auroc_sd, 3)\n",
    "aupr_mean_rounded = round(aupr_mean, 3)\n",
    "aupr_sd_rounded = round(aupr_sd, 3)\n",
    "\n",
    "auroc_mean_rounded, auroc_sd_rounded, aupr_mean_rounded, aupr_sd_rounded\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
