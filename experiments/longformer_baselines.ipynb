{"cells":[{"cell_type":"code","execution_count":null,"id":"0dbcf2c5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":0,"status":"ok","timestamp":1715854281947,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-60},"id":"0dbcf2c5","outputId":"83d81038-d599-430a-fe14-780d133f60fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n","Collecting accelerate>=0.21.0 (from transformers[torch])\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch->transformers[torch])\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Collecting transformers_interpret\n","  Downloading transformers_interpret-0.10.0-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m959.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting captum>=0.3.1 (from transformers_interpret)\n","  Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ipython<8.0.0,>=7.31.1 in /usr/local/lib/python3.10/dist-packages (from transformers_interpret) (7.34.0)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from transformers_interpret) (4.40.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum>=0.3.1->transformers_interpret) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum>=0.3.1->transformers_interpret) (1.25.2)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum>=0.3.1->transformers_interpret) (2.2.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum>=0.3.1->transformers_interpret) (4.66.4)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (67.7.2)\n","Collecting jedi>=0.16 (from ipython<8.0.0,>=7.31.1->transformers_interpret)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (3.0.43)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (4.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (0.20.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (0.4.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=3.0.0->transformers_interpret) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=3.0.0->transformers_interpret) (4.11.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.2.13)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->captum>=0.3.1->transformers_interpret) (12.4.127)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (2.8.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (2024.2.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum>=0.3.1->transformers_interpret) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum>=0.3.1->transformers_interpret) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum>=0.3.1->transformers_interpret) (1.3.0)\n","Installing collected packages: jedi, captum, transformers_interpret\n","Successfully installed captum-0.7.0 jedi-0.19.1 transformers_interpret-0.10.0\n","Collecting git+https://github.com/allenai/longformer.git\n","  Cloning https://github.com/allenai/longformer.git to /tmp/pip-req-build-1i1qb01e\n","  Running command git clone --filter=blob:none --quiet https://github.com/allenai/longformer.git /tmp/pip-req-build-1i1qb01e\n","  Resolved https://github.com/allenai/longformer.git to commit caefee668e39cacdece7dd603a0bebf24df6d8ca\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers (from longformer==0.1)\n","  Cloning http://github.com/ibeltagy/transformers.git (to revision longformer_encoder_decoder) to /tmp/pip-install-un__tl_n/transformers_909cea5d42ef4b24a0fc44eac9509dd2\n","  Running command git clone --filter=blob:none --quiet http://github.com/ibeltagy/transformers.git /tmp/pip-install-un__tl_n/transformers_909cea5d42ef4b24a0fc44eac9509dd2\n","  warning: redirecting to https://github.com/ibeltagy/transformers.git/\n","  Running command git checkout -b longformer_encoder_decoder --track origin/longformer_encoder_decoder\n","  warning: redirecting to https://github.com/ibeltagy/transformers.git/\n","  Switched to a new branch 'longformer_encoder_decoder'\n","  Branch 'longformer_encoder_decoder' set up to track remote branch 'longformer_encoder_decoder' from 'origin'.\n","  Resolved http://github.com/ibeltagy/transformers.git to commit 52d6236dc15ad5142b4146ff74d2ec973fa3da22\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning (from longformer==0.1)\n","  Cloning http://github.com/ibeltagy/pytorch-lightning.git (to revision v0.8.5_fixes) to /tmp/pip-install-un__tl_n/pytorch-lightning_fbcd92888b99470a89e3469589ae7a82\n","  Running command git clone --filter=blob:none --quiet http://github.com/ibeltagy/pytorch-lightning.git /tmp/pip-install-un__tl_n/pytorch-lightning_fbcd92888b99470a89e3469589ae7a82\n","  warning: redirecting to https://github.com/ibeltagy/pytorch-lightning.git/\n","  Running command git checkout -b v0.8.5_fixes --track origin/v0.8.5_fixes\n","  warning: redirecting to https://github.com/ibeltagy/pytorch-lightning.git/\n","  Switched to a new branch 'v0.8.5_fixes'\n","  Branch 'v0.8.5_fixes' set up to track remote branch 'v0.8.5_fixes' from 'origin'.\n","  Resolved http://github.com/ibeltagy/pytorch-lightning.git to commit 7ed5d849a0c76fa2199162f0283507e36601ded6\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from longformer==0.1) (2.2.1+cu121)\n","Collecting tensorboardX (from longformer==0.1)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting test-tube==0.7.5 (from longformer==0.1)\n","  Downloading test_tube-0.7.5.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting nlp (from longformer==0.1)\n","  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rouge_score (from longformer==0.1)\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from test-tube==0.7.5->longformer==0.1) (2.0.3)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from test-tube==0.7.5->longformer==0.1) (1.25.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from test-tube==0.7.5->longformer==0.1) (2.31.6)\n","Requirement already satisfied: tensorboard>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from test-tube==0.7.5->longformer==0.1) (2.15.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from test-tube==0.7.5->longformer==0.1) (0.18.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->longformer==0.1) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->longformer==0.1) (12.4.127)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from nlp->longformer==0.1) (14.0.2)\n","Collecting dill (from nlp->longformer==0.1)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from nlp->longformer==0.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from nlp->longformer==0.1) (4.66.4)\n","Collecting xxhash (from nlp->longformer==0.1)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (6.0.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score->longformer==0.1) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score->longformer==0.1) (3.8.1)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score->longformer==0.1) (1.16.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX->longformer==0.1) (24.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->longformer==0.1) (3.20.3)\n","Collecting tokenizers==0.8.1.rc2 (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1)\n","  Downloading tokenizers-0.8.1rc2.tar.gz (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (2023.12.25)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (0.1.99)\n","Collecting sacremoses (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1)\n","  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.3.0->test-tube==0.7.5->longformer==0.1) (9.4.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.3->test-tube==0.7.5->longformer==0.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.3->test-tube==0.7.5->longformer==0.1) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.3->test-tube==0.7.5->longformer==0.1) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp->longformer==0.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp->longformer==0.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp->longformer==0.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp->longformer==0.1) (2024.2.2)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (1.63.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (3.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->longformer==0.1) (2.1.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score->longformer==0.1) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score->longformer==0.1) (1.4.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->longformer==0.1) (1.3.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (1.3.1)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=1.15.0->test-tube==0.7.5->longformer==0.1) (3.2.2)\n","Building wheels for collected packages: longformer, test-tube, pytorch-lightning, rouge_score, transformers, tokenizers\n","  Building wheel for longformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for longformer: filename=longformer-0.1-py3-none-any.whl size=548844 sha256=0a7b4e5fb9264af632c981d237c0d734f4b0f441f12f665e9fcd94e34ea96569\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-wg2zf9ec/wheels/40/53/f6/aeb3075613ec6e2bde1f8b8b0f7cf3b8da8eeb3415371134d0\n","  Building wheel for test-tube (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for test-tube: filename=test_tube-0.7.5-py3-none-any.whl size=25328 sha256=8bceca3660f0ac7c17ef1e3758cbbe48d526f646d84c7970fd65ecc5e1369d1f\n","  Stored in directory: /root/.cache/pip/wheels/28/d4/8b/1aeb47c0dedd931b8e6aec55a8091864a69ac6f0adc5b12ea9\n","  Building wheel for pytorch-lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorch-lightning: filename=pytorch_lightning-0.8.5-py3-none-any.whl size=313127 sha256=9e7971a631aec7bb94711df87c8e49d40d70d3c82b18881d5f07ca6263e38bbf\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-wg2zf9ec/wheels/c0/3c/cd/529986c29d4f94a139f330bcedfedb9ce15d823b0a60c068f9\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=95b4cc21dc1a94f9730eca421b2269f25488c5f813ffb935fcee3224f0e5b9e3\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-3.1.0-py3-none-any.whl size=884306 sha256=04fec8dd5a7302a6d5044f5eb20b0f7ea1e5af5d74336d9cd72eed771cd8e9d8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-wg2zf9ec/wheels/c5/a5/cc/b92ba849f87344f6105a886066b2a56df0ca46e7b5daba28e7\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully built longformer test-tube pytorch-lightning rouge_score transformers\n","Failed to build tokenizers\n","\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0mCollecting convokit\n","  Downloading convokit-3.0.0.tar.gz (183 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.2/183.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n","Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.0.3)\n","Collecting msgpack-numpy>=0.4.3.2 (from convokit)\n","  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.2)\n","Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n","Collecting dill>=0.2.9 (from convokit)\n","  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n","Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.4.2)\n","Collecting clean-text>=0.6.0 (from convokit)\n","  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n","Collecting unidecode>=1.1.1 (from convokit)\n","  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.4)\n","Collecting pymongo>=4.0 (from convokit)\n","  Downloading pymongo-4.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (670 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.0/670.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.1)\n","Collecting dnspython>=1.16.0 (from convokit)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\n","  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (24.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n","Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2023.12.25)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2024.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.5.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.9.4)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (6.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.7.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (67.7.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.4.0)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.18.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2024.2.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.1.4)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.3.5->convokit) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.1.1)\n","Building wheels for collected packages: convokit, emoji\n","  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for convokit: filename=convokit-3.0.0-py3-none-any.whl size=216707 sha256=5d96003b59f7ed40e5973b16280e4781410739155ca565c1e85fa9c3f7a72eaa\n","  Stored in directory: /root/.cache/pip/wheels/c4/89/8c/2677fdb888588b6f93cb6ac86bdfb020f1f1c33e0d5525b231\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171034 sha256=d9834f859628446901d33967240dda0c8c7b0c9235757f2318c2aeead7971362\n","  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n","Successfully built convokit emoji\n","Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dnspython, dill, pymongo, clean-text, convokit\n","Successfully installed clean-text-0.6.0 convokit-3.0.0 dill-0.3.8 dnspython-2.6.1 emoji-1.7.0 ftfy-6.2.0 msgpack-numpy-0.4.8 pymongo-4.7.2 unidecode-1.3.8\n"]}],"source":["\"\"\"\n","!pip install transformers[torch]\n","!pip install accelerate\n","!pip install transformers_interpret\n","!pip install git+https://github.com/allenai/longformer.git\n","!pip install convokit\n","\"\"\""]},{"cell_type":"code","execution_count":null,"id":"LM8_wwR_vLeR","metadata":{"id":"LM8_wwR_vLeR"},"outputs":[],"source":["import random\n","import torch\n","import json\n","import random\n","import torch\n","from torch.nn import L1Loss\n","from sklearn.metrics import mean_absolute_error\n","import numpy as np\n","from sklearn.metrics import mean_squared_error\n","from scipy import stats\n","from transformers import (\n","    Trainer,\n","    TrainingArguments,\n","    RobertaForSequenceClassification,\n","    RobertaTokenizerFast,\n","    LongformerForSequenceClassification,\n","    LongformerTokenizerFast,\n","    EarlyStoppingCallback)\n","\n","class conv_data_loader(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx]).to(device)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","\n","# Define the compute_metrics function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    rmse = mean_squared_error(labels, predictions, squared=False)\n","    return {\"rmse\": rmse}\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    mae = mean_absolute_error(labels, predictions)\n","    return {\"mae\": mae}"]},{"cell_type":"markdown","id":"5DUosDTrGDzk","metadata":{"id":"5DUosDTrGDzk"},"source":["# 1. Reading Data"]},{"cell_type":"markdown","id":"3gdaCuidi3g2","metadata":{"id":"3gdaCuidi3g2"},"source":["## 1.1. OUM dataset"]},{"cell_type":"code","execution_count":null,"id":"GzTjJfHnECk5","metadata":{"id":"GzTjJfHnECk5"},"outputs":[],"source":["# Define the OUMDataset class\n","class OUMDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx]).to(device)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Load the data\n","def load_data_oum(label='after'):\n","    final_convs = []\n","    final_labels = []\n","    final_experience_features = []\n","    wizards_data = []\n","    moral_foundations = [\"care\", \"fairness\", \"liberty\", \"loyalty\", \"authority\", \"sanctity\", \"none\"]\n","    input_files = {\"wizards\": \"wizards_dialogues.json\", \"final_argubot\": \"argubot_final_exp.json\",\n","                   \"models_dialogues\": \"models_dialogues.json\"}\n","    dials_with_scores = {\"wizards\": {}, \"final_argubot\": {}, \"models_dialogues\": {}}\n","\n","\n","    for key in input_files:\n","        input_file = input_files[key]\n","        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        for d in data:\n","            is_wiki = False\n","            for m in d[\"messages\"]:\n","                if 'model' in m and (m['model'] == 'wikibot' or m['model'] == 'controlbot'):\n","                    is_wiki = True\n","                    break\n","            if is_wiki:\n","                continue\n","            yes_no = 'none'\n","            k = 'Did you vote for (Leave) or against (Remain) Brexit in the 2016 UK referendum?'\n","            if k in d['participant_info']:\n","                if d['participant_info'][k].lower() == 'against (remain)':\n","                    yes_no = 'no'\n","                elif d['participant_info'][k].lower() == 'for (leave)':\n","                    yes_no = 'yes'\n","                else:\n","                    yes_no = 'none'\n","\n","            k = 'In the referendum on whether the UK should remain a member of the EU (BREXIT), how did you vote?'\n","            if k in d['participant_info']:\n","                if d['participant_info'][k].lower() == 'remain (against brexit)':\n","                    yes_no = 'no'\n","                elif d['participant_info'][k].lower() == 'leave (for brexit)':\n","                    yes_no = 'yes'\n","                else:\n","                    yes_no = 'none'\n","            k = 'Have you had at least one dose of an approved Covid-19 vaccine?'\n","            if k in d['participant_info']:\n","                if d['participant_info'][k].lower() == 'yes':\n","                    yes_no = 'yes'\n","                elif d['participant_info'][k].lower() == 'no':\n","                    yes_no = 'no'\n","            k = 'Are you a vegan?'\n","            if k in d['participant_info']:\n","                if d['participant_info'][k].lower() == 'yes':\n","                    yes_no = 'yes'\n","                elif d['participant_info'][k].lower() == 'no':\n","                    yes_no = 'no'\n","\n","            if yes_no == 'none':\n","                continue\n","\n","            if 'Questions' in d['participant_info']:\n","                for q in d['participant_info']['Questions']:\n","                    if \"final\" in input_file:\n","                        if label == 'oum':\n","                            continue\n","                        if d['participant_info']['Questions'][q]['after'] == -1:\n","                            continue\n","                    elif d['participant_info']['Questions'][q]['before'] == -1 or d['participant_info']['Questions'][q]['after'] == -1:\n","                        continue\n","                    if 'good reasons' in q.lower():\n","                        if d['topic'] != 'brexit' and 'not' in q.lower() and yes_no == 'no':\n","                            continue\n","                        if d['topic'] != 'brexit' and 'not' not in q.lower() and yes_no == 'yes':\n","                            continue\n","                        if 'leave' in q.lower() and yes_no == 'yes':\n","                            continue\n","                        if 'remain' in q.lower() and yes_no == 'no':\n","                            continue\n","                        if d[\"_id\"] not in dials_with_scores[key]:\n","                            text = ''\n","                            dials_with_scores[key][d[\"_id\"]] = {\"topic\": d[\"topic\"], \"dataset\": key}\n","                            for message in d['messages']:\n","                                if message['role'] == 'admin' or 'modified_argument' not in message:\n","                                    continue\n","\n","                                text = text + '\\n\\n' + '<' + message['role'] + '>' + '\\n' + message['modified_argument']\n","                            dials_with_scores[key][d[\"_id\"]]['text'] = text.strip()\n","                            final_convs.append(text.strip())\n","\n","\n","\n","                    if 'good reasons' in q.lower():\n","                        if False and label == 'oum':\n","                            final_labels.append(float(d['participant_info']['Questions'][q]['after']) - float(d['participant_info']['Questions'][q]['before']))\n","                        else:\n","                            final_labels.append(float(d['participant_info']['Questions'][q]['after']))\n","                        oum = d['participant_info']['Questions'][q]['after'] - d['participant_info']['Questions'][q]['before'] if \"final\" not in input_file else None\n","                        dials_with_scores[key][d[\"_id\"]][\"good_reasons\"] = {\"oum\": oum, \"after\": d['participant_info']['Questions'][q]['after']}\n","                        if 'before' in d['participant_info']['Questions'][q] and d['participant_info']['Questions'][q]['before'] != -1:\n","                            dials_with_scores[key][d[\"_id\"]][\"good_reasons\"]['before'] = d['participant_info']['Questions'][q]['before']\n","                        else:\n","                            dials_with_scores[key][d[\"_id\"]][\"good_reasons\"]['before'] = None\n","\n","\n","\n","    assert len(final_convs) == len(final_labels)\n","    return final_convs, final_labels \n","conversations, labels = load_data_oum()"]},{"cell_type":"markdown","id":"2y9q67IHU1gA","metadata":{"id":"2y9q67IHU1gA"},"source":["## 1.2. Wikitactics"]},{"cell_type":"code","execution_count":null,"id":"lHxASKPYU6D5","metadata":{"id":"lHxASKPYU6D5"},"outputs":[],"source":["import json\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","\n","def load_data_wikitac():\n","    with open('./wikitactics.json') as f:\n","        data = json.load(f)\n","\n","    conversations = []\n","    utterances_cleaned = []\n","    labels = []\n","\n","    for dispute in data:\n","        conversation = []\n","        utt_cleaned = []\n","        users = list()\n","        for utterance in dispute['utterances']:\n","            username = utterance['username']\n","            text = utterance['text']\n","            conversation.append(f\"<{username}>\\n{text}\\n\\n\")\n","            utt_cleaned.append(text)\n","        conversations.append('\\n'.join(conversation))\n","        utterances_cleaned.append('\\n'.join(utt_cleaned))\n","        labels.append(dispute['escalation_label'])\n","\n","    return conversations, labels\n","\n","conversations, labels = load_data_wikitac()"]},{"cell_type":"markdown","id":"PtV8VwMC_XmT","metadata":{"id":"PtV8VwMC_XmT"},"source":["## 1.3. AFD data"]},{"cell_type":"code","execution_count":null,"id":"akDCvsRt_WoL","metadata":{"id":"akDCvsRt_WoL"},"outputs":[],"source":["def load_data_afd():\n","    # Load the data from the JSON file\n","    with open('afd_1000_randomised_dialogues.json', 'r') as json_file:\n","        data_dict = json.load(json_file)\n","\n","    # Extract the conversations, utterances, and labels from the data dictionary\n","    conversations = data_dict['conversations']\n","    utterances = data_dict['utterances']\n","    labels = data_dict['labels']\n","    labels = [1 if i == 0 else 0 for i in labels]\n","    return conversations, utterances, labels\n","\n","conversations, utterances, labels = load_data_afd()"]},{"cell_type":"markdown","id":"eRYOwXt-GGk-","metadata":{"id":"eRYOwXt-GGk-"},"source":["# 2. Training the Model"]},{"cell_type":"markdown","id":"3ee00962-342a-41ad-a80d-4355a63f9ec4","metadata":{"id":"3ee00962-342a-41ad-a80d-4355a63f9ec4"},"source":["## 2.1. Seven-fold Flat Cross Validation"]},{"cell_type":"markdown","id":"7abd3d89-bfd7-4414-939a-4f6166089c41","metadata":{"id":"7abd3d89-bfd7-4414-939a-4f6166089c41"},"source":["### 2.1.1. OUM data"]},{"cell_type":"markdown","id":"2d241e4d-6d9c-4418-9b73-f2bb52ead5cb","metadata":{"id":"2d241e4d-6d9c-4418-9b73-f2bb52ead5cb","jp-MarkdownHeadingCollapsed":true},"source":["#### 2.1.1.1. Fine-tune the whole model"]},{"cell_type":"code","execution_count":null,"id":"e259fc8f-c787-408e-a092-ff395b227fe7","metadata":{"id":"e259fc8f-c787-408e-a092-ff395b227fe7"},"outputs":[],"source":["import random\n","import torch\n","from torch.nn import L1Loss\n","from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import mean_absolute_error\n","from scipy.stats import spearmanr\n","import itertools\n","import time"]},{"cell_type":"code","execution_count":null,"id":"9dbd832b","metadata":{},"outputs":[],"source":["LRs = [2e-5, 1e-4]\n","WARMUP_EPOCHS = [3, 1] \n","MAX_LENGTH = [2048, 4096]\n","\n","hyperparameter_combinations = list(itertools.product(LRs, WARMUP_EPOCHS, MAX_LENGTH))\n","\n","#################################################\n","##### Define Trainer and Args class objects #####\n","#################################################\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False): # Using MAE as the loss function\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","\n","        labels = labels.float()\n","\n","        loss_fct = L1Loss()\n","        mae_loss = loss_fct(logits.squeeze(), labels)\n","\n","        return (mae_loss, outputs) if return_outputs else mae_loss\n","\n","class Args:\n","    model_path = 'models/'\n","    num_labels = 1\n","    num_epochs = 5\n","    train_batch_size = 1\n","    valid_batch_size = 1\n","    model_name = 'allenai/longformer-large-4096' \n","    model_type = 'longformer' \n","    logging_steps = 1  \n","    # save_steps = 300\n","    mode = 'train'\n","    labels = 'after'\n","\n","args = Args()\n","\n","########################################\n","##### Define seeds and model setup #####\n","########################################\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","seed = 42\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(seed)\n","\n","####################################\n","##### Define Data and Training #####\n","####################################\n","x, y = load_data_oum()\n","all_data = list(zip(x, y))\n","random.shuffle(all_data)\n","\n","num_folds = 7 # 7-fold cross validation\n","kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","results_file = \"oum_hyperparameters_results.csv\"\n","with open(results_file, \"w\") as f:\n","    f.write(\"Warmup Steps,Learning Rate,Max Length,Spearman Correlation,MAE\\n\")\n","\n","for i, (lr, warmup_epochs, max_length) in enumerate(hyperparameter_combinations):\n","    print(f'\\n\\n{i}')\n","    print(f\"Training with hyperparameters: Warmup Steps={warmup_epochs}, Learning Rate={lr}, Max Length={max_length}\")\n","\n","    all_predictions = []\n","    all_labels = []\n","    all_val_indexes = [] \n","\n","    for fold, (train_index, val_index) in enumerate(kf.split(x, y), start=1):\n","        print(f\"Fold {fold}\")\n","        train_data = [all_data[i] for i in train_index]\n","        val_data = [all_data[i] for i in val_index]\n","        train_convs, train_labels = zip(*train_data)\n","        val_convs, val_labels = zip(*val_data)\n","\n","        MODEL_CLASSES = {\n","            \"longformer\": (LongformerForSequenceClassification, LongformerTokenizerFast)\n","        }\n","        model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","        model = model_class.from_pretrained(args.model_name, num_labels=args.num_labels).to(device)\n","        tokenizer = tokenizer_class.from_pretrained(args.model_name) \n","        tokenizer.model_max_length = max_length\n","\n","        train_encodings = tokenizer(list(train_convs), truncation=True, padding=True, max_length=max_length)\n","        val_encodings = tokenizer(list(val_convs), truncation=True, padding=True, max_length=max_length)\n","\n","        train_dataset = conv_data_loader(train_encodings, train_labels)\n","        val_dataset = conv_data_loader(val_encodings, val_labels)\n","\n","\n","        warmup_steps = len(train_dataset) // (args.train_batch_size * 32) * warmup_epochs\n","\n","        training_args = TrainingArguments(\n","            output_dir=args.model_path + f'single_model_fold{fold}/',\n","            num_train_epochs=args.num_epochs,\n","            per_device_train_batch_size=args.train_batch_size,\n","            per_device_eval_batch_size=args.valid_batch_size,\n","            warmup_steps=warmup_steps,\n","            learning_rate=lr,\n","            logging_dir=args.model_path + f'single_model_fold{fold}/logs',\n","            load_best_model_at_end=False,\n","            metric_for_best_model='loss',\n","            logging_steps=args.logging_steps,\n","            evaluation_strategy=\"epoch\", \n","            save_strategy=\"epoch\",\n","            save_total_limit=1,\n","            gradient_accumulation_steps=32,  # The true batch size is 32\n","            logging_strategy=\"steps\",\n","            logging_first_step=True,\n","        )\n","\n","        trainer = CustomTrainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            eval_dataset=val_dataset,\n","            tokenizer=tokenizer,\n","            compute_metrics=compute_metrics\n","        )\n","\n","        if args.mode == 'train':\n","            trainer.train()\n","            model.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","            tokenizer.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","            eval_output = trainer.evaluate()\n","            print('Evaluation results:', eval_output)\n","\n","        output = trainer.predict(val_dataset)\n","        predictions = output.predictions.squeeze()\n","        all_predictions.extend(predictions)\n","        all_labels.extend(val_labels)\n","        all_val_indexes.extend(val_index)\n","\n","        # Clean GPU memory\n","        import gc\n","        del model, train_dataset, val_dataset, train_data, val_data, train_convs, val_convs, train_labels, val_labels, tokenizer\n","        del train_encodings, val_encodings, trainer, output, predictions\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        torch.cuda.reset_peak_memory_stats()\n","\n","        model = None\n","        train_dataset = None\n","        val_dataset = None\n","        train_data = None\n","        val_data = None\n","        train_convs = None\n","        val_convs = None\n","        train_labels = None\n","        val_labels = None\n","        train_encodings = None\n","        val_encodings = None\n","        trainer = None\n","        output = None\n","        predictions = None\n","        tokenizer = False\n","\n","    spearman_corr = spearmanr(all_predictions, all_labels)[0]\n","    mae = mean_absolute_error(all_labels, all_predictions)\n","\n","    print(f\"Spearman Correlation: {spearman_corr}\")\n","    print(f\"MAE: {mae}\")\n","\n","    with open(results_file, \"a\") as f:\n","        f.write(f\"{warmup_steps},{lr},{max_length},{spearman_corr},{mae}\\n\")\n","\n","    predictions_labels_file = f\"oum_predictions_labels_warmup={warmup_steps}_lr={lr}_maxlen={max_length}.csv\"\n","    with open(predictions_labels_file, \"w\") as f:\n","        f.write(\"Prediction,Label,Index\\n\")\n","        for pred, label, index in zip(all_predictions, all_labels, all_val_indexes):\n","            f.write(f\"{pred},{label},{index}\\n\")\n","\n","    time.sleep(100) "]},{"cell_type":"code","execution_count":null,"id":"8141ade4","metadata":{},"outputs":[],"source":["#############################################\n","##### Using the optimal hyperparameters #####\n","#############################################\n","LRs = [2e-5]\n","WARMUP_EPOCHS = [1]\n","MAX_LENGTH = [2048]\n","\n","hyperparameter_combinations = list(itertools.product(LRs, WARMUP_EPOCHS, MAX_LENGTH))\n","\n","#################################################\n","##### Define Trainer and Args class objects #####\n","#################################################\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False): # Using MAE as the loss function\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","\n","        labels = labels.float()\n","\n","        loss_fct = L1Loss()\n","        mae_loss = loss_fct(logits.squeeze(), labels)\n","\n","        return (mae_loss, outputs) if return_outputs else mae_loss\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    mae = mean_absolute_error(labels, predictions)\n","    return {\"mae\": mae}\n","\n","class Args:\n","    model_path = 'models/'\n","    num_labels = 1\n","    num_epochs = 5\n","    train_batch_size = 1\n","    valid_batch_size = 1\n","    model_name = 'allenai/longformer-large-4096' \n","    model_type = 'longformer' \n","    logging_steps = 1\n","    mode = 'train'\n","    labels = 'after'\n","\n","args = Args()\n","\n","########################################\n","##### Define seeds and model setup #####\n","########################################\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","for seed in [1, 2, 3]:\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","    if device == 'cuda':\n","        torch.cuda.manual_seed_all(seed)\n","\n","    ####################################\n","    ##### Define Data and Training #####\n","    ####################################\n","    x, y = load_data_oum()\n","    all_data = list(zip(x, y))\n","    random.shuffle(all_data)\n","\n","    num_folds = 7\n","    kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","\n","    for i, (lr, warmup_epochs, max_length) in enumerate(hyperparameter_combinations):\n","        print(f'\\n\\n{i}')\n","        print(f\"Training with hyperparameters: Warmup Steps={warmup_epochs}, Learning Rate={lr}, Max Length={max_length}\")\n","\n","        all_predictions = []\n","        all_labels = []\n","        all_val_convs = []\n","        for fold, (train_index, val_index) in enumerate(kf.split(x, y), start=1):\n","            print(f\"Fold {fold}\")\n","            train_data = [all_data[i] for i in train_index]\n","            val_data = [all_data[i] for i in val_index]\n","            train_convs, train_labels = zip(*train_data)\n","            val_convs, val_labels = zip(*val_data)\n","\n","            MODEL_CLASSES = {\n","                \"longformer\": (LongformerForSequenceClassification, LongformerTokenizerFast)\n","            }\n","            model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","            model = model_class.from_pretrained(args.model_name, num_labels=args.num_labels).to(device)\n","            tokenizer = tokenizer_class.from_pretrained(args.model_name) # do_lower_case=True\n","            tokenizer.model_max_length = max_length\n","\n","            train_encodings = tokenizer(list(train_convs), truncation=True, padding=True, max_length=max_length)\n","            val_encodings = tokenizer(list(val_convs), truncation=True, padding=True, max_length=max_length)\n","\n","            train_dataset = conv_data_loader(train_encodings, train_labels)\n","            val_dataset = conv_data_loader(val_encodings, val_labels)\n","\n","            # Calculate warmup steps based on the current warmup_epochs value\n","            warmup_steps = len(train_dataset) // (args.train_batch_size * 32) * warmup_epochs\n","\n","            training_args = TrainingArguments(\n","                output_dir=args.model_path + f'single_model_fold{fold}/',\n","                num_train_epochs=args.num_epochs,\n","                per_device_train_batch_size=args.train_batch_size,\n","                per_device_eval_batch_size=args.valid_batch_size,\n","                warmup_steps=warmup_steps,\n","                learning_rate=lr,\n","                logging_dir=args.model_path + f'single_model_fold{fold}/logs',\n","                load_best_model_at_end=False,\n","                metric_for_best_model='loss',\n","                logging_steps=args.logging_steps,\n","                evaluation_strategy=\"epoch\", \n","                save_strategy=\"epoch\",\n","                save_total_limit=1,\n","                gradient_accumulation_steps=32, \n","                logging_strategy=\"steps\",\n","                logging_first_step=True,\n","            )\n","\n","            trainer = CustomTrainer(\n","                model=model,\n","                args=training_args,\n","                train_dataset=train_dataset,\n","                eval_dataset=val_dataset,\n","                tokenizer=tokenizer,\n","                compute_metrics=compute_metrics\n","            )\n","\n","            if args.mode == 'train':\n","                trainer.train()\n","                model.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","                tokenizer.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","                eval_output = trainer.evaluate()\n","                print('Evaluation results:', eval_output)\n","\n","            output = trainer.predict(val_dataset)\n","            predictions = output.predictions.squeeze()\n","            all_predictions.extend(predictions)\n","            all_labels.extend(val_labels)\n","            all_val_convs.extend(val_convs)\n","\n","            # Clean GPU memory\n","            import gc\n","            del model, train_dataset, val_dataset, train_data, val_data, train_convs, val_convs, train_labels, val_labels, tokenizer\n","            del train_encodings, val_encodings, trainer, output, predictions\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            torch.cuda.reset_peak_memory_stats()\n","\n","            model = None\n","            train_dataset = None\n","            val_dataset = None\n","            train_data = None\n","            val_data = None\n","            train_convs = None\n","            val_convs = None\n","            train_labels = None\n","            val_labels = None\n","            train_encodings = None\n","            val_encodings = None\n","            trainer = None\n","            output = None\n","            predictions = None\n","            tokenizer = False\n","\n","        spearman_corr = spearmanr(all_predictions, all_labels)[0]\n","        mae = mean_absolute_error(all_labels, all_predictions)\n","\n","        print(f\"Spearman Correlation: {spearman_corr}\")\n","        print(f\"MAE: {mae}\")\n","\n","        predictions_labels_file = f\"oum_lonformer_predictions_seed={seed}.json\"\n","\n","        data = []\n","        for conv, pred, label in zip(all_val_convs, all_predictions, all_labels):\n","            data.append({\n","                \"Conversation\": conv,\n","                \"Prediction\": float(pred),\n","                \"Label\": int(label)  \n","            })\n","\n","        with open(predictions_labels_file, \"w\") as f:\n","            json.dump(data, f, indent=4)\n","\n","        time.sleep(100) "]},{"cell_type":"markdown","id":"648e5ced-53c9-44a7-b567-e34849fff81f","metadata":{"id":"648e5ced-53c9-44a7-b567-e34849fff81f"},"source":["#### 2.1.1.2. Fine-tuning only the last layer"]},{"cell_type":"code","execution_count":null,"id":"733404a4","metadata":{},"outputs":[],"source":["import random\n","import torch\n","from torch.nn import L1Loss\n","from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import mean_absolute_error\n","from scipy.stats import spearmanr\n","import itertools\n","import time\n","import os\n","\n","LRs = [2e-5, 1e-4]\n","WARMUP_EPOCHS = [3, 1] # warmup_steps for 1 or 3 epochs.\n","MAX_LENGTH = [2048, 4096]\n","\n","hyperparameter_combinations = list(itertools.product(LRs, WARMUP_EPOCHS, MAX_LENGTH))\n","\n","#################################################\n","##### Define Trainer and Args class objects #####\n","#################################################\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False): # Using MAE as the loss function\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","\n","        labels = labels.float()\n","\n","        loss_fct = L1Loss()\n","        mae_loss = loss_fct(logits.squeeze(), labels)\n","\n","        return (mae_loss, outputs) if return_outputs else mae_loss\n","\n","class Args:\n","    model_path = 'models/'\n","    num_labels = 1\n","    num_epochs = 5\n","    train_batch_size = 1\n","    valid_batch_size = 1\n","    model_name = 'allenai/longformer-large-4096' \n","    model_type = 'longformer'  \n","    logging_steps = 1 \n","    # save_steps = 300\n","    mode = 'train'\n","    labels = 'after'\n","\n","args = Args()\n","\n","########################################\n","##### Define seeds and model setup #####\n","########################################\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","seed = 42\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(seed)\n","\n","####################################\n","##### Define Data and Training #####\n","####################################\n","x, y = load_data_oum()\n","all_data = list(zip(x, y))\n","random.shuffle(all_data)\n","\n","num_folds = 7 # 7-fold cross validation\n","kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","results_file = \"oum_hyperparameters_results_lf-last-layer.csv\"\n","\n","if not os.path.exists(results_file):\n","    with open(results_file, \"w\") as f:\n","        f.write(\"Warmup Steps,Learning Rate,Max Length,Spearman Correlation,MAE\\n\")\n","\n","for i, (lr, warmup_epochs, max_length) in enumerate(hyperparameter_combinations):\n","    print(f'\\n\\n{i}')\n","    print(f\"Training with hyperparameters: Warmup Steps={warmup_epochs}, Learning Rate={lr}, Max Length={max_length}\")\n","\n","    all_predictions = []\n","    all_labels = []\n","    all_val_indexes = [] # useful to know the original indexes of data points (e.g., do topical analysis).\n","\n","    for fold, (train_index, val_index) in enumerate(kf.split(x, y), start=1):\n","        print(f\"Fold {fold}\")\n","        train_data = [all_data[i] for i in train_index]\n","        val_data = [all_data[i] for i in val_index]\n","        train_convs, train_labels = zip(*train_data)\n","        val_convs, val_labels = zip(*val_data)\n","\n","        MODEL_CLASSES = {\n","            \"longformer\": (LongformerForSequenceClassification, LongformerTokenizerFast)\n","        }\n","        model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","        model = model_class.from_pretrained(args.model_name, num_labels=args.num_labels).to(device)\n","\n","        # Freeze all layers except the last one\n","        for name, param in model.named_parameters():\n","            if 'classifier' not in name:  \n","                param.requires_grad = False\n","\n","        params_to_update = []\n","        for name, param in model.named_parameters():\n","            if param.requires_grad:\n","                params_to_update.append(param)\n","\n","        tokenizer = tokenizer_class.from_pretrained(args.model_name)\n","        tokenizer.model_max_length = max_length\n","\n","        train_encodings = tokenizer(list(train_convs), truncation=True, padding=True, max_length=max_length)\n","        val_encodings = tokenizer(list(val_convs), truncation=True, padding=True, max_length=max_length)\n","\n","        train_dataset = conv_data_loader(train_encodings, train_labels)\n","        val_dataset = conv_data_loader(val_encodings, val_labels)\n","\n","        # Calculate warmup steps based on the current warmup_epochs value\n","        warmup_steps = len(train_dataset) // (args.train_batch_size * 32) * warmup_epochs\n","\n","        training_args = TrainingArguments(\n","            output_dir=args.model_path + f'single_model_fold{fold}/',\n","            num_train_epochs=args.num_epochs,\n","            per_device_train_batch_size=args.train_batch_size,\n","            per_device_eval_batch_size=args.valid_batch_size,\n","            warmup_steps=warmup_steps,\n","            learning_rate=lr,\n","            logging_dir=args.model_path + f'single_model_fold{fold}/logs',\n","            load_best_model_at_end=False,\n","            metric_for_best_model='loss',\n","            logging_steps=args.logging_steps,\n","            evaluation_strategy=\"epoch\",\n","            save_strategy=\"epoch\",  \n","            save_total_limit=1,\n","            gradient_accumulation_steps=32,  # The true batch size is 32\n","            logging_strategy=\"steps\",\n","            logging_first_step=True,\n","        )\n","\n","        trainer = CustomTrainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            eval_dataset=val_dataset,\n","            tokenizer=tokenizer,\n","            compute_metrics=compute_metrics\n","        )\n","\n","        if args.mode == 'train':\n","            trainer.train()\n","            model.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","            tokenizer.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","            eval_output = trainer.evaluate()\n","            print('Evaluation results:', eval_output)\n","\n","        output = trainer.predict(val_dataset)\n","        predictions = output.predictions.squeeze()\n","        all_predictions.extend(predictions)\n","        all_labels.extend(val_labels)\n","        all_val_indexes.extend(val_index)\n","\n","        # Clean GPU memory\n","        import gc\n","        del model, train_dataset, val_dataset, train_data, val_data, train_convs, val_convs, train_labels, val_labels, tokenizer\n","        del train_encodings, val_encodings, trainer, output, predictions\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        torch.cuda.reset_peak_memory_stats()\n","\n","        model = None\n","        train_dataset = None\n","        val_dataset = None\n","        train_data = None\n","        val_data = None\n","        train_convs = None\n","        val_convs = None\n","        train_labels = None\n","        val_labels = None\n","        train_encodings = None\n","        val_encodings = None\n","        trainer = None\n","        output = None\n","        predictions = None\n","        tokenizer = False\n","\n","    spearman_corr = spearmanr(all_predictions, all_labels)[0]\n","    mae = mean_absolute_error(all_labels, all_predictions)\n","\n","    print(f\"Spearman Correlation: {spearman_corr}\")\n","    print(f\"MAE: {mae}\")\n","\n","    with open(results_file, \"a\") as f:\n","        f.write(f\"{warmup_steps},{lr},{max_length},{spearman_corr},{mae}\\n\")\n","\n","    predictions_labels_file = f\"oum_predictions_labels_warmup={warmup_steps}_lr={lr}_maxlen={max_length}_lf-last-layer.csv\"\n","    with open(predictions_labels_file, \"w\") as f:\n","        f.write(\"Prediction,Label,Index\\n\")\n","        for pred, label, index in zip(all_predictions, all_labels, all_val_indexes):\n","            f.write(f\"{pred},{label},{index}\\n\")\n","\n","    time.sleep(100) "]},{"cell_type":"markdown","id":"7430c7e6-1459-4804-8fc6-dd3bf2423947","metadata":{"id":"7430c7e6-1459-4804-8fc6-dd3bf2423947"},"source":["### 2.1.2. AFD"]},{"cell_type":"markdown","id":"683781e2-862b-4ed0-a3dc-b8660d19ec81","metadata":{"id":"683781e2-862b-4ed0-a3dc-b8660d19ec81","jp-MarkdownHeadingCollapsed":true},"source":["#### 2.1.2.1. Fine-tuning the whole model"]},{"cell_type":"code","execution_count":null,"id":"a9aba67c-46b3-4f4a-9b71-59959203e982","metadata":{"id":"a9aba67c-46b3-4f4a-9b71-59959203e982"},"outputs":[],"source":["import random\n","import torch\n","from torch.nn import L1Loss\n","from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import mean_absolute_error\n","from scipy.stats import spearmanr\n","import itertools\n","import time\n","from transformers import AutoModel\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n","# from torch.nn import CrossEntropyLoss\n","from torch.nn import BCEWithLogitsLoss, BCELoss\n","from sklearn.model_selection import train_test_split\n","from torch.nn import BCEWithLogitsLoss"]},{"cell_type":"code","execution_count":null,"id":"f1a65a37","metadata":{},"outputs":[],"source":["LRs = [2e-5, 1e-4]\n","WARMUP_EPOCHS = [3, 1] # warmup_steps for 1 or 3 epochs.\n","MAX_LENGTH = [2048, 4096]\n","\n","hyperparameter_combinations = list(itertools.product(LRs, WARMUP_EPOCHS, MAX_LENGTH))\n","\n","#################################################\n","##### Define Trainer and Args class objects #####\n","#################################################\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        labels = labels.float()\n","\n","        # Apply sigmoid function to the logits\n","        sigmoid_logits = torch.sigmoid(logits)\n","\n","        loss_fct = BCELoss()\n","        bce_loss = loss_fct(sigmoid_logits.view(-1), labels.view(-1))\n","\n","        return (bce_loss, outputs) if return_outputs else bce_loss\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = predictions.reshape(-1)  # Flatten the predictions\n","    labels = labels.reshape(-1)  # Flatten the labels\n","\n","    # Apply sigmoid function to the predictions\n","    sigmoid_predictions = torch.sigmoid(torch.from_numpy(predictions)).numpy()\n","\n","    auroc = roc_auc_score(labels, sigmoid_predictions)\n","    aupr = average_precision_score(labels, sigmoid_predictions)\n","\n","    return {\"auroc\": auroc, \"aupr\": aupr}\n","\n","\n","class Args:\n","    model_path = 'models/'\n","    num_labels = 1\n","    num_epochs = 5\n","    train_batch_size = 1\n","    valid_batch_size = 1\n","    model_name = 'allenai/longformer-large-4096'  \n","    model_type = 'longformer'  \n","    logging_steps = 1  \n","    mode = 'train'\n","    labels = 'after'\n","\n","args = Args()\n","\n","########################################\n","##### Define seeds and model setup #####\n","########################################\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","seed = 42\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(seed)\n","\n","####################################\n","##### Define Data and Training #####\n","####################################\n","x, _, y = load_data_afd()\n","x, y = x, y\n","\n","all_data = list(zip(x, y))\n","random.shuffle(all_data)\n","\n","num_folds = 7 # 7-fold cross validation\n","kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","results_file = \"afd_hyperparameters_results.csv\"\n","with open(results_file, \"w\") as f:\n","    f.write(\"Warmup Steps,Learning Rate,Max Length,AUROC,AUPR\\n\")\n","\n","for i, (lr, warmup_epochs, max_length) in enumerate(hyperparameter_combinations):\n","    print(f'\\n\\n{i}')\n","    print(f\"Training with hyperparameters: Warmup Steps={warmup_epochs}, Learning Rate={lr}, Max Length={max_length}\")\n","\n","    all_predictions = []\n","    all_labels = []\n","    all_val_convs = []\n","\n","    for fold, (train_index, val_index) in enumerate(kf.split(x, y), start=1):\n","        print(f\"Fold {fold}\")\n","        train_data = [all_data[i] for i in train_index]\n","        val_data = [all_data[i] for i in val_index]\n","        train_convs, train_labels = zip(*train_data)\n","        val_convs, val_labels = zip(*val_data)\n","\n","        MODEL_CLASSES = {\n","            \"longformer\": (LongformerForSequenceClassification, LongformerTokenizerFast)\n","        }\n","        model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","        model = model_class.from_pretrained(args.model_name, num_labels=args.num_labels).to(device)\n","        tokenizer = tokenizer_class.from_pretrained(args.model_name) # do_lower_case=True\n","        tokenizer.model_max_length = max_length\n","\n","        train_encodings = tokenizer(list(train_convs), truncation=True, padding=True, max_length=max_length)\n","        val_encodings = tokenizer(list(val_convs), truncation=True, padding=True, max_length=max_length)\n","\n","        train_dataset = conv_data_loader(train_encodings, train_labels)\n","        val_dataset = conv_data_loader(val_encodings, val_labels)\n","\n","        # Calculate warmup steps based on the current warmup_epochs value\n","        warmup_steps = len(train_dataset) // (args.train_batch_size * 32) * warmup_epochs\n","\n","        training_args = TrainingArguments(\n","            output_dir=args.model_path + f'single_model_fold{fold}/',\n","            num_train_epochs=args.num_epochs,\n","            per_device_train_batch_size=args.train_batch_size,\n","            per_device_eval_batch_size=args.valid_batch_size,\n","            warmup_steps=warmup_steps,\n","            learning_rate=lr,\n","            logging_dir=args.model_path + f'single_model_fold{fold}/logs',\n","            load_best_model_at_end=False,\n","            metric_for_best_model='loss',\n","            logging_steps=args.logging_steps,\n","            evaluation_strategy=\"epoch\", \n","            save_strategy=\"epoch\", \n","            save_total_limit=1,\n","            gradient_accumulation_steps=32,  # The true batch size is 32\n","            logging_strategy=\"steps\",\n","            logging_first_step=True,\n","        )\n","\n","        trainer = CustomTrainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            eval_dataset=val_dataset,\n","            tokenizer=tokenizer,\n","            compute_metrics=compute_metrics\n","        )\n","\n","        if args.mode == 'train':\n","            trainer.train()\n","            model.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","            tokenizer.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","            eval_output = trainer.evaluate()\n","            print('Evaluation results:', eval_output)\n","\n","        output = trainer.predict(val_dataset)\n","        predictions = output.predictions.squeeze()\n","        all_predictions.extend(torch.sigmoid(torch.tensor(predictions)).tolist()) # apply sigmoid\n","\n","        all_labels.extend(val_labels)\n","        all_val_convs.extend(val_convs)\n","\n","        # Clean GPU memory\n","        import gc\n","        del model, train_dataset, val_dataset, train_data, val_data, train_convs, val_convs, train_labels, val_labels, tokenizer\n","        del train_encodings, val_encodings, trainer, output, predictions\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        torch.cuda.reset_peak_memory_stats()\n","\n","        model = None\n","        train_dataset = None\n","        val_dataset = None\n","        train_data = None\n","        val_data = None\n","        train_convs = None\n","        val_convs = None\n","        train_labels = None\n","        val_labels = None\n","        train_encodings = None\n","        val_encodings = None\n","        trainer = None\n","        output = None\n","        predictions = None\n","        tokenizer = False\n","\n","    auroc = roc_auc_score(all_labels, all_predictions)\n","    aupr = average_precision_score(all_labels, all_predictions)\n","\n","    print(f\"AUROC: {auroc}\")\n","    print(f\"AUPR: {aupr}\")\n","\n","    with open(results_file, \"a\") as f:\n","        f.write(f\"{warmup_steps},{lr},{max_length},{auroc},{aupr}\\n\")\n","\n","    predictions_labels_file = f\"afd_predictions_labels_warmup={warmup_steps}_lr={lr}_maxlen={max_length}.csv\"\n","    with open(predictions_labels_file, \"w\") as f:\n","        f.write(\"Conversation,Prediction,Label\\n\")\n","        for conv, pred, label in zip(all_val_convs, all_predictions, all_labels):\n","            f.write(f\"{conv},{pred},{label}\\n\")\n","\n","    time.sleep(100) "]},{"cell_type":"code","execution_count":null,"id":"493b922f","metadata":{},"outputs":[],"source":["#############################################\n","##### Using the optimal hyperparameters #####\n","#############################################\n","LRs = [2e-5]\n","WARMUP_EPOCHS = [3]\n","MAX_LENGTH = [2048]\n","\n","hyperparameter_combinations = list(itertools.product(LRs, WARMUP_EPOCHS, MAX_LENGTH))\n","\n","#################################################\n","##### Define Trainer and Args class objects #####\n","#################################################\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        labels = labels.float()\n","\n","        # Apply sigmoid function to the logits\n","        sigmoid_logits = torch.sigmoid(logits)\n","\n","        loss_fct = BCELoss()\n","        bce_loss = loss_fct(sigmoid_logits.view(-1), labels.view(-1))\n","\n","        return (bce_loss, outputs) if return_outputs else bce_loss\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = predictions.reshape(-1)  # Flatten the predictions\n","    labels = labels.reshape(-1)  # Flatten the labels\n","\n","    # Apply sigmoid function to the predictions\n","    sigmoid_predictions = torch.sigmoid(torch.from_numpy(predictions)).numpy()\n","\n","    auroc = roc_auc_score(labels, sigmoid_predictions)\n","    aupr = average_precision_score(labels, sigmoid_predictions)\n","\n","    return {\"auroc\": auroc, \"aupr\": aupr}\n","\n","\n","class Args:\n","    model_path = 'models/'\n","    num_labels = 1\n","    num_epochs = 5\n","    train_batch_size = 1\n","    valid_batch_size = 1\n","    model_name = 'allenai/longformer-large-4096' \n","    model_type = 'longformer'  \n","    logging_steps = 1 \n","    # save_steps = 300\n","    mode = 'train'\n","    labels = 'after'\n","\n","args = Args()\n","\n","########################################\n","##### Define seeds and model setup #####\n","########################################\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","for seed in [1,2,3]:\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","    if device == 'cuda':\n","        torch.cuda.manual_seed_all(seed)\n","\n","    ####################################\n","    ##### Define Data and Training #####\n","    ####################################\n","    x, _, y = load_data_afd()\n","    x, y = x, y\n","\n","    all_data = list(zip(x, y))\n","    random.shuffle(all_data)\n","\n","    num_folds = 7 # 7-fold cross validation\n","    kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","    for i, (lr, warmup_epochs, max_length) in enumerate(hyperparameter_combinations):\n","        print(f'\\n\\n{i}')\n","        print(f\"Training with hyperparameters: Warmup Steps={warmup_epochs}, Learning Rate={lr}, Max Length={max_length}\")\n","\n","        all_predictions = []\n","        all_labels = []\n","        all_val_convs = []\n","\n","        for fold, (train_index, val_index) in enumerate(kf.split(x, y), start=1):\n","            print(f\"Fold {fold}\")\n","            train_data = [all_data[i] for i in train_index]\n","            val_data = [all_data[i] for i in val_index]\n","            train_convs, train_labels = zip(*train_data)\n","            val_convs, val_labels = zip(*val_data)\n","\n","            MODEL_CLASSES = {\n","                \"longformer\": (LongformerForSequenceClassification, LongformerTokenizerFast)\n","            }\n","            model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","            model = model_class.from_pretrained(args.model_name, num_labels=args.num_labels).to(device)\n","            tokenizer = tokenizer_class.from_pretrained(args.model_name) \n","            tokenizer.model_max_length = max_length\n","\n","            train_encodings = tokenizer(list(train_convs), truncation=True, padding=True, max_length=max_length)\n","            val_encodings = tokenizer(list(val_convs), truncation=True, padding=True, max_length=max_length)\n","\n","            train_dataset = conv_data_loader(train_encodings, train_labels)\n","            val_dataset = conv_data_loader(val_encodings, val_labels)\n","\n","            # Calculate warmup steps based on the current warmup_epochs value\n","            warmup_steps = len(train_dataset) // (args.train_batch_size * 32) * warmup_epochs\n","\n","            training_args = TrainingArguments(\n","                output_dir=args.model_path + f'single_model_fold{fold}/',\n","                num_train_epochs=args.num_epochs,\n","                per_device_train_batch_size=args.train_batch_size,\n","                per_device_eval_batch_size=args.valid_batch_size,\n","                warmup_steps=warmup_steps,\n","                learning_rate=lr,\n","                logging_dir=args.model_path + f'single_model_fold{fold}/logs',\n","                load_best_model_at_end=False,\n","                metric_for_best_model='loss',\n","                logging_steps=args.logging_steps,\n","                evaluation_strategy=\"epoch\", \n","                save_strategy=\"epoch\", \n","                save_total_limit=1,\n","                gradient_accumulation_steps=32,  \n","                logging_strategy=\"steps\",\n","                logging_first_step=True,\n","            )\n","\n","            trainer = CustomTrainer(\n","                model=model,\n","                args=training_args,\n","                train_dataset=train_dataset,\n","                eval_dataset=val_dataset,\n","                tokenizer=tokenizer,\n","                compute_metrics=compute_metrics\n","            )\n","\n","            if args.mode == 'train':\n","                trainer.train()\n","                model.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","                tokenizer.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","                eval_output = trainer.evaluate()\n","                print('Evaluation results:', eval_output)\n","\n","            output = trainer.predict(val_dataset)\n","            predictions = output.predictions.squeeze()\n","            \n","            all_predictions.extend(torch.sigmoid(torch.tensor(predictions)).tolist()) \n","\n","            all_labels.extend(val_labels)\n","            all_val_convs.extend(val_convs)\n","\n","            # Clean GPU memory\n","            import gc\n","            del model, train_dataset, val_dataset, train_data, val_data, train_convs, val_convs, train_labels, val_labels, tokenizer\n","            del train_encodings, val_encodings, trainer, output, predictions\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            torch.cuda.reset_peak_memory_stats()\n","\n","            model = None\n","            train_dataset = None\n","            val_dataset = None\n","            train_data = None\n","            val_data = None\n","            train_convs = None\n","            val_convs = None\n","            train_labels = None\n","            val_labels = None\n","            train_encodings = None\n","            val_encodings = None\n","            trainer = None\n","            output = None\n","            predictions = None\n","            tokenizer = False\n","\n","        auroc = roc_auc_score(all_labels, all_predictions)\n","        aupr = average_precision_score(all_labels, all_predictions)\n","\n","        print(f\"AUROC: {auroc}\")\n","        print(f\"AUPR: {aupr}\")\n","\n","        predictions_labels_file = f\"afd_lonformer_predictions_seed={seed}.json\"\n","\n","        data = []\n","        for conv, pred, label in zip(all_val_convs, all_predictions, all_labels):\n","            data.append({\n","                \"Conversation\": conv,\n","                \"Prediction\": float(pred),\n","                \"Label\": int(label) \n","            })\n","\n","        with open(predictions_labels_file, \"w\") as f:\n","            json.dump(data, f, indent=4)\n","\n","        time.sleep(100) "]},{"cell_type":"markdown","id":"92368acc-846b-41bc-9b63-cd06408058f1","metadata":{"id":"92368acc-846b-41bc-9b63-cd06408058f1","jp-MarkdownHeadingCollapsed":true},"source":["#### 2.1.2.2. Fine-tuning only the last layer"]},{"cell_type":"code","execution_count":null,"id":"e14a4ad3","metadata":{},"outputs":[],"source":["LRs = [2e-5, 1e-4]\n","WARMUP_EPOCHS = [3, 1] # warmup_steps for 1 or 3 epochs.\n","MAX_LENGTH = [2048, 4096]\n","\n","hyperparameter_combinations = list(itertools.product(LRs, WARMUP_EPOCHS, MAX_LENGTH))\n","\n","#################################################\n","##### Define Trainer and Args class objects #####\n","#################################################\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        labels = labels.float()\n","\n","        # Apply sigmoid function to the logits\n","        sigmoid_logits = torch.sigmoid(logits)\n","\n","        loss_fct = BCELoss()\n","        bce_loss = loss_fct(sigmoid_logits.view(-1), labels.view(-1))\n","\n","        return (bce_loss, outputs) if return_outputs else bce_loss\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = predictions.reshape(-1)  # Flatten the predictions\n","    labels = labels.reshape(-1)  # Flatten the labels\n","\n","    # Apply sigmoid function to the predictions\n","    sigmoid_predictions = torch.sigmoid(torch.from_numpy(predictions)).numpy()\n","\n","    auroc = roc_auc_score(labels, sigmoid_predictions)\n","    aupr = average_precision_score(labels, sigmoid_predictions)\n","\n","    return {\"auroc\": auroc, \"aupr\": aupr}\n","\n","\n","class Args:\n","    model_path = 'models/'\n","    num_labels = 1\n","    num_epochs = 5\n","    train_batch_size = 1\n","    valid_batch_size = 1\n","    model_name = 'allenai/longformer-large-4096'  \n","    model_type = 'longformer' \n","    logging_steps = 1 \n","    mode = 'train'\n","    labels = 'after'\n","\n","args = Args()\n","\n","########################################\n","##### Define seeds and model setup #####\n","########################################\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","seed = 42\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(seed)\n","\n","####################################\n","##### Define Data and Training #####\n","####################################\n","x, _, y = load_data_afd()\n","\n","all_data = list(zip(x, y))\n","random.shuffle(all_data)\n","\n","num_folds = 7\n","kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","results_file = \"afd_hyperparameters_results_lf-last-layer.csv\"\n","with open(results_file, \"w\") as f:\n","    f.write(\"Warmup Steps,Learning Rate,Max Length,AUROC,AUPR\\n\")\n","\n","for i, (lr, warmup_epochs, max_length) in enumerate(hyperparameter_combinations):\n","    print(f'\\n\\n{i}')\n","    print(f\"Training with hyperparameters: Warmup Steps={warmup_epochs}, Learning Rate={lr}, Max Length={max_length}\")\n","\n","    all_predictions = []\n","    all_labels = []\n","    all_val_convs = []\n","\n","    for fold, (train_index, val_index) in enumerate(kf.split(x, y), start=1):\n","        print(f\"Fold {fold}\")\n","        train_data = [all_data[i] for i in train_index]\n","        val_data = [all_data[i] for i in val_index]\n","        train_convs, train_labels = zip(*train_data)\n","        val_convs, val_labels = zip(*val_data)\n","\n","        MODEL_CLASSES = {\n","            \"longformer\": (LongformerForSequenceClassification, LongformerTokenizerFast)\n","        }\n","        model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","        model = model_class.from_pretrained(args.model_name, num_labels=args.num_labels).to(device)\n","\n","        # Freeze all layers except the last one\n","        for name, param in model.named_parameters():\n","            if 'classifier' not in name: \n","                param.requires_grad = False\n","\n","        params_to_update = []\n","        for name, param in model.named_parameters():\n","            if param.requires_grad:\n","                params_to_update.append(param)\n","\n","        tokenizer = tokenizer_class.from_pretrained(args.model_name)\n","        tokenizer.model_max_length = max_length\n","\n","        train_encodings = tokenizer(list(train_convs), truncation=True, padding=True, max_length=max_length)\n","        val_encodings = tokenizer(list(val_convs), truncation=True, padding=True, max_length=max_length)\n","\n","        train_dataset = conv_data_loader(train_encodings, train_labels)\n","        val_dataset = conv_data_loader(val_encodings, val_labels)\n","\n","        # Calculate warmup steps based on the current warmup_epochs value\n","        warmup_steps = len(train_dataset) // (args.train_batch_size * 32) * warmup_epochs\n","\n","        training_args = TrainingArguments(\n","            output_dir=args.model_path + f'single_model_fold{fold}/',\n","            num_train_epochs=args.num_epochs,\n","            per_device_train_batch_size=args.train_batch_size,\n","            per_device_eval_batch_size=args.valid_batch_size,\n","            warmup_steps=warmup_steps,\n","            learning_rate=lr,\n","            logging_dir=args.model_path + f'single_model_fold{fold}/logs',\n","            load_best_model_at_end=False,\n","            metric_for_best_model='loss',\n","            logging_steps=args.logging_steps,\n","            evaluation_strategy=\"epoch\", \n","            save_strategy=\"epoch\",  \n","            save_total_limit=1,\n","            gradient_accumulation_steps=32,  \n","            logging_strategy=\"steps\",\n","            logging_first_step=True,\n","        )\n","\n","        trainer = CustomTrainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            eval_dataset=val_dataset,\n","            tokenizer=tokenizer,\n","            compute_metrics=compute_metrics\n","        )\n","\n","        if args.mode == 'train':\n","            trainer.train()\n","            model.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","            tokenizer.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","            eval_output = trainer.evaluate()\n","            print('Evaluation results:', eval_output)\n","\n","        output = trainer.predict(val_dataset)\n","        predictions = output.predictions.squeeze()\n","        all_predictions.extend(torch.sigmoid(torch.tensor(predictions)).tolist())\n","\n","        all_labels.extend(val_labels)\n","        all_val_convs.extend(val_convs)\n","\n","        # Clean GPU memory\n","        import gc\n","        del model, train_dataset, val_dataset, train_data, val_data, train_convs, val_convs, train_labels, val_labels, tokenizer\n","        del train_encodings, val_encodings, trainer, output, predictions\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        torch.cuda.reset_peak_memory_stats()\n","\n","        model = None\n","        train_dataset = None\n","        val_dataset = None\n","        train_data = None\n","        val_data = None\n","        train_convs = None\n","        val_convs = None\n","        train_labels = None\n","        val_labels = None\n","        train_encodings = None\n","        val_encodings = None\n","        trainer = None\n","        output = None\n","        predictions = None\n","        tokenizer = False\n","\n","    auroc = roc_auc_score(all_labels, all_predictions)\n","    aupr = average_precision_score(all_labels, all_predictions)\n","\n","    print(f\"AUROC: {auroc}\")\n","    print(f\"AUPR: {aupr}\")\n","\n","    with open(results_file, \"a\") as f:\n","        f.write(f\"{warmup_steps},{lr},{max_length},{auroc},{aupr}\\n\")\n","\n","    predictions_labels_file = f\"afd_predictions_labels_warmup={warmup_steps}_lr={lr}_maxlen={max_length}_lf-last-layer.csv\"\n","    with open(predictions_labels_file, \"w\") as f:\n","        f.write(\"Conversation,Prediction,Label\\n\")\n","        for conv, pred, label in zip(all_val_convs, all_predictions, all_labels):\n","            f.write(f\"{conv},{pred},{label}\\n\")\n","\n","    time.sleep(100) "]},{"cell_type":"markdown","id":"e0ea5f81-9ff0-44e7-ab9c-f36f25bd1c05","metadata":{"id":"e0ea5f81-9ff0-44e7-ab9c-f36f25bd1c05"},"source":["### 2.1.3. Wikitactics"]},{"cell_type":"markdown","id":"8ecc0c10-4964-4dd8-9882-4a6f5719ce88","metadata":{"id":"8ecc0c10-4964-4dd8-9882-4a6f5719ce88"},"source":["#### 2.1.3.1. Fine-tuning the whole model"]},{"cell_type":"code","execution_count":null,"id":"NHrynPpdMOyE","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16885,"status":"ok","timestamp":1715854335828,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-60},"id":"NHrynPpdMOyE","outputId":"97eb04be-65a8-4514-9987-835b0a733f5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"60eed901-7233-4b35-95ef-62e1c35700fe","metadata":{"id":"60eed901-7233-4b35-95ef-62e1c35700fe"},"outputs":[],"source":["import random\n","import torch\n","from torch.nn import L1Loss\n","from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import mean_absolute_error\n","from scipy.stats import spearmanr\n","import itertools\n","import time\n","from transformers import AutoModel\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n","# from torch.nn import CrossEntropyLoss\n","from torch.nn import BCEWithLogitsLoss, BCELoss\n","from sklearn.model_selection import train_test_split\n","from torch.nn import BCEWithLogitsLoss"]},{"cell_type":"code","execution_count":null,"id":"dac84d3e","metadata":{},"outputs":[],"source":["LRs = [2e-5, 1e-4]\n","WARMUP_EPOCHS = [3, 1]\n","MAX_LENGTH = [2048, 4096]\n","\n","hyperparameter_combinations = list(itertools.product(LRs, WARMUP_EPOCHS, MAX_LENGTH))\n","\n","#################################################\n","##### Define Trainer and Args class objects #####\n","#################################################\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        labels = labels.float()\n","\n","        # Apply sigmoid function to the logits\n","        sigmoid_logits = torch.sigmoid(logits)\n","\n","        loss_fct = BCELoss()\n","        bce_loss = loss_fct(sigmoid_logits.view(-1), labels.view(-1))\n","\n","        return (bce_loss, outputs) if return_outputs else bce_loss\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = predictions.reshape(-1)  # Flatten the predictions\n","    labels = labels.reshape(-1)  # Flatten the labels\n","\n","    # Apply sigmoid function to the predictions\n","    sigmoid_predictions = torch.sigmoid(torch.from_numpy(predictions)).numpy()\n","\n","    auroc = roc_auc_score(labels, sigmoid_predictions)\n","    aupr = average_precision_score(labels, sigmoid_predictions)\n","\n","    return {\"auroc\": auroc, \"aupr\": aupr}\n","\n","\n","class Args:\n","    model_path = 'models/'\n","    num_labels = 1\n","    num_epochs = 5\n","    train_batch_size = 1\n","    valid_batch_size = 1\n","    model_name = 'allenai/longformer-large-4096'  \n","    model_type = 'longformer' \n","    logging_steps = 1  \n","    mode = 'train'\n","    labels = 'after'\n","\n","args = Args()\n","\n","########################################\n","##### Define seeds and model setup #####\n","########################################\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","seed = 42\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(seed)\n","\n","####################################\n","##### Define Data and Training #####\n","####################################\n","x, y = load_data_wikitac()\n","\n","all_data = list(zip(x, y))\n","random.shuffle(all_data)\n","\n","num_folds = 7 # 7-fold cross validation\n","kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","results_file = \"wikitac_hyperparameters_results.csv\"\n","with open(results_file, \"w\") as f:\n","    f.write(\"Warmup Steps,Learning Rate,Max Length,AUROC,AUPR\\n\")\n","\n","for i, (lr, warmup_epochs, max_length) in enumerate(hyperparameter_combinations):\n","\n","    print(f'\\n\\n{i}')\n","    print(f\"Training with hyperparameters: Warmup Steps={warmup_epochs}, Learning Rate={lr}, Max Length={max_length}\")\n","\n","    all_predictions = []\n","    all_labels = []\n","    all_val_convs = []\n","\n","    for fold, (train_index, val_index) in enumerate(kf.split(x, y), start=1):\n","        print(f\"Fold {fold}\")\n","        train_data = [all_data[i] for i in train_index]\n","        val_data = [all_data[i] for i in val_index]\n","        train_convs, train_labels = zip(*train_data)\n","        val_convs, val_labels = zip(*val_data)\n","\n","        MODEL_CLASSES = {\n","            \"longformer\": (LongformerForSequenceClassification, LongformerTokenizerFast)\n","        }\n","        model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","        model = model_class.from_pretrained(args.model_name, num_labels=args.num_labels).to(device)\n","        tokenizer = tokenizer_class.from_pretrained(args.model_name)\n","        tokenizer.model_max_length = max_length\n","\n","        train_encodings = tokenizer(list(train_convs), truncation=True, padding=True, max_length=max_length)\n","        val_encodings = tokenizer(list(val_convs), truncation=True, padding=True, max_length=max_length)\n","\n","        train_dataset = conv_data_loader(train_encodings, train_labels)\n","        val_dataset = conv_data_loader(val_encodings, val_labels)\n","\n","        warmup_steps = len(train_dataset) // (args.train_batch_size * 32) * warmup_epochs\n","\n","        training_args = TrainingArguments(\n","            output_dir=args.model_path + f'single_model_fold{fold}/',\n","            num_train_epochs=args.num_epochs,\n","            per_device_train_batch_size=args.train_batch_size,\n","            per_device_eval_batch_size=args.valid_batch_size,\n","            warmup_steps=warmup_steps,\n","            learning_rate=lr,\n","            logging_dir=args.model_path + f'single_model_fold{fold}/logs',\n","            load_best_model_at_end=False,\n","            metric_for_best_model='loss',\n","            logging_steps=args.logging_steps,\n","            evaluation_strategy=\"epoch\",\n","            save_strategy=\"epoch\", \n","            save_total_limit=1,\n","            gradient_accumulation_steps=32, \n","            logging_strategy=\"steps\",\n","            logging_first_step=True,\n","        )\n","\n","        trainer = CustomTrainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            eval_dataset=val_dataset,\n","            tokenizer=tokenizer,\n","            compute_metrics=compute_metrics\n","        )\n","\n","        if args.mode == 'train':\n","            trainer.train()\n","            model.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","            tokenizer.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","            eval_output = trainer.evaluate()\n","            print('Evaluation results:', eval_output)\n","\n","        output = trainer.predict(val_dataset)\n","        predictions = output.predictions.squeeze()\n","        all_predictions.extend(torch.sigmoid(torch.tensor(predictions)).tolist())\n","\n","        all_labels.extend(val_labels)\n","        all_val_convs.extend(val_convs)\n","\n","        task = 'wikitac'\n","        model_path = f\"./finetuned_longformers/{task}_fold={fold}\" \n","        model.save_pretrained(model_path)\n","        tokenizer.save_pretrained(model_path)\n","\n","\n","        import gc\n","        del model, train_dataset, val_dataset, train_data, val_data, train_convs, val_convs, train_labels, val_labels, tokenizer\n","        del train_encodings, val_encodings, trainer, output, predictions\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        torch.cuda.reset_peak_memory_stats()\n","\n","        model = None\n","        train_dataset = None\n","        val_dataset = None\n","        train_data = None\n","        val_data = None\n","        train_convs = None\n","        val_convs = None\n","        train_labels = None\n","        val_labels = None\n","        train_encodings = None\n","        val_encodings = None\n","        trainer = None\n","        output = None\n","        predictions = None\n","        tokenizer = False\n","\n","    auroc = roc_auc_score(all_labels, all_predictions)\n","    aupr = average_precision_score(all_labels, all_predictions)\n","\n","    print(f\"AUROC: {auroc}\")\n","    print(f\"AUPR: {aupr}\")\n","\n","    with open(results_file, \"a\") as f:\n","        f.write(f\"{warmup_steps},{lr},{max_length},{auroc},{aupr}\\n\")\n","\n","    predictions_labels_file = f\"./wikitac_predictions_labels_warmup={warmup_steps}_lr={lr}_maxlen={max_length}.csv\"\n","    with open(predictions_labels_file, \"w\") as f:\n","        f.write(\"Conversation,Prediction,Label\\n\")\n","        for conv, pred, label in zip(all_val_convs, all_predictions, all_labels):\n","            f.write(f\"{conv},{pred},{label}\\n\")\n","\n","    time.sleep(100) "]},{"cell_type":"markdown","id":"4cada110","metadata":{},"source":["#### 2.1.3.2. Fine-tuning the last layer only"]},{"cell_type":"code","execution_count":null,"id":"v81sesTuOJWf","metadata":{"id":"v81sesTuOJWf"},"outputs":[],"source":["# Training with hyperparameters: Warmup Steps=3, Learning Rate=0.0001, Max Length=4096 --> The best."]},{"cell_type":"code","execution_count":null,"id":"c3e781ed","metadata":{},"outputs":[],"source":["\"\"\"\n","Using the optimal combination of hyperparameters\n","\"\"\"\n","LRs = [1e-4]\n","WARMUP_EPOCHS = [3]\n","MAX_LENGTH = [4096]\n","\n","hyperparameter_combinations = list(itertools.product(LRs, WARMUP_EPOCHS, MAX_LENGTH))\n","\n","#################################################\n","##### Define Trainer and Args class objects #####\n","#################################################\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        labels = labels.float()\n","\n","        # Apply sigmoid function to the logits\n","        sigmoid_logits = torch.sigmoid(logits)\n","\n","        loss_fct = BCELoss()\n","        bce_loss = loss_fct(sigmoid_logits.view(-1), labels.view(-1))\n","\n","        return (bce_loss, outputs) if return_outputs else bce_loss\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = predictions.reshape(-1)  # Flatten the predictions\n","    labels = labels.reshape(-1)  # Flatten the labels\n","\n","    # Apply sigmoid function to the predictions\n","    sigmoid_predictions = torch.sigmoid(torch.from_numpy(predictions)).numpy()\n","\n","    auroc = roc_auc_score(labels, sigmoid_predictions)\n","    aupr = average_precision_score(labels, sigmoid_predictions)\n","\n","    return {\"auroc\": auroc, \"aupr\": aupr}\n","\n","\n","class Args:\n","    model_path = 'models/'\n","    num_labels = 1\n","    num_epochs = 5\n","    train_batch_size = 1\n","    valid_batch_size = 1\n","    model_name = 'allenai/longformer-large-4096' \n","    model_type = 'longformer'  \n","    logging_steps = 1  \n","    # save_steps = 300\n","    mode = 'train'\n","    labels = 'after'\n","\n","args = Args()\n","\n","########################################\n","##### Define seeds and model setup #####\n","########################################\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","for seed in [1, 2, 3]:\n","\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","    if device == 'cuda':\n","        torch.cuda.manual_seed_all(seed)\n","\n","    ####################################\n","    ##### Define Data and Training #####\n","    ####################################\n","    x, y = load_data_wikitac()\n","\n","    all_data = list(zip(x, y))\n","    random.shuffle(all_data)\n","\n","    num_folds = 7 # 7-fold cross validation\n","    kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","\n","    for i, (lr, warmup_epochs, max_length) in enumerate(hyperparameter_combinations):\n","        print(f'\\n\\n{i}')\n","        print(f\"Training with hyperparameters: Warmup Steps={warmup_epochs}, Learning Rate={lr}, Max Length={max_length}\")\n","\n","        all_predictions = []\n","        all_labels = []\n","        all_val_convs = []\n","\n","        for fold, (train_index, val_index) in enumerate(kf.split(x, y), start=1):\n","            print(f\"Fold {fold}\")\n","            train_data = [all_data[i] for i in train_index]\n","            val_data = [all_data[i] for i in val_index]\n","            train_convs, train_labels = zip(*train_data)\n","            val_convs, val_labels = zip(*val_data)\n","\n","            MODEL_CLASSES = {\n","                \"longformer\": (LongformerForSequenceClassification, LongformerTokenizerFast)\n","            }\n","            model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","            model = model_class.from_pretrained(args.model_name, num_labels=args.num_labels).to(device)\n","            tokenizer = tokenizer_class.from_pretrained(args.model_name) # do_lower_case=True\n","            tokenizer.model_max_length = max_length\n","\n","            train_encodings = tokenizer(list(train_convs), truncation=True, padding=True, max_length=max_length)\n","            val_encodings = tokenizer(list(val_convs), truncation=True, padding=True, max_length=max_length)\n","\n","            train_dataset = conv_data_loader(train_encodings, train_labels)\n","            val_dataset = conv_data_loader(val_encodings, val_labels)\n","\n","            # Calculate warmup steps based on the current warmup_epochs value\n","            warmup_steps = len(train_dataset) // (args.train_batch_size * 32) * warmup_epochs\n","\n","            training_args = TrainingArguments(\n","                output_dir=args.model_path + f'single_model_fold{fold}/',\n","                num_train_epochs=args.num_epochs,\n","                per_device_train_batch_size=args.train_batch_size,\n","                per_device_eval_batch_size=args.valid_batch_size,\n","                warmup_steps=warmup_steps,\n","                learning_rate=lr,\n","                logging_dir=args.model_path + f'single_model_fold{fold}/logs',\n","                load_best_model_at_end=False,\n","                metric_for_best_model='loss',\n","                logging_steps=args.logging_steps,\n","                evaluation_strategy=\"epoch\",\n","                save_strategy=\"epoch\",  \n","                save_total_limit=1,\n","                gradient_accumulation_steps=32,\n","                logging_strategy=\"steps\",\n","                logging_first_step=True,\n","            )\n","\n","            trainer = CustomTrainer(\n","                model=model,\n","                args=training_args,\n","                train_dataset=train_dataset,\n","                eval_dataset=val_dataset,\n","                tokenizer=tokenizer,\n","                compute_metrics=compute_metrics,\n","                # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n","            )\n","\n","            if args.mode == 'train':\n","                trainer.train()\n","                model.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","                tokenizer.save_pretrained(args.model_path + f'single_model_fold{fold}/')\n","                eval_output = trainer.evaluate()\n","                print('Evaluation results:', eval_output)\n","\n","            output = trainer.predict(val_dataset)\n","            predictions = output.predictions.squeeze()\n","            all_predictions.extend(torch.sigmoid(torch.tensor(predictions)).tolist())\n","\n","            all_labels.extend(val_labels)\n","            all_val_convs.extend(val_convs)\n","\n","\n","            task = 'wikitac'\n","            model_path = f\"./finetuned_longformers/{task}_seed={seed}_fold={fold}\" \n","            model.save_pretrained(model_path)\n","            tokenizer.save_pretrained(model_path)\n","\n","            # Clean GPU memory\n","            import gc\n","            del model, train_dataset, val_dataset, train_data, val_data, train_convs, val_convs, train_labels, val_labels, tokenizer\n","            del train_encodings, val_encodings, trainer, output, predictions\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            torch.cuda.reset_peak_memory_stats()\n","\n","            model = None\n","            train_dataset = None\n","            val_dataset = None\n","            train_data = None\n","            val_data = None\n","            train_convs = None\n","            val_convs = None\n","            train_labels = None\n","            val_labels = None\n","            train_encodings = None\n","            val_encodings = None\n","            trainer = None\n","            output = None\n","            predictions = None\n","            tokenizer = False\n","\n","        auroc = roc_auc_score(all_labels, all_predictions)\n","        aupr = average_precision_score(all_labels, all_predictions)\n","\n","        print(f\"AUROC: {auroc}\")\n","        print(f\"AUPR: {aupr}\")\n","\n","        predictions_labels_file = f\"./wikitac_predictions_labels_seed={seed}.csv\"\n","        with open(predictions_labels_file, \"w\") as f:\n","            f.write(\"Conversation,Prediction,Label\\n\")\n","            for conv, pred, label in zip(all_val_convs, all_predictions, all_labels):\n","                f.write(f\"{conv},{pred},{label}\\n\")\n","\n","        time.sleep(100)"]},{"cell_type":"markdown","id":"48LwyMumwd2P","metadata":{"id":"48LwyMumwd2P","jp-MarkdownHeadingCollapsed":true},"source":["# 3. Evaluation time"]},{"cell_type":"markdown","id":"b52f28ec","metadata":{},"source":["## 3.1. Overall Evaluation"]},{"cell_type":"code","execution_count":null,"id":"182f82a6-5250-42ff-849f-e5d50955121b","metadata":{"id":"182f82a6-5250-42ff-849f-e5d50955121b"},"outputs":[],"source":["from scipy.stats import spearmanr, pearsonr\n","\n","# Prediction on the evaluation set\n","output = trainer.predict(val_dataset)  \n","predictions = output.predictions.squeeze() \n","\n","# Compute Spearman correlation\n","spearman_corr = spearmanr(predictions, val_labels)[0]\n","\n","print(f\"Spearman Correlation: {spearman_corr}\")"]},{"cell_type":"code","execution_count":null,"id":"sNpxR9HPb4Zp","metadata":{"id":"sNpxR9HPb4Zp"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, accuracy_score, cohen_kappa_score, f1_score, confusion_matrix, recall_score, precision_score\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","\n","threshold = Counter(val_labels)[1] / len(val_labels)\n","\n","# Evaluate on the validation set\n","auroc_val = roc_auc_score(val_labels, predictions)\n","precision_val, recall_val, _ = precision_recall_curve(val_labels, predictions)\n","auprc_val = auc(recall_val, precision_val)\n","accuracy_val = accuracy_score(val_labels, [1 if p >= threshold else 0 for p in predictions])\n","kappa_val = cohen_kappa_score(val_labels, [1 if p >= threshold else 0 for p in predictions])\n","f1_val = f1_score(val_labels, [1 if p >= threshold else 0 for p in predictions], average='binary')\n","cm_val = confusion_matrix(val_labels, [1 if p >= threshold else 0 for p in predictions])\n","\n","# Calculate recall, precision, and specificity\n","y_pred_val_binary = [1 if p >= threshold else 0 for p in predictions]\n","recall = recall_score(val_labels, y_pred_val_binary)\n","precision = precision_score(val_labels, y_pred_val_binary)\n","tn, fp, fn, tp = cm_val.ravel()\n","specificity = tn / (tn + fp)\n","\n","print('Validation Set:')\n","print(\"AUROC:\", auroc_val)\n","print(\"AUPRC:\", auprc_val)\n","print(\"Accuracy:\", accuracy_val)\n","print(\"Cohen's Kappa:\", kappa_val)\n","print(\"F1 Score:\", f1_val)\n","print(\"Recall:\", recall)\n","print(\"Precision:\", precision)\n","print(\"Specificity:\", specificity)\n","print(\"Confusion Matrix:\")\n","print(cm_val)\n","\n","# Validation data plot with density\n","plt.figure(figsize=(8, 6))\n","plt.hexbin(predictions, val_labels, gridsize=50, cmap='viridis', mincnt=1)\n","plt.colorbar(label='Count in bin')\n","plt.xlabel('Predicted Probability')\n","plt.ylabel('True Label')\n","plt.title('Validation Data Predictions')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"46ee4fe9","metadata":{},"source":["## 3.2. Robustness Evaluation (OUM)"]},{"cell_type":"code","execution_count":null,"id":"gWh6YPDahyoJ","metadata":{"colab":{"background_save":true},"id":"gWh6YPDahyoJ"},"outputs":[],"source":["from transformers_interpret import SequenceClassificationExplainer\n","import pandas as pd\n","from scipy import stats\n","\n","df = pd.DataFrame({\n","    'Conversations': val_convs,\n","    'Labels': val_labels,\n","    'Predictions': predictions\n","})\n","\n","csv_filename = f'predict_oum_val_data_corr={round(spearman_corr, 2)}.csv'\n","df.to_csv(csv_filename, index=False)\n","\n","df = pd.read_csv(f'predict_oum_val_data_corr={round(spearman_corr, 2)}.csv')\n","df['#words'] = df['Conversations'].map(lambda c: len(c.split()))\n","\n","def topic_categorisation(conv):\n","    if 'veganism' in conv.lower():\n","        return 'veganism'\n","    elif 'covid' in conv.lower():\n","        return 'covid'\n","    elif 'brexit' in conv.lower():\n","        return 'brexit'\n","    else:\n","        return 'other'\n","\n","df['topic'] = df['Conversations'].map(topic_categorisation)\n","\n","\n","overall_corr, _ = stats.spearmanr(df['Labels'], df['Predictions'])\n","print(f\"Overall Spearman correlation: {overall_corr:.3f}\")\n","\n","topic_corrs = {}\n","for topic in df['topic'].unique():\n","    topic_df = df[df['topic'] == topic]\n","    topic_corr, _ = stats.spearmanr(topic_df['Labels'], topic_df['Predictions'])\n","    topic_corrs[topic] = topic_corr\n","\n","for topic, corr in topic_corrs.items():\n","    print(f\"Spearman correlation for topic '{topic}': {corr:.3f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["3gdaCuidi3g2","PtV8VwMC_XmT","34d58f63-296e-4465-b5fd-b410060fa007","qOj-Um0bwVZj","0TH-2zoPDjMM","Prk10SRrDmqR","uRT6c3AWHB4b","twDFmQn1smba","gGHkm0j0sZ7f","A6uqUEL7ssy4","7cFP_yKLsfFR","mwUh32rlV1fx","A52X7NIFV4j8","2d241e4d-6d9c-4418-9b73-f2bb52ead5cb","648e5ced-53c9-44a7-b567-e34849fff81f","683781e2-862b-4ed0-a3dc-b8660d19ec81","92368acc-846b-41bc-9b63-cd06408058f1","48LwyMumwd2P","IXy1rDVyLFzp","U39H0LFrkzMx","9uutoyr_N45L"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}
