{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98671,"status":"ok","timestamp":1724354904857,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"ag8gtk78FtpN","outputId":"601cc3df-fe6f-4785-cf9b-9a7512d82fcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting convokit\n","  Downloading convokit-3.0.0.tar.gz (183 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m174.1/183.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.2/183.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n","Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.1.4)\n","Collecting msgpack-numpy>=0.4.3.2 (from convokit)\n","  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.6)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.13.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.2)\n","Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n","Collecting dill>=0.2.9 (from convokit)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.4.2)\n","Collecting clean-text>=0.6.0 (from convokit)\n","  Downloading clean_text-0.6.0-py3-none-any.whl.metadata (6.6 kB)\n","Collecting unidecode>=1.1.1 (from convokit)\n","  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.5)\n","Collecting pymongo>=4.0 (from convokit)\n","  Downloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.2)\n","Collecting dnspython>=1.16.0 (from convokit)\n","  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n","Collecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\n","  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n","Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2024.5.15)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2024.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.5.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.12.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.8.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (71.0.4)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.4.0)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.20.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.12.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2024.7.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (13.7.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (0.18.1)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (7.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (2.16.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (0.1.2)\n","Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n","Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n","Downloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: convokit, emoji\n","  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for convokit: filename=convokit-3.0.0-py3-none-any.whl size=216707 sha256=28c8555f829343f0f2c1b325ebb46e225b99d00bb83497258c9f984356dbb5be\n","  Stored in directory: /root/.cache/pip/wheels/c4/89/8c/2677fdb888588b6f93cb6ac86bdfb020f1f1c33e0d5525b231\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171031 sha256=ce5d9d2edcddcdff15b880e4179cc066095df9ace00a0019935ad29f340c8d5c\n","  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n","Successfully built convokit emoji\n","Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dnspython, dill, pymongo, clean-text, convokit\n","Successfully installed clean-text-0.6.0 convokit-3.0.0 dill-0.3.8 dnspython-2.6.1 emoji-1.7.0 ftfy-6.2.3 msgpack-numpy-0.4.8 pymongo-4.8.0 unidecode-1.3.8\n","Collecting openai==0.28\n","  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n","Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: openai\n","Successfully installed openai-0.28.0\n","Collecting jsonlines\n","  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (24.2.0)\n","Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-4.0.0\n","Collecting shap\n","  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.3.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.1.4)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n","Collecting slicer==0.0.8 (from shap)\n","  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n","Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n","Installing collected packages: slicer, shap\n","Successfully installed shap-0.46.0 slicer-0.0.8\n","Collecting pdpbox\n","  Downloading PDPbox-0.3.0-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: joblib>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pdpbox) (1.4.2)\n","Requirement already satisfied: matplotlib>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from pdpbox) (3.7.1)\n","Requirement already satisfied: numpy>=1.21.5 in /usr/local/lib/python3.10/dist-packages (from pdpbox) (1.26.4)\n","Requirement already satisfied: pandas>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from pdpbox) (2.1.4)\n","Requirement already satisfied: plotly>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from pdpbox) (5.15.0)\n","Collecting pqdm>=0.2.0 (from pdpbox)\n","  Downloading pqdm-0.2.0-py2.py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: psutil>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from pdpbox) (5.9.5)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pdpbox) (7.4.4)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from pdpbox) (1.3.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pdpbox) (71.0.4)\n","Requirement already satisfied: sphinx>=5.0.2 in /usr/local/lib/python3.10/dist-packages (from pdpbox) (5.0.2)\n","Collecting sphinx-rtd-theme>=1.1.1 (from pdpbox)\n","  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pdpbox) (4.66.5)\n","Collecting numpydoc>=1.4.0 (from pdpbox)\n","  Downloading numpydoc-1.8.0-py3-none-any.whl.metadata (4.3 kB)\n","Requirement already satisfied: xgboost>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from pdpbox) (2.1.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.2->pdpbox) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.2->pdpbox) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.2->pdpbox) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.2->pdpbox) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.2->pdpbox) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.2->pdpbox) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.2->pdpbox) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.2->pdpbox) (2.8.2)\n","Collecting sphinx>=5.0.2 (from pdpbox)\n","  Downloading sphinx-8.0.2-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.4.0->pdpbox) (0.9.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.4.0->pdpbox) (2.0.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.4->pdpbox) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.4->pdpbox) (2024.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.9.0->pdpbox) (9.0.0)\n","Collecting bounded-pool-executor (from pqdm>=0.2.0->pdpbox)\n","  Downloading bounded_pool_executor-0.0.3-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pqdm>=0.2.0->pdpbox) (4.12.2)\n","Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pdpbox) (1.13.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pdpbox) (3.5.0)\n","Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (2.0.0)\n","Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (2.0.0)\n","Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (1.0.1)\n","Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (2.1.0)\n","Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (2.0.0)\n","Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (2.0.0)\n","Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (3.1.4)\n","Collecting Pygments>=2.17 (from sphinx>=5.0.2->pdpbox)\n","  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n","Collecting docutils<0.22,>=0.20 (from sphinx>=5.0.2->pdpbox)\n","  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (2.2.0)\n","Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (2.16.0)\n","Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (0.7.16)\n","Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (1.4.1)\n","Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.0.2->pdpbox) (2.32.3)\n","Collecting sphinx>=5.0.2 (from pdpbox)\n","  Downloading sphinx-7.4.7-py3-none-any.whl.metadata (6.1 kB)\n","Collecting docutils<0.22,>=0.20 (from sphinx>=5.0.2->pdpbox)\n","  Downloading docutils-0.20.1-py3-none-any.whl.metadata (2.8 kB)\n","Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme>=1.1.1->pdpbox)\n","  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n","Collecting nvidia-nccl-cu12 (from xgboost>=1.7.1->pdpbox)\n","  Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pdpbox) (2.0.0)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pdpbox) (1.5.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pdpbox) (1.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=5.0.2->pdpbox) (2.1.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.2->pdpbox) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.0.2->pdpbox) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.0.2->pdpbox) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.0.2->pdpbox) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.0.2->pdpbox) (2024.7.4)\n","Downloading PDPbox-0.3.0-py3-none-any.whl (35.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpydoc-1.8.0-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pqdm-0.2.0-py2.py3-none-any.whl (6.8 kB)\n","Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sphinx-7.4.7-py3-none-any.whl (3.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docutils-0.20.1-py3-none-any.whl (572 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bounded_pool_executor-0.0.3-py3-none-any.whl (3.4 kB)\n","Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bounded-pool-executor, Pygments, pqdm, nvidia-nccl-cu12, docutils, sphinx, sphinxcontrib-jquery, numpydoc, sphinx-rtd-theme, pdpbox\n","  Attempting uninstall: Pygments\n","    Found existing installation: Pygments 2.16.1\n","    Uninstalling Pygments-2.16.1:\n","      Successfully uninstalled Pygments-2.16.1\n","  Attempting uninstall: docutils\n","    Found existing installation: docutils 0.18.1\n","    Uninstalling docutils-0.18.1:\n","      Successfully uninstalled docutils-0.18.1\n","  Attempting uninstall: sphinx\n","    Found existing installation: Sphinx 5.0.2\n","    Uninstalling Sphinx-5.0.2:\n","      Successfully uninstalled Sphinx-5.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.22.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Pygments-2.18.0 bounded-pool-executor-0.0.3 docutils-0.20.1 numpydoc-1.8.0 nvidia-nccl-cu12-2.22.3 pdpbox-0.3.0 pqdm-0.2.0 sphinx-7.4.7 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1\n"]}],"source":["!pip install convokit\n","# !pip install transformers[torch]\n","# !pip install accelerate\n","# !pip install transformers_interpret\n","# !pip install git+https://github.com/allenai/longformer.git\n","!pip install openai==0.28\n","!pip install jsonlines\n","!pip install shap\n","!pip install pdpbox"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40680,"status":"ok","timestamp":1724354945534,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"JGFMti3WL90K","outputId":"46c47808-30b0-4fea-e38c-e7b7e6f39ba5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","import openai\n","import time\n","import pandas as pd\n","import jsonlines\n","import csv\n","import json\n","import random\n","import torch\n","import convokit\n","import spacy\n","\n","from sklearn.datasets import fetch_openml\n","from sklearn import tree\n","from datetime import date\n","from torch.nn import L1Loss\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from scipy import stats\n","from convokit import Corpus, Speaker, Utterance\n","from convokit import download\n","from convokit import TextParser\n","\n","openai.api_key = \"XXXXXXXX\""]},{"cell_type":"markdown","metadata":{"id":"vVpaL1ME3ACA"},"source":["# 0. Load Data Function"]},{"cell_type":"markdown","metadata":{"id":"79W5-E3HSwDP"},"source":["## 0.1. OUM data"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":801,"status":"ok","timestamp":1724354946333,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"-r1EuVjP2-v3"},"outputs":[],"source":["# Load the data\n","def load_data_oum(label='after'):\n","    final_convs = []\n","    final_labels = []\n","    wizards_data = []\n","    moral_foundations = [\"care\", \"fairness\", \"liberty\", \"loyalty\", \"authority\", \"sanctity\", \"none\"]\n","    input_files = {\"wizards\": \"wizards_dialogues.json\", \"final_argubot\": \"argubot_final_exp.json\",\n","                   \"models_dialogues\": \"models_dialogues.json\"}\n","    dials_with_scores = {\"wizards\": {}, \"final_argubot\": {}, \"models_dialogues\": {}}\n","\n","    for key in input_files:\n","        input_file = input_files[key]\n","        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        for d in data:\n","            is_wiki = False\n","            for m in d[\"messages\"]:\n","                if 'model' in m and (m['model'] == 'wikibot' or m['model'] == 'controlbot'):\n","                    is_wiki = True\n","                    break\n","            if is_wiki:\n","                continue\n","            yes_no = 'none'\n","            k = 'Did you vote for (Leave) or against (Remain) Brexit in the 2016 UK referendum?'\n","            if k in d['participant_info']:\n","                if d['participant_info'][k].lower() == 'against (remain)':\n","                    yes_no = 'no'\n","                elif d['participant_info'][k].lower() == 'for (leave)':\n","                    yes_no = 'yes'\n","                else:\n","                    yes_no = 'none'\n","\n","            k = 'In the referendum on whether the UK should remain a member of the EU (BREXIT), how did you vote?'\n","            if k in d['participant_info']:\n","                if d['participant_info'][k].lower() == 'remain (against brexit)':\n","                    yes_no = 'no'\n","                elif d['participant_info'][k].lower() == 'leave (for brexit)':\n","                    yes_no = 'yes'\n","                else:\n","                    yes_no = 'none'\n","            k = 'Have you had at least one dose of an approved Covid-19 vaccine?'\n","            if k in d['participant_info']:\n","                if d['participant_info'][k].lower() == 'yes':\n","                    yes_no = 'yes'\n","                elif d['participant_info'][k].lower() == 'no':\n","                    yes_no = 'no'\n","            k = 'Are you a vegan?'\n","            if k in d['participant_info']:\n","                if d['participant_info'][k].lower() == 'yes':\n","                    yes_no = 'yes'\n","                elif d['participant_info'][k].lower() == 'no':\n","                    yes_no = 'no'\n","\n","            if yes_no == 'none':\n","                continue\n","\n","            if 'Questions' in d['participant_info']:\n","                for q in d['participant_info']['Questions']:\n","                    if \"final\" in input_file:\n","                        if label == 'oum':\n","                            continue\n","                        if d['participant_info']['Questions'][q]['after'] == -1:\n","                            continue\n","                    elif d['participant_info']['Questions'][q]['before'] == -1 or d['participant_info']['Questions'][q]['after'] == -1:\n","                        continue\n","                    if 'good reasons' in q.lower():\n","                        if d['topic'] != 'brexit' and 'not' in q.lower() and yes_no == 'no':\n","                            continue\n","                        if d['topic'] != 'brexit' and 'not' not in q.lower() and yes_no == 'yes':\n","                            continue\n","                        if 'leave' in q.lower() and yes_no == 'yes':\n","                            continue\n","                        if 'remain' in q.lower() and yes_no == 'no':\n","                            continue\n","                        if d[\"_id\"] not in dials_with_scores[key]:\n","                            text = ''\n","                            dials_with_scores[key][d[\"_id\"]] = {\"topic\": d[\"topic\"], \"dataset\": key}\n","                            for message in d['messages']:\n","                                if message['role'] == 'admin' or 'modified_argument' not in message:\n","                                    continue\n","\n","                                text = text + '\\n\\n' + '<' + message['role'] + '>' + '\\n' + message['modified_argument']\n","                            dials_with_scores[key][d[\"_id\"]]['text'] = text.strip()\n","                            final_convs.append(text.strip())\n","\n","                    if 'good reasons' in q.lower():\n","                        if False and label == 'oum': ############## False to ensure that we always calcuclate the OUM after the conversation\n","                            final_labels.append(float(d['participant_info']['Questions'][q]['after']) - float(d['participant_info']['Questions'][q]['before']))\n","                        else:\n","                            final_labels.append(float(d['participant_info']['Questions'][q]['after']))\n","                        oum = d['participant_info']['Questions'][q]['after'] - d['participant_info']['Questions'][q]['before'] if \"final\" not in input_file else None\n","                        dials_with_scores[key][d[\"_id\"]][\"good_reasons\"] = {\"oum\": oum, \"after\": d['participant_info']['Questions'][q]['after']}\n","                        if 'before' in d['participant_info']['Questions'][q] and d['participant_info']['Questions'][q]['before'] != -1:\n","                            dials_with_scores[key][d[\"_id\"]][\"good_reasons\"]['before'] = d['participant_info']['Questions'][q]['before']\n","                        else:\n","                            dials_with_scores[key][d[\"_id\"]][\"good_reasons\"]['before'] = None\n","\n","    assert len(final_convs) == len(final_labels)\n","    return final_convs, final_labels\n","\n","\n","\n","def get_utterances_oum(text):\n","    # Splitting the input text into lines\n","    lines = text.split('\\n')\n","    # Variable to keep the cleaned lines\n","    cleaned_lines = []\n","    # Variable to keep track of whether the next line should be added\n","    add_next_line = False\n","    for line in lines:\n","        # If the line is a participant tag, set the flag to add the next line\n","        if line.strip() == '<participant>':\n","            add_next_line = True\n","        elif line.strip() in ['<woz>', '<chatbot>']:\n","            add_next_line = True\n","        elif add_next_line:\n","            # If the flag is set, add this line to the cleaned list and reset the flag\n","            cleaned_lines.append(line)\n","            add_next_line = False\n","    # Join the cleaned lines back into a single string\n","    cleaned_text = '\\n\\n'.join(cleaned_lines)\n","    return cleaned_text\n","\n","### Bot/Woz's data\n","conversations, labels = load_data_oum(label='after')\n","utterances = [get_utterances_oum(c) for c in conversations]\n"]},{"cell_type":"markdown","metadata":{"id":"b4h9oC4vZ-AT"},"source":["## 0.2. Wikitactics dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1724354946333,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"JexdnD0saAHk"},"outputs":[],"source":["import json\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","\n","def load_data_wikitac():\n","    with open('./wikitactics.json') as f:\n","        data = json.load(f)\n","\n","    conversations = []\n","    utterances_cleaned = []\n","    labels = []\n","    # Extract conversations/disputes for ESCALATED disputes\n","    for dispute in data:\n","        users = list()\n","        conversation = ''\n","        utt_cleaned = ''\n","        for utterance in dispute['utterances']:\n","            username = utterance['username']\n","            text = utterance['text']\n","            conversation += f\"<user_id={username}>\\n{text}\\n\\n\"\n","            utt_cleaned += text + '\\n\\n'\n","        conversations.append(conversation)\n","        utterances_cleaned.append(utt_cleaned)\n","        labels.append(dispute['escalation_label'])\n","\n","    return conversations, utterances_cleaned, labels\n","\n","conversations, utterances, labels = load_data_wikitac()"]},{"cell_type":"markdown","metadata":{"id":"-vQenNTlBNml"},"source":["## 0.3. Wikipedia for Deletion"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1724354960789,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"MKepAO6wBjtk"},"outputs":[],"source":["import collections\n","import random\n","import json\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","from convokit import Corpus, download\n","\n","def load_data_afd():\n","    # Load the data from the JSON file\n","    with open('afd_1000_randomised_dialogues.json', 'r') as json_file:\n","        data_dict = json.load(json_file)\n","\n","    # Extract the conversations, utterances, and labels from the data dictionary\n","    conversations = data_dict['conversations']\n","    utterances = data_dict['utterances']\n","    labels = data_dict['labels']\n","    labels = [1 if i == 0 else 0 for i in labels]\n","    return conversations, utterances, labels\n","\n","conversations, utterances, labels = load_data_afd()\n","labels"]},{"cell_type":"markdown","metadata":{"id":"zh9G81O2QTi9"},"source":["# 1. Reading Data & Training Models\n"]},{"cell_type":"markdown","metadata":{"id":"5Zu0OwizGPxE"},"source":["## 1.1. Loading and parsing dataset"]},{"cell_type":"markdown","metadata":{"id":"LWA1H2nxHo2N"},"source":["### 1.1.1. OUM dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opuDO8EDHbmI"},"outputs":[],"source":["conversations, labels = load_data_oum(label='after')\n","utterances = [get_utterances_oum(c) for c in conversations]"]},{"cell_type":"markdown","metadata":{"id":"CJ6eygOMa3nC"},"source":["### 1.1.2. Wikitactics dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1qWpID2a6Nx"},"outputs":[],"source":["conversations, utterances, labels = load_data_wikitac()"]},{"cell_type":"markdown","metadata":{"id":"8oM5wq5tBEdY"},"source":["### 1.1.3. Wikipedia for Deletion"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1724354964328,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"i6WseacjBLA5"},"outputs":[],"source":["conversations, utterances, labels = load_data_afd()"]},{"cell_type":"markdown","metadata":{"id":"hS_7KCSXGSmM"},"source":["## 1.2. Politeness Markers"]},{"cell_type":"markdown","metadata":{"id":"boAHipeTGwen"},"source":["### 1.2.0. Explaining the features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1716937572660,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-60},"id":"Xh27KminGwrT","outputId":"d7dd7441-ec21-4cf4-b655-b4c200931840"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nPlease:\\n\\nChecks if the word \"please\" appears anywhere in the sentence.\\n\\nStart with \\'Please\\':\\n\\nChecks if the sentence starts with the word \"please\".\\n\\nHas Subject Hedge:\\n\\nIdentifies if any subject in the sentence depends on a hedge word.\\n\\nUse of \\'by the way\\':\\n\\nChecks if the phrase \"by the way\" is used in the sentence.\\n\\nHedge words:\\n\\nIdentifies if any word in the sentence is a hedge word.\\n\\nAssert factuality:\\n\\nChecks for words that assert factuality, like \"in fact,\" \"actually,\" or \"really\".\\n\\nStart with Deference:\\n\\nChecks if the sentence starts with deferential words like \"great,\" \"good,\" or \"nice\".\\n\\nGratitude:\\n\\nIdentifies expressions of gratitude, like \"thank\" or \"thanks\".\\n\\nApologising:\\n\\nIdentifies apologetic expressions like \"sorry,\" \"oops,\" or \"I apologize\".\\n\\n1st person plural:\\n\\nChecks for first-person plural pronouns like \"we,\" \"our,\" or \"us\".\\n\\n1st person pronouns:\\n\\nIdentifies first-person singular pronouns like \"I,\" \"my,\" or \"mine\".\\n\\nStart with 1st person:\\n\\nChecks if the sentence starts with a first-person singular pronoun.\\n\\n2nd person pronouns:\\n\\nIdentifies second-person pronouns like \"you,\" \"your,\" or \"yours\".\\n\\nStart with 2nd person:\\n\\nChecks if the sentence starts with a second-person pronoun.\\n\\nStart with greeting:\\n\\nChecks if the sentence starts with a greeting like \"hi,\" \"hello,\" or \"hey\".\\n\\nStarts with Question:\\n\\nIdentifies if the sentence starts with a question word like \"what,\" \"why,\" \"who,\" or \"how\".\\n\\nStarts with Conjunction:\\n\\nChecks if the sentence starts with a conjunction or transition word like \"so,\" \"then,\" \"and,\" \"but,\" or \"or\".\\n\\nPositive words:\\n\\nIdentifies the presence of positive sentiment words.\\n\\nNegative words:\\n\\nIdentifies the presence of negative sentiment words.\\n\\nSubjunctive words:\\n\\nChecks for the use of subjunctive mood words like \"could\" or \"would\" when preceded by \"you\".\\n\\nIndicative words:\\n\\nChecks for the use of indicative mood words like \"can\" or \"will\" when preceded by \"you\".\\n'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","Please:\n","\n","Checks if the word \"please\" appears anywhere in the sentence.\n","\n","Start with 'Please':\n","\n","Checks if the sentence starts with the word \"please\".\n","\n","Has Subject Hedge:\n","\n","Identifies if any subject in the sentence depends on a hedge word.\n","\n","Use of 'by the way':\n","\n","Checks if the phrase \"by the way\" is used in the sentence.\n","\n","Hedge words:\n","\n","Identifies if any word in the sentence is a hedge word.\n","\n","Assert factuality:\n","\n","Checks for words that assert factuality, like \"in fact,\" \"actually,\" or \"really\".\n","\n","Start with Deference:\n","\n","Checks if the sentence starts with deferential words like \"great,\" \"good,\" or \"nice\".\n","\n","Gratitude:\n","\n","Identifies expressions of gratitude, like \"thank\" or \"thanks\".\n","\n","Apologising:\n","\n","Identifies apologetic expressions like \"sorry,\" \"oops,\" or \"I apologize\".\n","\n","1st person plural:\n","\n","Checks for first-person plural pronouns like \"we,\" \"our,\" or \"us\".\n","\n","1st person pronouns:\n","\n","Identifies first-person singular pronouns like \"I,\" \"my,\" or \"mine\".\n","\n","Start with 1st person:\n","\n","Checks if the sentence starts with a first-person singular pronoun.\n","\n","2nd person pronouns:\n","\n","Identifies second-person pronouns like \"you,\" \"your,\" or \"yours\".\n","\n","Start with 2nd person:\n","\n","Checks if the sentence starts with a second-person pronoun.\n","\n","Start with greeting:\n","\n","Checks if the sentence starts with a greeting like \"hi,\" \"hello,\" or \"hey\".\n","\n","Starts with Question:\n","\n","Identifies if the sentence starts with a question word like \"what,\" \"why,\" \"who,\" or \"how\".\n","\n","Starts with Conjunction:\n","\n","Checks if the sentence starts with a conjunction or transition word like \"so,\" \"then,\" \"and,\" \"but,\" or \"or\".\n","\n","Positive words:\n","\n","Identifies the presence of positive sentiment words.\n","\n","Negative words:\n","\n","Identifies the presence of negative sentiment words.\n","\n","Subjunctive words:\n","\n","Checks for the use of subjunctive mood words like \"could\" or \"would\" when preceded by \"you\".\n","\n","Indicative words:\n","\n","Checks for the use of indicative mood words like \"can\" or \"will\" when preceded by \"you\".\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1716937573285,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-60},"id":"NqWjWODMNh9K","outputId":"461a3926-4f78-4ce5-b72e-1e94ede53042"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n# words: This represents the total number of words in a message. It is calculated by counting the number of tokens (words) in the message.\\n\\n# me pronoun: This feature counts the number of first-person singular pronouns (e.g., \"I\", \"me\", \"my\") in a message. These pronouns are specified in the pron_me list of the lexicons dictionary.\\n\\n# we pronoun: This feature counts the number of first-person plural pronouns (e.g., \"we\", \"us\", \"our\") in a message. These pronouns are specified in the pron_we list of the lexicons dictionary.\\n\\n# you pronoun: This feature counts the number of second-person pronouns (e.g., \"you\", \"your\") in a message. These pronouns are specified in the pron_you list of the lexicons dictionary.\\n\\n# 3rd person pronouns: This feature counts the number of third-person pronouns (e.g., \"he\", \"she\", \"they\") in a message. These pronouns are specified in the pron_3rd list of the lexicons dictionary.\\n\\n# Geography terms: This feature counts the number of geography-related terms in a message. These terms are specified in the geo list, which is loaded from the my_geo.txt file in the lexicons directory.\\n\\n# Meta terms: This feature counts the number of meta-discourse terms in a message. These terms are specified in the meta list, which is loaded from the my_meta.txt file in the lexicons directory.\\n# Meta terms: Terms are associated with the functionalities, actions, and elements within the StreetCrowd environment\\n\\n# Certainty terms: This feature counts the number of words expressing certainty in a message. These terms are specified in the certain list, which is loaded from the my_certain.txt file in the lexicons directory.\\n\\n# Hedging terms: This feature counts the number of hedging terms (words that indicate uncertainty) in a message. These terms are specified in the hedge list, which is loaded from the my_hedges.txt file in the lexicons directory.\\n\\n# New content words: This represents the number of new content words introduced by a user in a message. New content words are content words (significant words such as nouns, verbs, adjectives) that have not been seen in any previous messages. They exclude common stopwords and are identified based on part-of-speech tags.\\n\\n# New content words * # Certainty terms: This represents the number of new content words introduced in a message that are also accompanied by certainty terms. It is calculated as the product of the number of new content words (# New content words) and the count of certainty terms (# Certainty terms) in the message.\\n\\n# New content words * # Hedging terms: This represents the number of new content words introduced in a message that are also accompanied by hedging terms. It is calculated as the product of the number of new content words (# New content words) and the count of hedging terms (# Hedging terms) in the message.\\n\\nThese features are designed to capture various aspects of the language used in messages, including the use of personal pronouns, specific types of terms, and the introduction of new content in the conversation. The multiplication features specifically look at how new content words interact with expressions of certainty and hedging within the messages.\\n'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","# words: This represents the total number of words in a message. It is calculated by counting the number of tokens (words) in the message.\n","\n","# me pronoun: This feature counts the number of first-person singular pronouns (e.g., \"I\", \"me\", \"my\") in a message. These pronouns are specified in the pron_me list of the lexicons dictionary.\n","\n","# we pronoun: This feature counts the number of first-person plural pronouns (e.g., \"we\", \"us\", \"our\") in a message. These pronouns are specified in the pron_we list of the lexicons dictionary.\n","\n","# you pronoun: This feature counts the number of second-person pronouns (e.g., \"you\", \"your\") in a message. These pronouns are specified in the pron_you list of the lexicons dictionary.\n","\n","# 3rd person pronouns: This feature counts the number of third-person pronouns (e.g., \"he\", \"she\", \"they\") in a message. These pronouns are specified in the pron_3rd list of the lexicons dictionary.\n","\n","# Geography terms: This feature counts the number of geography-related terms in a message. These terms are specified in the geo list, which is loaded from the my_geo.txt file in the lexicons directory.\n","\n","# Meta terms: This feature counts the number of meta-discourse terms in a message. These terms are specified in the meta list, which is loaded from the my_meta.txt file in the lexicons directory.\n","# Meta terms: Terms are associated with the functionalities, actions, and elements within the StreetCrowd environment\n","\n","# Certainty terms: This feature counts the number of words expressing certainty in a message. These terms are specified in the certain list, which is loaded from the my_certain.txt file in the lexicons directory.\n","\n","# Hedging terms: This feature counts the number of hedging terms (words that indicate uncertainty) in a message. These terms are specified in the hedge list, which is loaded from the my_hedges.txt file in the lexicons directory.\n","\n","# New content words: This represents the number of new content words introduced by a user in a message. New content words are content words (significant words such as nouns, verbs, adjectives) that have not been seen in any previous messages. They exclude common stopwords and are identified based on part-of-speech tags.\n","\n","# New content words * # Certainty terms: This represents the number of new content words introduced in a message that are also accompanied by certainty terms. It is calculated as the product of the number of new content words (# New content words) and the count of certainty terms (# Certainty terms) in the message.\n","\n","# New content words * # Hedging terms: This represents the number of new content words introduced in a message that are also accompanied by hedging terms. It is calculated as the product of the number of new content words (# New content words) and the count of hedging terms (# Hedging terms) in the message.\n","\n","These features are designed to capture various aspects of the language used in messages, including the use of personal pronouns, specific types of terms, and the introduction of new content in the conversation. The multiplication features specifically look at how new content words interact with expressions of certainty and hedging within the messages.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"23GYTcGLVBgR"},"source":["### 1.2.1. Generating Politeness Markers"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":565},"executionInfo":{"elapsed":282,"status":"ok","timestamp":1724355062095,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"DQHYKjeaDMDJ","outputId":"bf2feaaa-4819-4b37-e6b0-45aa57f2ddb3"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"poli_feat_df"},"text/html":["\n","  <div id=\"df-67aacdaf-98be-4b84-833f-4ee16ec758a1\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PM - Please, x̄</th>\n","      <th>PM - Start with 'Please', x̄</th>\n","      <th>PM - Has Subject Hedge, x̄</th>\n","      <th>PM - Use of 'by the way', x̄</th>\n","      <th>PM - Hedge words, x̄</th>\n","      <th>PM - Assert factuality, x̄</th>\n","      <th>PM - Start with deference, x̄</th>\n","      <th>PM - Gratitude, x̄</th>\n","      <th>PM - Apologising, x̄</th>\n","      <th>PM - 1st person plural, x̄</th>\n","      <th>...</th>\n","      <th>PM - Start with 1st person, ∇</th>\n","      <th>PM - 2nd person pronouns, ∇</th>\n","      <th>PM - Start with 2nd person, ∇</th>\n","      <th>PM - Start with greeting, ∇</th>\n","      <th>PM - Starts with Question, ∇</th>\n","      <th>PM - Starts with Conjunction, ∇</th>\n","      <th>PM - Positive sentiment words, ∇</th>\n","      <th>PM - Negative sentiment words, ∇</th>\n","      <th>PM - Subjunctive words, ∇</th>\n","      <th>PM - Indicative words, ∇</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.250000</td>\n","      <td>...</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>-0.300000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.272727</td>\n","      <td>0.0</td>\n","      <td>0.090909</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.181818</td>\n","      <td>...</td>\n","      <td>5.891684e-18</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.036364</td>\n","      <td>0.063636</td>\n","      <td>-0.063636</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.100000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.100000</td>\n","      <td>...</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>-0.054545</td>\n","      <td>0.030303</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.200000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.200000</td>\n","      <td>...</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>-0.200000</td>\n","      <td>-0.100000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.444444</td>\n","      <td>0.0</td>\n","      <td>0.333333</td>\n","      <td>0.111111</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.111111</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>-0.016667</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>-0.066667</td>\n","      <td>-0.016667</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-0.500000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.250000</td>\n","      <td>0.0</td>\n","      <td>0.062500</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.007353</td>\n","      <td>-0.033824</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.400000</td>\n","      <td>0.0</td>\n","      <td>0.200000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>-1.000000e-01</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.100000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.333333</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>8.571429e-02</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.085714</td>\n","      <td>-0.142857</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>-0.400000</td>\n","      <td>-0.400000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 42 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67aacdaf-98be-4b84-833f-4ee16ec758a1')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-67aacdaf-98be-4b84-833f-4ee16ec758a1 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-67aacdaf-98be-4b84-833f-4ee16ec758a1');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-10702d8c-bd4c-4ba9-aaaf-638683d1a7d3\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-10702d8c-bd4c-4ba9-aaaf-638683d1a7d3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-10702d8c-bd4c-4ba9-aaaf-638683d1a7d3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_66c50859-c577-48d5-99a7-be302874347b\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('poli_feat_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_66c50859-c577-48d5-99a7-be302874347b button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('poli_feat_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["     PM - Please, x̄  PM - Start with 'Please', x̄  \\\n","0                0.0                           0.0   \n","1                0.0                           0.0   \n","2                0.0                           0.0   \n","3                0.0                           0.0   \n","4                0.0                           0.0   \n","..               ...                           ...   \n","995              0.0                           0.0   \n","996              0.0                           0.0   \n","997              0.0                           0.0   \n","998              0.0                           0.0   \n","999              0.0                           0.0   \n","\n","     PM - Has Subject Hedge, x̄  PM - Use of 'by the way', x̄  \\\n","0                      0.000000                           0.0   \n","1                      0.272727                           0.0   \n","2                      0.000000                           0.0   \n","3                      0.200000                           0.0   \n","4                      0.444444                           0.0   \n","..                          ...                           ...   \n","995                    0.000000                           0.0   \n","996                    0.250000                           0.0   \n","997                    0.400000                           0.0   \n","998                    0.333333                           0.0   \n","999                    0.000000                           0.0   \n","\n","     PM - Hedge words, x̄  PM - Assert factuality, x̄  \\\n","0                0.000000                    0.000000   \n","1                0.090909                    0.000000   \n","2                0.000000                    0.100000   \n","3                0.000000                    0.000000   \n","4                0.333333                    0.111111   \n","..                    ...                         ...   \n","995              0.000000                    0.000000   \n","996              0.062500                    0.000000   \n","997              0.200000                    0.000000   \n","998              0.000000                    0.000000   \n","999              0.000000                    0.000000   \n","\n","     PM - Start with deference, x̄  PM - Gratitude, x̄  PM - Apologising, x̄  \\\n","0                              0.0                 0.0              0.000000   \n","1                              0.0                 0.0              0.000000   \n","2                              0.0                 0.0              0.000000   \n","3                              0.0                 0.0              0.000000   \n","4                              0.0                 0.0              0.111111   \n","..                             ...                 ...                   ...   \n","995                            0.0                 0.0              0.000000   \n","996                            0.0                 0.0              0.000000   \n","997                            0.0                 0.0              0.000000   \n","998                            0.0                 0.0              0.000000   \n","999                            0.0                 0.0              0.000000   \n","\n","     PM - 1st person plural, x̄  ...  PM - Start with 1st person, ∇  \\\n","0                      0.250000  ...                   0.000000e+00   \n","1                      0.181818  ...                   5.891684e-18   \n","2                      0.100000  ...                   0.000000e+00   \n","3                      0.200000  ...                   0.000000e+00   \n","4                      0.000000  ...                   0.000000e+00   \n","..                          ...  ...                            ...   \n","995                    0.000000  ...                   0.000000e+00   \n","996                    0.000000  ...                   0.000000e+00   \n","997                    0.000000  ...                  -1.000000e-01   \n","998                    0.000000  ...                   8.571429e-02   \n","999                    0.000000  ...                   0.000000e+00   \n","\n","     PM - 2nd person pronouns, ∇  PM - Start with 2nd person, ∇  \\\n","0                            0.0                       0.000000   \n","1                            0.0                       0.000000   \n","2                            0.0                       0.000000   \n","3                            0.0                       0.000000   \n","4                            0.0                      -0.016667   \n","..                           ...                            ...   \n","995                          0.0                       0.000000   \n","996                          0.0                       0.000000   \n","997                          0.0                       0.000000   \n","998                          0.0                       0.000000   \n","999                          0.0                       0.000000   \n","\n","     PM - Start with greeting, ∇  PM - Starts with Question, ∇  \\\n","0                            0.0                           0.0   \n","1                            0.0                           0.0   \n","2                            0.0                           0.0   \n","3                            0.0                           0.0   \n","4                            0.0                           0.0   \n","..                           ...                           ...   \n","995                          0.0                           0.0   \n","996                          0.0                           0.0   \n","997                          0.0                           0.0   \n","998                          0.0                           0.0   \n","999                          0.0                           0.0   \n","\n","     PM - Starts with Conjunction, ∇  PM - Positive sentiment words, ∇  \\\n","0                           0.000000                         -0.300000   \n","1                           0.036364                          0.063636   \n","2                           0.000000                         -0.054545   \n","3                           0.000000                         -0.200000   \n","4                           0.000000                         -0.066667   \n","..                               ...                               ...   \n","995                         0.000000                          0.000000   \n","996                         0.000000                          0.007353   \n","997                         0.000000                          0.100000   \n","998                         0.000000                          0.085714   \n","999                         0.000000                         -0.400000   \n","\n","     PM - Negative sentiment words, ∇  PM - Subjunctive words, ∇  \\\n","0                            0.000000                        0.0   \n","1                           -0.063636                        0.0   \n","2                            0.030303                        0.0   \n","3                           -0.100000                        0.0   \n","4                           -0.016667                        0.0   \n","..                                ...                        ...   \n","995                         -0.500000                        0.0   \n","996                         -0.033824                        0.0   \n","997                          0.000000                        0.0   \n","998                         -0.142857                        0.0   \n","999                         -0.400000                        0.0   \n","\n","     PM - Indicative words, ∇  \n","0                         0.0  \n","1                         0.0  \n","2                         0.0  \n","3                         0.0  \n","4                         0.0  \n","..                        ...  \n","995                       0.0  \n","996                       0.0  \n","997                       0.0  \n","998                       0.0  \n","999                       0.0  \n","\n","[1000 rows x 42 columns]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["from convokit import PolitenessStrategies\n","from collections import defaultdict\n","import numpy as np\n","\n","if len(conversations) != 213 and len(conversations) != 542 and len(conversations) != 1000:\n","    ps = PolitenessStrategies()\n","    spacy_nlp = spacy.load('en_core_web_sm', disable=['ner'])\n","\n","    poli_features_mean = defaultdict(list)\n","    poli_features_grad = defaultdict(list)\n","\n","    for i, c in enumerate(utterances):\n","        if i%100==0:\n","            print(i)\n","        poli_evolution = defaultdict(list)\n","        for u in c.split('\\n\\n'):\n","            utt = ps.transform_utterance(u, spacy_nlp=spacy_nlp, markers=True)\n","            for feature, presence in utt.meta['politeness_strategies'].items():\n","                poli_evolution[feature].append(presence)\n","        for feature in utt.meta['politeness_strategies'].keys():\n","            poli_features_mean[feature].append(np.mean(poli_evolution[feature]))   # Calculate the mean\n","\n","            x = np.arange(len(poli_evolution[feature]))  # X-coordinates (indices of the list)\n","            y = poli_evolution[feature]                  # Y-coordinates (values of the list)\n","            try:\n","                slope, _ = np.polyfit(x, y, 1)       # Fitting a linear regression (polynomial of degree 1) to the data\n","            except Exception as e:\n","                print(e)\n","                slope = 0 \n","            poli_features_grad[feature].append(slope)    # The slope is the gradient we want\n","\n","    a = pd.DataFrame(poli_features_mean).add_suffix('_mean')\n","    b = pd.DataFrame(poli_features_grad).add_suffix('_grad')\n","    poli_feat_df = pd.concat([a, b], axis=1)\n","\n","else:\n","    if len(conversations) == 213:\n","        poli_feat_df = pd.read_csv('213_wikitac_politeness_markers.csv') \n","    if len(conversations) == 542:\n","        poli_feat_df = pd.read_csv('542_oum_politeness_markers.csv') \n","    if len(conversations) == 1000:\n","        poli_feat_df = pd.read_csv('1000_afd_politeness_markers.csv')\n","\n","# Detailed codes: https://github.com/CornellNLP/ConvoKit/blob/master/convokit/politeness_collections/politeness_api/features/politeness_strategies.py\n","feature_names = ['Please', \"Start with 'Please'\", 'Has Subject Hedge', \"Use of 'by the way'\",\n","                 \"Hedge words\", \"Assert factuality\", \"Start with deference\", \"Gratitude\", \"Apologising\",\n","                 \"1st person plural\", \"1st person pronouns\", \"Start with 1st person\",\n","                 \"2nd person pronouns\", \"Start with 2nd person\",\n","                 \"Start with greeting\", \"Starts with Question\", \"Starts with Conjunction\",\n","                 \"Positive sentiment words\", \"Negative sentiment words\", \"Subjunctive words\", \"Indicative words\"]\n","\n","feature_names = [f + ', x̄' for f in feature_names] +  [f + ', ∇' for f in feature_names]\n","feature_names = ['PM - ' + f for f in feature_names]\n","poli_feat_df.columns = feature_names\n","poli_feat_df"]},{"cell_type":"markdown","metadata":{"id":"sA37XSB8VGmh"},"source":["## 1.3. Training the Feature-based Model for the OUM dataset (Regression).\n","All LLM-generated features and discrete features about collaboration markers (in tandem with the politeness markers above) should have been already generated; see \"feature_engineering\" folder for details on how we generate those feature sets beyond politeness markers."]},{"cell_type":"markdown","metadata":{"id":"0gZYvp-LfmDo"},"source":["### 1.3.1 Reading Data"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":284,"status":"ok","timestamp":1724354967396,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"EvgWlJyh6cNu"},"outputs":[],"source":["import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","from scipy.stats import spearmanr\n","import matplotlib.pyplot as plt\n","\n","# X = poli_feat_df  ### Politeness Markers Data\n","y = pd.DataFrame(data={'labels':labels})['labels']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if len(conversations) == 542:\n","    colab_markers_df = pd.read_csv('542_oum_collaboration_markers.csv')  ### Collaboration Markers DATA\n","if len(conversations) == 213:\n","    colab_markers_df = pd.read_csv('213_wikitac_collaboration_markers.csv')\n","if len(conversations) == 1000:\n","    colab_markers_df = pd.read_csv('1000_afd_collaboration_markers.csv')\n","\n","feature_names = ['# words', '# me pronoun', '# we pronoun', '# you pronoun', '# 3rd person pronouns',\n","                 '# Geography terms', '# Meta terms', '# Certainty terms', '# Headging terms',\n","                 '# New content words', '# New content words * # Certainty terms', '# New content words * # Hedging terms']\n","\n","feature_names = [f + ', x̄' for f in feature_names] +  [f + ', ∇' for f in feature_names]\n","feature_names = ['CM - ' + f for f in feature_names]\n","colab_markers_df.columns = feature_names"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if len(conversations) == 542:\n","    dispute_tactics_df = pd.read_csv('542_oum_gpt4_dispute_tactics.csv')  ### Dispute Tactics \n","if len(conversations) == 213:\n","    dispute_tactics_df = pd.read_csv('213_wikitac_gpt4_dispute_tactics.csv')\n","if len(conversations) == 1000:\n","    dispute_tactics_df = pd.read_csv('1000_afd_gpt4_dispute_tactics.csv')\n","\n","feature_names = ['Name calling/hostility', 'Ad hominem/ad argument', 'Attempted derailing/off-topic', 'Policing the discussion',\n","                 'Stating your stance/repeated argument', 'Counterargument', 'Refutation', 'Refuting the central point',\n","                 'Bailing out', 'Contextualisation', 'Asking questions', 'Providing clarification', 'Suggesting a compromise',\n","                 'Coordinating', 'Conceding/recanting', 'I don’t know', 'Others non-disagreement tactics']\n","\n","feature_names = [f + ', x̄' for f in feature_names] +  [f + ', ∇' for f in feature_names]\n","feature_names = ['DT - ' + f for f in feature_names]\n","dispute_tactics_df.columns = feature_names"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1724355011079,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"9-AEMDTcP9Cx"},"outputs":[],"source":["if len(conversations) == 542:\n","    AQ_feat_df = pd.read_csv('542_oum_gpt4_AQ_1dim.csv') ### Argument Quality \n","if len(conversations) == 213:\n","    AQ_feat_df = pd.read_csv('213_wikitac_gpt4_AQ_1dim.csv')\n","if len(conversations) == 1000:\n","    AQ_feat_df = pd.read_csv('1000_afd_gpt4_AQ_1dim.csv')\n","    aq_mean = AQ_feat_df['AQ'].mean() # row 10 is empty\n","    AQ_feat_df['AQ'] = AQ_feat_df['AQ'].fillna(aq_mean)\n","AQ_feat_df['Quality of Arguments'] = AQ_feat_df['AQ'] # Rename\n","del AQ_feat_df['AQ']"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":259,"status":"ok","timestamp":1724355019756,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"tXu-l298AwSG","outputId":"c90045bf-f9e2-40db-89b7-723b0fc777bd"},"outputs":[{"data":{"text/plain":["Index(['IC - Low propositional density, x̄',\n","       'IC - High propositional density, x̄', 'IC - low Content density, x̄',\n","       'IC - high Content density, x̄', 'ST - Low formality, x̄',\n","       'ST - High formality, x̄', 'ST - Low politeness, x̄',\n","       'ST - High politeness, x̄', 'ST - Negative sentiment, x̄',\n","       'ST - Neutral sentiment, x̄', 'ST - Positive sentiment, x̄',\n","       'ST - Epistemic uncertainty, x̄', 'ST - Doxastic uncertainty, x̄',\n","       'ST - Investigative uncertainty, x̄',\n","       'ST - Conditional uncertainty, x̄', 'ST - No uncertainty, x̄',\n","       'IC - Low propositional density, ∇',\n","       'IC - High propositional density, ∇', 'IC - low Content density, ∇',\n","       'IC - high Content density, ∇', 'ST - Low formality, ∇',\n","       'ST - High formality, ∇', 'ST - Low politeness, ∇',\n","       'ST - High politeness, ∇', 'ST - Negative sentiment, ∇',\n","       'ST - Neutral sentiment, ∇', 'ST - Positive sentiment, ∇',\n","       'ST - Epistemic uncertainty, ∇', 'ST - Doxastic uncertainty, ∇',\n","       'ST - Investigative uncertainty, ∇', 'ST - Conditional uncertainty, ∇',\n","       'ST - No uncertainty, ∇'],\n","      dtype='object')"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["if len(conversations) == 542:\n","    crlf_df = pd.read_csv('542_oum_gpt4_crlf.csv')  ### Information Content & Styles and Tones\n","if len(conversations) == 213:\n","    crlf_df = pd.read_csv('213_wikitac_gpt4_crlf.csv')\n","if len(conversations) == 1000:\n","    crlf_df = pd.read_csv('1000_afd_gpt4_crlf.csv')\n","\n","feature_names = ['Low propositional density', 'High propositional density', 'low Content density', 'high Content density',\n","                 'Low formality', 'High formality',\n","                 'Low politeness', 'High politeness',\n","                 'Negative sentiment', 'Neutral sentiment', 'Positive sentiment',\n","                 'Epistemic uncertainty', 'Doxastic uncertainty', 'Investigative uncertainty', 'Conditional uncertainty', 'No uncertainty']\n","\n","feature_names = [f + ', x̄' for f in feature_names] +  [f + ', ∇' for f in feature_names]\n","feature_names = ['IC - ' + f if 'density' in f else f for f in feature_names]\n","feature_names = ['ST - ' + f if 'density' not in f else f for f in feature_names]\n","crlf_df.columns = feature_names"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":286,"status":"ok","timestamp":1724355067503,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"QTRca-Kmz18k"},"outputs":[],"source":["N = len(conversations)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":392,"status":"ok","timestamp":1724355078458,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"gyW3l78oGa6U","outputId":"a424016e-d082-4186-e48b-4a2658b5af19"},"outputs":[{"name":"stdout","output_type":"stream","text":["No constant columns found in X.\n"]},{"data":{"text/plain":["Index(['DT - Name calling/hostility, x̄', 'DT - Ad hominem/ad argument, x̄',\n","       'DT - Attempted derailing/off-topic, x̄',\n","       'DT - Policing the discussion, x̄',\n","       'DT - Stating your stance/repeated argument, x̄',\n","       'DT - Counterargument, x̄', 'DT - Refutation, x̄',\n","       'DT - Refuting the central point, x̄', 'DT - Bailing out, x̄',\n","       'DT - Contextualisation, x̄', 'DT - Asking questions, x̄',\n","       'DT - Providing clarification, x̄', 'DT - Suggesting a compromise, x̄',\n","       'DT - Coordinating, x̄', 'DT - Conceding/recanting, x̄',\n","       'DT - I don’t know, x̄', 'DT - Others non-disagreement tactics, x̄',\n","       'DT - Name calling/hostility, ∇', 'DT - Ad hominem/ad argument, ∇',\n","       'DT - Attempted derailing/off-topic, ∇',\n","       'DT - Policing the discussion, ∇',\n","       'DT - Stating your stance/repeated argument, ∇',\n","       'DT - Counterargument, ∇', 'DT - Refutation, ∇',\n","       'DT - Refuting the central point, ∇', 'DT - Bailing out, ∇',\n","       'DT - Contextualisation, ∇', 'DT - Asking questions, ∇',\n","       'DT - Providing clarification, ∇', 'DT - Suggesting a compromise, ∇',\n","       'DT - Coordinating, ∇', 'DT - Conceding/recanting, ∇',\n","       'DT - I don’t know, ∇', 'DT - Others non-disagreement tactics, ∇'],\n","      dtype='object')"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["X = pd.concat([poli_feat_df.iloc[:N,:],\n","               colab_markers_df.iloc[:N, :],\n","               AQ_feat_df.iloc[:N, -1:],\n","               dispute_tactics_df.iloc[:N, :],\n","               crlf_df.iloc[:, :] # crlf_df.iloc[:, [0, 1, 2, 3, 16, 17, 18, 19]] --> Information Content; [i for i in range(32) if i not in [0, 1, 2, 3, 16, 17, 18, 19]] --> Style and Tone\n","               ], axis=1)\n","\n","constant_columns = X.columns[X.nunique() == 1]\n","X = X.drop(columns=constant_columns)\n","if len(constant_columns) > 0:\n","    print(\"Removed constant columns:\", constant_columns)\n","else:\n","    print(\"No constant columns found in X.\")\n","\n","X = X.reset_index(drop=True)\n","full_data = pd.concat([X, pd.DataFrame(data={'Target variable':labels})], axis = 1)"]},{"cell_type":"markdown","metadata":{"id":"DWKWmNjOD4o9"},"source":["### 1.3.2. Ridge Linear Regression model / Logistic Regression "]},{"cell_type":"markdown","metadata":{"id":"Xex3OvxDj8CW"},"source":["#### 1.3.2.1. Ridge Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWVKX6pF1IMW"},"outputs":[],"source":["def conv2topic(c):\n","  if 'Brexit' in c:\n","    return 'Brexit'\n","\n","  elif 'veganism' in c or 'vegan' in c:\n","    return 'veganism'\n","\n","  elif 'vaccination' in c or 'vaccine' in c:\n","    return 'vaccination'\n","\n","  else:\n","    raise 'NO TOPIC WAS CHOSEN!'\n","\n","if len(conversations) == 542:\n","    topics = [conv2topic(c) for c in conversations]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1915,"status":"ok","timestamp":1718480546545,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-60},"id":"JHUZ1JuCgqKI","outputId":"257547cf-10b7-4d0d-d6f4-cd61491735ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finding optimal alpha with seed=1\n","\n","ALPHA: 0.1\n","MAE: 1.4702449850389454\n","Spearman Correlation: 0.34619843919521837\n","Mean Score: 1.0620232729218635\n","\n","ALPHA: 1\n","MAE: 1.4731830445589564\n","Spearman Correlation: 0.33350949810580227\n","Mean Score: 1.069836773226577\n","\n","ALPHA: 10\n","MAE: 1.513405180056012\n","Spearman Correlation: 0.3146294194720946\n","Mean Score: 1.0993878802919586\n","\n","ALPHA: 100\n","MAE: 1.59368241797387\n","Spearman Correlation: 0.22172861349417122\n","Mean Score: 1.1859769022398494\n","\n","Optimal alpha found: 0.1\n","\n","Evaluating with seed=42\n","MAE: 1.4593720820279288\n","Spearman Correlation: 0.3532750676518215\n","\n","Evaluating with seed=123\n","MAE: 1.4643285954159402\n","Spearman Correlation: 0.3548715651200828\n","\n","Evaluating with seed=456\n","MAE: 1.4799838190812995\n","Spearman Correlation: 0.33581348645748044\n","\n","Results for optimal alpha=0.1:\n","Mean MAE Test: 1.468 (0.009)\n","Mean Spearman Correlation Test: 0.348 (0.009)\n"]}],"source":["from sklearn.model_selection import KFold\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_absolute_error\n","from scipy.stats import spearmanr\n","import numpy as np\n","import pandas as pd\n","\n","# Different seeds for reproducibility\n","seeds = [42, 123, 456]\n","\n","# Alpha values to test\n","alphas = [0.1, 1, 10, 100]\n","\n","# To store the results for finding the optimal alpha\n","optimal_results = []\n","\n","# Step 1: Finding the optimal alpha based on seed=1\n","print('Finding optimal alpha with seed=1')\n","kf = KFold(n_splits=7, shuffle=True, random_state=1)\n","\n","for alpha in alphas:\n","    print('\\nALPHA:', alpha)\n","    model = Ridge(alpha=alpha)\n","\n","    all_y_pred_test = []\n","    all_y_test = []\n","\n","    for train_index, test_index in kf.split(X):\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","        model.fit(X_train, y_train)\n","\n","        y_pred_test = model.predict(X_test)\n","        all_y_pred_test.extend(y_pred_test)\n","        all_y_test.extend(y_test)\n","\n","    mae_test = mean_absolute_error(all_y_test, all_y_pred_test)\n","    corr_test, _ = spearmanr(all_y_pred_test, all_y_test)\n","    mean_score = (mae_test + (1 - corr_test)) / 2  \n","\n","    optimal_results.append((alpha, mean_score))\n","    print('MAE:', mae_test)\n","    print('Spearman Correlation:', corr_test)\n","    print('Mean Score:', mean_score)\n","\n","optimal_alpha = min(optimal_results, key=lambda x: x[1])[0]\n","print(f'\\nOptimal alpha found: {optimal_alpha}')\n","\n","# Step 2: Evaluate the optimal alpha across different seeds and report mean and SD\n","results = {'mae_test': [], 'corr_test': []}\n","\n","for seed in seeds:\n","    print(f'\\nEvaluating with seed={seed}')\n","    kf = KFold(n_splits=7, shuffle=True, random_state=seed)\n","    model = Ridge(alpha=optimal_alpha, max_iter=10000)\n","\n","    all_y_pred_test = []\n","    all_y_test = []\n","\n","    for train_index, test_index in kf.split(X):\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","        model.fit(X_train, y_train)\n","\n","        y_pred_test = model.predict(X_test)\n","        all_y_pred_test.extend(y_pred_test)\n","        all_y_test.extend(y_test)\n","\n","    mae_test = mean_absolute_error(all_y_test, all_y_pred_test)\n","    corr_test, _ = spearmanr(all_y_pred_test, all_y_test)\n","\n","    results['mae_test'].append(mae_test)\n","    results['corr_test'].append(corr_test)\n","\n","    print('MAE:', mae_test)\n","    print('Spearman Correlation:', corr_test)\n","\n","# Calculate mean and standard deviation for MAE and Spearman correlation\n","mean_mae_test = np.mean(results['mae_test'])\n","std_mae_test = np.std(results['mae_test'])\n","mean_corr_test = np.mean(results['corr_test'])\n","std_corr_test = np.std(results['corr_test'])\n","\n","print(f'\\nResults for optimal alpha={optimal_alpha}:')\n","print(f'Mean MAE Test: {mean_mae_test:.3f} ({std_mae_test:.3f})')\n","print(f'Mean Spearman Correlation Test: {mean_corr_test:.3f} ({std_corr_test:.3f})')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1071,"status":"ok","timestamp":1718480547614,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-60},"id":"E0gL5dr78mZb","outputId":"1c6de1bc-91a1-4525-a771-88bc3cb8b4e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","TEST:\n","Mean Absolute Error (MAE): 1.4628048820059367\n","Spearman Correlation: 0.34296528152139055\n","\n","TEST - Brexit:\n","Mean Absolute Error (MAE): 1.3823092521502698\n","Spearman Correlation: 0.4606229905032811\n","\n","\n","\n","TEST - vaccination:\n","Mean Absolute Error (MAE): 1.4718787399484359\n","Spearman Correlation: 0.337556617484527\n","\n","\n","\n","TEST - veganism:\n","Mean Absolute Error (MAE): 1.5131915962086917\n","Spearman Correlation: 0.2519040930195204\n","\n","\n","\n","\n","\n","\n","TEST:\n","Mean Absolute Error (MAE): 1.470847255417941\n","Spearman Correlation: 0.34228316462234165\n","\n","TEST - Brexit:\n","Mean Absolute Error (MAE): 1.551764511217121\n","Spearman Correlation: 0.34454990674098507\n","\n","\n","\n","TEST - vaccination:\n","Mean Absolute Error (MAE): 1.383218223303109\n","Spearman Correlation: 0.43167746893831693\n","\n","\n","\n","TEST - veganism:\n","Mean Absolute Error (MAE): 1.4929316473572813\n","Spearman Correlation: 0.25318642627490795\n","\n","\n","\n","\n","\n","\n","TEST:\n","Mean Absolute Error (MAE): 1.475594359226507\n","Spearman Correlation: 0.33301438686768214\n","\n","TEST - Brexit:\n","Mean Absolute Error (MAE): 1.427129412060035\n","Spearman Correlation: 0.31519654385159473\n","\n","\n","\n","TEST - vaccination:\n","Mean Absolute Error (MAE): 1.5268320043576187\n","Spearman Correlation: 0.3482015685373433\n","\n","\n","\n","TEST - veganism:\n","Mean Absolute Error (MAE): 1.4635226052952532\n","Spearman Correlation: 0.35562577217952474\n","\n","\n","Brexit - MAE Mean: 1.454(0.072)\n","Brexit - Correlation Mean: 0.373 (0.063)\n","vaccination - MAE Mean: 1.461(0.059)\n","vaccination - Correlation Mean: 0.372 (0.042)\n","veganism - MAE Mean: 1.49(0.02)\n","veganism - Correlation Mean: 0.287 (0.049)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import KFold\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_absolute_error\n","from scipy.stats import spearmanr\n","\n","# Function to calculate mean and standard deviation\n","def calculate_mean_sd(values):\n","    return np.mean(values), np.std(values)\n","\n","# Initialize dictionaries to store the results\n","results = {\n","    'Brexit': {'mae': [], 'corr': []},\n","    'vaccination': {'mae': [], 'corr': []},\n","    'veganism': {'mae': [], 'corr': []}\n","}\n","\n","for seed in [42, 123, 456]:\n","    # Initialize KFold (we use leave-one-out approach)\n","    kf = KFold(n_splits=7, shuffle=True, random_state=seed)\n","\n","    # Initialize the Ridge regression model with an alpha value\n","    model = Ridge(alpha=1.0)\n","\n","    # To accumulate predictions and true values\n","    all_y_pred_train = []\n","    all_y_train = []\n","    all_y_pred_test = []\n","    all_y_test = []\n","\n","    for train_index, test_index in kf.split(X):\n","        # Split into training and testing sets using .iloc for pandas\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","        # Fit the model on the training data\n","        model.fit(X_train, y_train)\n","\n","        # Training Predictions\n","        y_pred_train = model.predict(X_train)\n","        all_y_pred_train.extend(y_pred_train)\n","        all_y_train.extend(y_train)\n","\n","        # Testing Predictions\n","        y_pred_test = model.predict(X_test)\n","        if len(X) == 542:\n","            y_pred_test = np.clip(y_pred_test, 1, 7)\n","\n","        all_y_pred_test.extend(y_pred_test)\n","        all_y_test.extend(y_test)\n","\n","    # Evaluate TEST\n","    mae_test = mean_absolute_error(all_y_test, all_y_pred_test)\n","    corr_test, _ = spearmanr(all_y_pred_test, all_y_test)\n","    print('\\n\\n\\n\\nTEST:')\n","    print(\"Mean Absolute Error (MAE):\", mae_test)\n","    print(\"Spearman Correlation:\", corr_test)\n","\n","    if len(X) == 542:\n","        # Convert topics to a numpy array for efficient indexing\n","        topics_array = np.array(topics)\n","\n","        # Unique topics for iteration\n","        unique_topics = ['Brexit', 'vaccination', 'veganism']\n","\n","        for topic in unique_topics:\n","            # Identify indexes for the current topic\n","            topic_indexes_test = np.where(topics_array == topic)[0]  # For test data\n","            topic_indexes_train = np.where(topics_array == topic)[0]  # For train data\n","\n","            # Filter predictions and true values based on the topic\n","            y_pred_train_topic = np.array(all_y_pred_train)[topic_indexes_train]\n","            y_train_topic = np.array(all_y_train)[topic_indexes_train]\n","            y_pred_test_topic = np.array(all_y_pred_test)[topic_indexes_test]\n","            y_test_topic = np.array(all_y_test)[topic_indexes_test]\n","\n","            # Calculate and print metrics for the current topic - TEST\n","            mae_test_topic = mean_absolute_error(y_test_topic, y_pred_test_topic)\n","            corr_test_topic, _ = spearmanr(y_pred_test_topic, y_test_topic)\n","            print(f'\\nTEST - {topic}:')\n","            print(f\"Mean Absolute Error (MAE): {mae_test_topic}\")\n","            print(f\"Spearman Correlation: {corr_test_topic}\")\n","            print(\"\\n\")  # For better separation in output\n","\n","            # Store the results\n","            results[topic]['mae'].append(mae_test_topic)\n","            results[topic]['corr'].append(corr_test_topic)\n","\n","# Calculate mean and SD for each topic\n","for topic in results:\n","    mae_mean, mae_sd = calculate_mean_sd(results[topic]['mae'])\n","    corr_mean, corr_sd = calculate_mean_sd(results[topic]['corr'])\n","    print(f'{topic} - MAE Mean: {round(mae_mean,3)}({round(mae_sd,3)})')\n","    print(f'{topic} - Correlation Mean: {round(corr_mean,3)} ({round(corr_sd,3)})')"]},{"cell_type":"markdown","metadata":{"id":"20BAYbFlkHXu"},"source":["#### 1.3.2.2. Logistic Regression"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19538,"status":"ok","timestamp":1724355108383,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-120},"id":"T8wul0IszCz_","outputId":"fa7283ff-b458-471d-9171-76d3cb07fadb"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n"," 0.1 lbfgs\n","TEST:\n","AUROC: 0.7564361143906599\n","AUPRC: 0.4445065400024353\n","[[512 244]\n"," [ 65 177]]\n","\n","\n"," 0.1 saga\n","TEST:\n","AUROC: 0.7564197166469894\n","AUPRC: 0.4444875483697387\n","[[512 244]\n"," [ 65 177]]\n","\n","\n"," 0.1 sag\n","TEST:\n","AUROC: 0.7564306484761031\n","AUPRC: 0.4444903944473114\n","[[512 244]\n"," [ 65 177]]\n","\n","\n"," 0.1 liblinear\n","TEST:\n","AUROC: 0.7515331890331891\n","AUPRC: 0.4446118594697436\n","[[492 264]\n"," [ 58 184]]\n","\n","\n"," 1 lbfgs\n","TEST:\n","AUROC: 0.7651159867068957\n","AUPRC: 0.45872020917106227\n","[[539 217]\n"," [ 77 165]]\n","\n","\n"," 1 saga\n","TEST:\n","AUROC: 0.7651159867068958\n","AUPRC: 0.4587210337036245\n","[[539 217]\n"," [ 77 165]]\n","\n","\n"," 1 sag\n","TEST:\n","AUROC: 0.7651214526214526\n","AUPRC: 0.45872975827707624\n","[[539 217]\n"," [ 77 165]]\n","\n","\n"," 1 liblinear\n","TEST:\n","AUROC: 0.7646349862258953\n","AUPRC: 0.4590650287995283\n","[[539 217]\n"," [ 77 165]]\n","\n","\n"," 10 lbfgs\n","TEST:\n","AUROC: 0.7662474310201584\n","AUPRC: 0.46558921503109657\n","[[552 204]\n"," [ 73 169]]\n","\n","\n"," 10 saga\n","TEST:\n","AUROC: 0.7662747605929423\n","AUPRC: 0.465602352727299\n","[[552 204]\n"," [ 73 169]]\n","\n","\n"," 10 sag\n","TEST:\n","AUROC: 0.7662583628492718\n","AUPRC: 0.4655807628608092\n","[[552 204]\n"," [ 73 169]]\n","\n","\n"," 10 liblinear\n","TEST:\n","AUROC: 0.7660998513271241\n","AUPRC: 0.4660119945655275\n","[[553 203]\n"," [ 73 169]]\n","\n","\n"," 100 lbfgs\n","TEST:\n","AUROC: 0.7619730858367222\n","AUPRC: 0.4644311091922325\n","[[546 210]\n"," [ 80 162]]\n","\n","\n"," 100 saga\n","TEST:\n","AUROC: 0.7619949494949495\n","AUPRC: 0.4641739742500311\n","[[546 210]\n"," [ 80 162]]\n","\n","\n"," 100 sag\n","TEST:\n","AUROC: 0.7620004154095064\n","AUPRC: 0.464241934269988\n","[[546 210]\n"," [ 80 162]]\n","\n","\n"," 100 liblinear\n","TEST:\n","AUROC: 0.7619184266911538\n","AUPRC: 0.46467038505432784\n","[[546 210]\n"," [ 80 162]]\n","Best hyperparameters: C=10, solver=liblinear\n","Best mean score (AUROC + AUPRC) / 2: 0.6160559229463258\n"]}],"source":["from sklearn.model_selection import KFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, recall_score, precision_score, confusion_matrix\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","\n","best_hyperparams = {'C': None, 'solver': None}\n","best_score = 0\n","\n","for C in [0.1, 1, 10, 100]:\n","    for solver in {'lbfgs', 'liblinear', 'sag', 'saga'}:\n","        print('\\n\\n', C, solver)\n","        # Initialize KFold (we use leave-one-out approach)\n","        kf = KFold(n_splits=7, shuffle=True, random_state=1)\n","\n","        # Initialize the Logistic Regression model\n","        model = LogisticRegression(random_state=42, max_iter=10000, C=C, solver=solver)\n","\n","        # To accumulate predictions and true values\n","        all_y_pred_train = []\n","        all_y_train = []\n","        all_y_pred_test = []\n","        all_y_test = []\n","\n","        for train_index, test_index in kf.split(X):\n","            # Split into training and testing sets using .iloc for pandas\n","            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","            # Fit the model on the training data\n","            model.fit(X_train, y_train)\n","\n","            # Training Predictions\n","            y_pred_train = model.predict_proba(X_train)[:, 1]\n","            all_y_pred_train.extend(y_pred_train)\n","            all_y_train.extend(y_train)\n","\n","            # Testing Predictions\n","            y_pred_test = model.predict_proba(X_test)[:, 1]\n","            all_y_pred_test.extend(y_pred_test)\n","            all_y_test.extend(y_test)\n","\n","        threshold = Counter(y_train)[1] / len(y_train)\n","\n","        # Evaluate TEST\n","        auroc_test = roc_auc_score(all_y_test, all_y_pred_test)\n","        precision_test, recall_test, _ = precision_recall_curve(all_y_test, all_y_pred_test)\n","        auprc_test = auc(recall_test, precision_test)\n","\n","        # Calculate recall, precision, and specificity\n","        y_pred_test_binary = [1 if p >= threshold else 0 for p in all_y_pred_test]\n","        recall = recall_score(all_y_test, y_pred_test_binary)\n","        precision = precision_score(all_y_test, y_pred_test_binary)\n","        cm_test = confusion_matrix(all_y_test, y_pred_test_binary)\n","        tn, fp, fn, tp = cm_test.ravel()\n","        specificity = tn / (tn + fp)\n","\n","        print('TEST:')\n","        print(\"AUROC:\", auroc_test)\n","        print(\"AUPRC:\", auprc_test)\n","        print(cm_test)\n","\n","        # Calculate the mean of AUROC and AUPRC\n","        mean_score = (auroc_test + auprc_test) / 2\n","\n","        # Update best hyperparameters if current mean score is higher\n","        if mean_score > best_score:\n","            best_score = mean_score\n","            best_hyperparams['C'] = C\n","            best_hyperparams['solver'] = solver\n","\n","print(f'Best hyperparameters: C={best_hyperparams[\"C\"]}, solver={best_hyperparams[\"solver\"]}')\n","print(f'Best mean score (AUROC + AUPRC) / 2: {best_score}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9856,"status":"ok","timestamp":1718482406380,"user":{"displayName":"Lexin Zhou","userId":"12761245568431137578"},"user_tz":-60},"id":"St7LKmveqh87","outputId":"0b4ca92e-1c17-416f-bb8f-a477bc3ade86"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Running for seed 42\n","\n","\n","Running for seed 123\n","\n","\n","Running for seed 456\n","\n","\n","TEST:\n","AUROC: 0.918 (0.000)\n","AUPRC: 0.772 (0.002)\n"]}],"source":["from sklearn.model_selection import KFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, recall_score, precision_score, confusion_matrix\n","import numpy as np\n","from collections import Counter\n","\n","# Initialize the optimal hyperparameters\n","C = best_hyperparams[\"C\"]\n","solver = best_hyperparams[\"solver\"]\n","seeds = [42, 123, 456]\n","\n","# To accumulate the results for each seed\n","auroc_scores = []\n","auprc_scores = []\n","recall_scores = []\n","precision_scores = []\n","specificity_scores = []\n","\n","for seed in seeds:\n","    print(f'\\n\\nRunning for seed {seed}')\n","\n","    # Initialize KFold (we use leave-one-out approach)\n","    kf = KFold(n_splits=7, shuffle=True, random_state=seed)\n","\n","    # Initialize the Logistic Regression model\n","    model = LogisticRegression(random_state=seed, max_iter=5000, C=C, solver=solver)\n","\n","    # To accumulate predictions and true values\n","    all_y_pred_train = []\n","    all_y_train = []\n","    all_y_pred_test = []\n","    all_y_test = []\n","\n","    for train_index, test_index in kf.split(X):\n","        # Split into training and testing sets using .iloc for pandas\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","        # Fit the model on the training data\n","        model.fit(X_train, y_train)\n","\n","        # Training Predictions\n","        y_pred_train = model.predict_proba(X_train)[:, 1]\n","        all_y_pred_train.extend(y_pred_train)\n","        all_y_train.extend(y_train)\n","\n","        # Testing Predictions\n","        y_pred_test = model.predict_proba(X_test)[:, 1]\n","        all_y_pred_test.extend(y_pred_test)\n","        all_y_test.extend(y_test)\n","\n","    threshold = Counter(y_train)[1] / len(y_train)\n","\n","    # Evaluate TEST\n","    auroc_test = roc_auc_score(all_y_test, all_y_pred_test)\n","    precision_test, recall_test, _ = precision_recall_curve(all_y_test, all_y_pred_test)\n","    auprc_test = auc(recall_test, precision_test)\n","\n","    # Store the results\n","    auroc_scores.append(auroc_test)\n","    auprc_scores.append(auprc_test)\n","\n","# Calculate mean and standard deviation for each metric\n","auroc_mean = np.mean(auroc_scores)\n","auroc_sd = np.std(auroc_scores)\n","auprc_mean = np.mean(auprc_scores)\n","auprc_sd = np.std(auprc_scores)\n","\n","# Print the results\n","print('\\n\\nTEST:')\n","print(f\"AUROC: {auroc_mean:.3f} ({auroc_sd:.3f})\")\n","print(f\"AUPRC: {auprc_mean:.3f} ({auprc_sd:.3f})\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMFPPxUuq4ZN/LAGvnedcoH","collapsed_sections":["79W5-E3HSwDP","b4h9oC4vZ-AT","UxT72-CIg_7n","LWA1H2nxHo2N","CJ6eygOMa3nC","boAHipeTGwen","QO_X_XCwZPPp","Xex3OvxDj8CW","wP8uMZ9NS7tz","oE1Kz0WWxY7n","rtiiPje_WTtA","Dg-zPna8xPRf","Ku42u6WXmEfq","l1bKRhMZmsRG","j-khhmrFmu8Q","287x_FSmfNxE","-j2B8RBkYVPC","8anE2skqqQfU","kCx3uy89CifY","YQBFyWltRKVK","sfCpXsaSRihS","j9qmnPABDlBB","kFqXeLf6Ak4S","WdIIBQKVmyzm","BxcXiIAE7XIS","ZfoXdFse7ptF"],"machine_shape":"hm","provenance":[{"file_id":"1yYFTBDnpdT6-sI760HJyAqN-FRv6DFiW","timestamp":1710609940227}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
